---
title: Sequence to Sequence(s2s)
categories: sequence to sequence
tags: 深度学习
icon: link
---


## 论文
1. [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)
2. [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf)
3. [Neural Machine Translation By Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)

## 原理
1. [seq2seq学习笔记](https://blog.csdn.net/Jerr__y/article/details/53749693)
2. [漫谈四种神经网络序列解码模型](http://jacoxu.com/encoder_decoder/)

## 实践应用
1. [TensorFlow文本摘要生成 - 基于注意力的序列到序列模型](https://blog.csdn.net/tensorflowshizhan/article/details/69230070)
2. [How to Develop an Encoder-Decoder Model for Sequence-to-Sequence Prediction in Keras](https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/)
3. [How to Define an Encoder-Decoder Sequence-to-Sequence Model for Neural Machine Translation in Keras](https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/)
4. [知乎sequence_to_sequence项目](https://zhuanlan.zhihu.com/p/27608348)

## 注意力机制
1. [**一文解读NLP中的注意力机制**](https://mp.weixin.qq.com/s/TM5poGwSGi5C9szO13GYxg)
2. [自然语言处理中的Attention Model：是什么及为什么](https://blog.csdn.net/malefactor/article/details/50550211)
3. [NLP中的Attention Model](https://www.jianshu.com/p/ed058614b73d?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation)
4. [自然语言处理中的自注意力机制(Self-attention Mechanism)](http://www.cnblogs.com/robert-dlut/p/8638283.html)


## Attention is all you need
1. **[详细图解attention is all you need](http://jalammar.github.io/illustrated-transformer/)**
2. **[详细解释加pytorch实现](http://nlp.seas.harvard.edu/2018/04/03/attention.html)**
3. [另一份详细解释](http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/)
4. [attention is all you need解读](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/)

5. [The Transformer – Attention is all you need.](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/)
6. [pytorch实现](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
7. [中文资料加pytorch实现](https://juejin.im/post/5b9f1af0e51d450e425eb32d)
8. [Google官方tensorflow实现](https://github.com/tensorflow/models/tree/master/official/transformer/model)
9. [中文资料](https://segmentfault.com/a/1190000015575985)
10. [Transformer各层图示](https://www.cnblogs.com/guoyaohua/p/transformer.html)

ua/p/transformer.html)

