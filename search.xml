<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/RNN/Seq2Seq/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
      <url>/2019/06/15/DeepLearning/RNN/Seq2Seq/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="RNN中的注意力机制"><a href="#RNN中的注意力机制" class="headerlink" title="RNN中的注意力机制"></a>RNN中的注意力机制</h1><h2 id="1-一文解读NLP中的注意力机制"><a href="#1-一文解读NLP中的注意力机制" class="headerlink" title="1. 一文解读NLP中的注意力机制"></a>1. <a href="https://mp.weixin.qq.com/s/TM5poGwSGi5C9szO13GYxg" target="_blank" rel="noopener">一文解读NLP中的注意力机制</a></h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/RNN/Seq2Seq/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/"/>
      <url>/2019/06/15/DeepLearning/RNN/Seq2Seq/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/</url>
      
        <content type="html"><![CDATA[<h1 id="Sequence-to-Sequence参考资料"><a href="#Sequence-to-Sequence参考资料" class="headerlink" title="Sequence to Sequence参考资料"></a>Sequence to Sequence参考资料</h1><h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><ol><li><a href="https://arxiv.org/pdf/1409.3215.pdf" target="_blank" rel="noopener">Sequence to Sequence Learning with Neural Networks</a></li><li><a href="https://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a></li><li><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Neural Machine Translation By Jointly Learning To Align and Translate</a></li></ol><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><ol><li><a href="https://blog.csdn.net/Jerr__y/article/details/53749693" target="_blank" rel="noopener">seq2seq学习笔记</a></li><li><a href="http://jacoxu.com/encoder_decoder/" target="_blank" rel="noopener">漫谈四种神经网络序列解码模型</a></li></ol><h2 id="实践应用"><a href="#实践应用" class="headerlink" title="实践应用"></a>实践应用</h2><ol><li><a href="https://blog.csdn.net/tensorflowshizhan/article/details/69230070" target="_blank" rel="noopener">TensorFlow文本摘要生成 - 基于注意力的序列到序列模型</a></li><li><a href="https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/" target="_blank" rel="noopener">How to Develop an Encoder-Decoder Model for Sequence-to-Sequence Prediction in Keras</a></li><li><a href="https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/" target="_blank" rel="noopener">How to Define an Encoder-Decoder Sequence-to-Sequence Model for Neural Machine Translation in Keras</a></li><li><a href="https://zhuanlan.zhihu.com/p/27608348" target="_blank" rel="noopener">知乎sequence_to_sequence项目</a></li></ol><h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><ol><li><a href="https://blog.csdn.net/malefactor/article/details/50550211" target="_blank" rel="noopener">自然语言处理中的Attention Model：是什么及为什么</a></li><li><a href="https://www.jianshu.com/p/ed058614b73d?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation" target="_blank" rel="noopener">NLP中的Attention Model</a></li><li><a href="http://www.cnblogs.com/robert-dlut/p/8638283.html" target="_blank" rel="noopener">自然语言处理中的自注意力机制（Self-attention Mechanism）</a></li></ol><h2 id="Attention-is-all-you-need"><a href="#Attention-is-all-you-need" class="headerlink" title="Attention is all you need"></a>Attention is all you need</h2><ol><li><p><strong><a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">详细图解attention is all you need</a></strong></p></li><li><p><strong><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">详细解释加pytorch实现</a></strong></p></li><li><p><a href="http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/" target="_blank" rel="noopener">另一份详细解释</a></p></li><li><p><a href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/" target="_blank" rel="noopener">attention is all you need解读</a></p></li><li><p><a href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/" target="_blank" rel="noopener">The Transformer – Attention is all you need.</a></p></li><li><p><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">pytorch实现</a></p></li><li><p><a href="https://juejin.im/post/5b9f1af0e51d450e425eb32d" target="_blank" rel="noopener">中文资料加pytorch实现</a></p></li><li><p><a href="https://github.com/tensorflow/models/tree/master/official/transformer/model" target="_blank" rel="noopener">Google官方tensorflow实现</a></p></li><li><p><a href="https://segmentfault.com/a/1190000015575985" target="_blank" rel="noopener">中文资料</a></p></li><li><p><a href="https://www.cnblogs.com/guoyaohua/p/transformer.html" target="_blank" rel="noopener">Transformer各层图示</a></p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/RNN/LSTM/%E5%AF%B9%E4%BA%8ELSTM%E8%BE%93%E5%85%A5%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/"/>
      <url>/2019/06/15/DeepLearning/RNN/LSTM/%E5%AF%B9%E4%BA%8ELSTM%E8%BE%93%E5%85%A5%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="对于LSTM输入层、隐含层及输出层参数的理解"><a href="#对于LSTM输入层、隐含层及输出层参数的理解" class="headerlink" title="对于LSTM输入层、隐含层及输出层参数的理解"></a>对于LSTM输入层、隐含层及输出层参数的理解</h1><hr><p>LSTM输入层要求的维度是三维的，其中包含三个参数:batch_size, input_dim和time_step。隐含层有一个参数：n_hidden。输出层有两个参数：n_hidden和output_dim。下面举两个例子：<a href="https://blog.csdn.net/mebiuw/article/details/52705731" target="_blank" rel="noopener">利用LSTM识别MNIST手写数字集</a>和<a href="https://blog.csdn.net/a819825294/article/details/54376781" target="_blank" rel="noopener">LSTM时间序列分析</a>，谈谈个人对这些参数含义的理解。</p><h2 id="1-利用LSTM识别MNIST手写数字集"><a href="#1-利用LSTM识别MNIST手写数字集" class="headerlink" title="1.利用LSTM识别MNIST手写数字集"></a>1.利用LSTM识别MNIST手写数字集</h2><pre><code class="python">n_input = 28  # 输入层的n n_steps = 28  # 28长度 n_hidden = 128  # 隐含层的特征数 n_classes = 10  # 输出的数量，因为是分类问题，0~9个数字，这里一共有10个batch_size = 128  </code></pre><p><strong>输入层</strong>：首先说下batch_size。这个参数其实和其他神经网络的batch_size意义相同，都指一次性输入到神经网络中训练的个数。这里batch_size=128，含义是一次性将128个图像输入到LSTM中进行训练，完成一次参数计算和更新。再说说n_steps。n_steps实际上指的是构造的LSTM总共有多少个时间上的输入。在这里取n_step = 28，指的是按时间顺序依次输入28次，在同一时刻输入的个数为batch_size * n_input。在MNIST数据集中，一幅图片表示为28*28的矩阵，因此如果一次输入1行，那么要先后依次输入28行才能将一个图片的信息完全输入。那么同时input_dim（在此处为n_input）的含义也很清楚了，就是一次输入的数据维数，在这里就是1行的数据个数。因此，输入端的操作是，在t时刻输入128幅图片的第1行矩阵，t+1时刻输入128幅图片的第2行矩阵。以此类推直到输入完毕。<br><strong>隐含层</strong>：隐含层只有一个新的参数：n_hidden。这个参数表示的是用于记忆和储存过去状态的节点个数。<br><strong>输出层</strong>：输出层也只有一个新的参数：output_dim（在此处为n_classes）。这个参数的含义是输出结果维数。在MNIST数据集中，由于做的是0~9的分类，所以输出维度自然是10，类似于softmax分类。</p><h2 id="2-LSTM时间序列分析"><a href="#2-LSTM时间序列分析" class="headerlink" title="2.LSTM时间序列分析"></a>2.LSTM时间序列分析</h2><p><strong>输入层</strong>：在这个例子中，使用了Keras作为搭建LSTM工具。查看Keras的文档，得知其对输入数据的要求是</p><blockquote><p>形如（samples，timesteps，input_dim）的3D张量</p></blockquote><p>而第二个例子中对于输入数据做的处理为<br><code>x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))</code><br>因此不难比较得到：</p><pre><code class="python">batch_size = x_train.shape[0]time_steps = x_train.shape[1]input_dim = 1</code></pre><p>由于这个例子是给定一个已知序列，对该序列接下来的走势进行预测，因此自然而然想到把一个序列切成训练集和测试集，训练集再根据合适的时间长度分成t~(t+n)的训练集和t+n+1的测试集。那么batch_size的含义是一次性输入训练的序列数。time_step为取的一个时间序列的长度，也就是上一句话的n。在这个例子中，input_dim为1，说明在一个时间点，一个序列只输入1个点。隐含层和输出层类似，不再重复。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/RNN/LSTM/TimeDistributedDense()%E5%B1%82/"/>
      <url>/2019/06/15/DeepLearning/RNN/LSTM/TimeDistributedDense()%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<ol><li><a href="https://github.com/keras-team/keras/issues/2405" target="_blank" rel="noopener">https://github.com/keras-team/keras/issues/2405</a></li><li><a href="https://github.com/CanePunma/Stock_Price_Prediction_With_RNNs/issues/1" target="_blank" rel="noopener">https://github.com/CanePunma/Stock_Price_Prediction_With_RNNs/issues/1</a></li><li><a href="https://stackoverflow.com/questions/42398645/timedistributed-vs-timedistributeddense-keras" target="_blank" rel="noopener">https://stackoverflow.com/questions/42398645/timedistributed-vs-timedistributeddense-keras</a></li><li><a href="https://stackoverflow.com/questions/44611006/timedistributeddense-vs-dense-in-keras-same-number-of-parameters" target="_blank" rel="noopener">https://stackoverflow.com/questions/44611006/timedistributeddense-vs-dense-in-keras-same-number-of-parameters</a></li><li><a href="https://stackoverflow.com/questions/45631235/importerror-cannot-import-name-timedistributeddense-in-keras" target="_blank" rel="noopener">https://stackoverflow.com/questions/45631235/importerror-cannot-import-name-timedistributeddense-in-keras</a></li><li><a href="https://stackoverflow.com/questions/41947039/keras-rnn-with-lstm-cells-for-predicting-multiple-output-time-series-based-on-mu" target="_blank" rel="noopener">https://stackoverflow.com/questions/41947039/keras-rnn-with-lstm-cells-for-predicting-multiple-output-time-series-based-on-mu</a></li><li><a href="https://stackoverflow.com/questions/49661708/keras-lstm-multiple-input-multiple-output" target="_blank" rel="noopener">https://stackoverflow.com/questions/49661708/keras-lstm-multiple-input-multiple-output</a></li><li><a href="https://blog.csdn.net/oQiCheng1234567/article/details/73051251" target="_blank" rel="noopener">https://blog.csdn.net/oQiCheng1234567/article/details/73051251</a></li><li><a href="https://github.com/keras-team/keras/issues/1029" target="_blank" rel="noopener">https://github.com/keras-team/keras/issues/1029</a></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/RNN/LSTM/LSTM%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%BB%9E%E5%90%8E%E9%97%AE%E9%A2%98/"/>
      <url>/2019/06/15/DeepLearning/RNN/LSTM/LSTM%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%BB%9E%E5%90%8E%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="LSTM时序预测滞后问题"><a href="#LSTM时序预测滞后问题" class="headerlink" title="LSTM时序预测滞后问题"></a>LSTM时序预测滞后问题</h1><ol><li><p><a href="https://blog.csdn.net/aliceyangxi1987/article/details/73420583" target="_blank" rel="noopener">用 LSTM 做时间序列预测的一个小例子</a></p></li><li><p><a href="http://www.ilovematlab.cn/thread-165451-1-1.html" target="_blank" rel="noopener">时间序列预测—-预测的结果跟实际的时间序列值存在滞后</a></p></li><li><p><a href="https://blog.csdn.net/CS13522431352/article/details/77369300?locationNum=7" target="_blank" rel="noopener">代码干货 | 基于Keras的LSTM多变量时间序列预测</a></p></li><li><p><a href="https://www.jianshu.com/p/5d6d5aac4dbd" target="_blank" rel="noopener">但如果只用time series数据，你很有可能得到的就是滞后一天的趋势</a></p></li><li><p><a href="https://github.com/owoshch/time_series/blob/master/airline_prediction_one_lstm_layer_with_time_steps.ipynb" target="_blank" rel="noopener">一个带输出图像的LSTM预测</a></p></li><li><p><a href="https://www.douban.com/group/topic/102741080/" target="_blank" rel="noopener">用LSTM预测时间序列存在延迟现象？</a></p></li><li><p><a href="https://stats.stackexchange.com/questions/307340/role-of-delays-in-lstm-networks#comment587967_307340" target="_blank" rel="noopener">Stackexchange problem</a></p></li><li><p><a href="https://stackoverflow.com/questions/35563758/delay-issue-in-time-series-prediction" target="_blank" rel="noopener">Stackoverflow-Delay issue in time series prediction</a></p></li><li><p><a href="https://github.com/keras-team/keras/issues/2856" target="_blank" rel="noopener">LSTM for time series prediction</a></p></li><li><p><a href="http://www.jakob-aungiers.com/articles/a/LSTM-Neural-Network-for-Time-Series-Prediction" target="_blank" rel="noopener">LSTM Neural Network for Time Series Prediction</a></p></li><li><p><a href="https://www.zhihu.com/question/21229371" target="_blank" rel="noopener">知乎问题-Pyhong的答案</a></p></li><li><p><a href="https://dashee87.github.io/deep%20learning/python/predicting-cryptocurrency-prices-with-deep-learning/" target="_blank" rel="noopener">https://dashee87.github.io/deep%20learning/python/predicting-cryptocurrency-prices-with-deep-learning/</a></p></li><li><p><a href="https://medium.com/@siavash_37715/how-to-predict-bitcoin-and-ethereum-price-with-rnn-lstm-in-keras-a6d8ee8a5109" target="_blank" rel="noopener">https://medium.com/@siavash_37715/how-to-predict-bitcoin-and-ethereum-price-with-rnn-lstm-in-keras-a6d8ee8a5109</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/31783805" target="_blank" rel="noopener">最全 LSTM 模型在量化交易中的应用汇总（代码+论文）</a></p></li><li><p><a href="https://www.kaggle.com/pablocastilla/predict-stock-prices-with-lstm" target="_blank" rel="noopener">https://www.kaggle.com/pablocastilla/predict-stock-prices-with-lstm</a></p></li><li><p><a href="https://stackoverflow.com/questions/48034625/keras-lstm-predicted-timeseries-squashed-and-shifted/48050810#48050810" target="_blank" rel="noopener">useful discussion</a></p></li><li><p><a href="https://stackoverflow.com/questions/49697457/lstm-nn-produces-shifted-forecast-low-quality-result/49700184#49700184" target="_blank" rel="noopener">another discussion</a></p></li><li><p><a href="https://stackoverflow.com/questions/39139446/keras-lstm-rnn-forecast-shifting-fitted-forecast-backward" target="_blank" rel="noopener">another another discussion</a></p></li><li><p><a href="https://jiasuhui.com/article/3855" target="_blank" rel="noopener">https://jiasuhui.com/article/3855</a></p></li><li><p><a href="https://www.zhihu.com/question/275040228" target="_blank" rel="noopener">知乎问题</a></p></li><li><p><a href="http://www.cnblogs.com/xuruilong100/p/8451790.html" target="_blank" rel="noopener">延时现象解决</a></p></li><li><p><a href="https://github.com/keras-team/keras/issues/2856" target="_blank" rel="noopener">github上的讨论</a></p></li><li><p><a href="https://www.researchgate.net/post/How_can_I_decrease_the_ANN_forecasting_delay" target="_blank" rel="noopener">ResearchGate上的讨论</a></p></li><li><p><strong>possible solution</strong></p><ol><li>randomize training samples in each batch, make sure they are not followed one by one</li><li>choose or design a better loss function other than MSE</li><li>extract some features from the input time series</li><li>manually limit the weight of x_{t-1}, x_{t-2}</li></ol></li><li><p><a href="https://www.datacamp.com/community/tutorials/lstm-python-stock-market" target="_blank" rel="noopener">https://www.datacamp.com/community/tutorials/lstm-python-stock-market</a></p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/RNN/LSTM/LSTM/"/>
      <url>/2019/06/15/DeepLearning/RNN/LSTM/LSTM/</url>
      
        <content type="html"><![CDATA[<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><h2 id="1-LSTM原理"><a href="#1-LSTM原理" class="headerlink" title="1. LSTM原理"></a>1. LSTM原理</h2><ol><li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">English version：Understanding LSTM Networks</a></li><li><a href="https://www.jianshu.com/p/4b4701beba92" target="_blank" rel="noopener">中文版：如何简单的理解LSTM——其实没有那么复杂</a></li><li><a href="https://zybuluo.com/hanbingtao/note/581764" target="_blank" rel="noopener">作业部落</a></li><li><a href="https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714" target="_blank" rel="noopener">好看的图</a></li></ol><h2 id="2-LSTM基础应用"><a href="#2-LSTM基础应用" class="headerlink" title="2. LSTM基础应用"></a>2. LSTM基础应用</h2><ol><li><a href="https://yq.aliyun.com/articles/202939" target="_blank" rel="noopener">LSTM在MNIST数据集中的运用</a></li></ol><h2 id="3-LSTM实战"><a href="#3-LSTM实战" class="headerlink" title="3. LSTM实战"></a>3. LSTM实战</h2><ol><li><a href="https://blog.csdn.net/mylove0414/article/details/56969181" target="_blank" rel="noopener"># Tensorflow实例：利用LSTM预测股票每日最高价（二）</a></li><li><a href="https://blog.csdn.net/flying_sfeng/article/details/78852816" target="_blank" rel="noopener">使用tensorflow的lstm网络进行时间序列预测</a></li><li><a href="https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/" target="_blank" rel="noopener">基于单变量的LSTM时间序列预测</a></li><li><a href="https://yq.aliyun.com/articles/174270" target="_blank" rel="noopener">基于多变量的LSTM时间序列预测</a></li><li><a href="https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/" target="_blank" rel="noopener">基于LSTM的多步预测</a></li><li><a href="https://www.jianshu.com/p/5d6d5aac4dbd" target="_blank" rel="noopener">实用LSTM时间预测例子</a></li><li><a href="https://yq.aliyun.com/articles/68463" target="_blank" rel="noopener">多层LSTM网络：教你打造股市晴雨表——通过LSTM神经网络预测股市</a></li><li><a href="https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/" target="_blank" rel="noopener">LSTM英文句子预测问题</a></li><li><a href="http://www.willfleury.com/machine-learning/forecasting/lstm/2017/09/01/short-term-forceasting-lstm.html" target="_blank" rel="noopener">lstm时间序列预测程序</a></li></ol><h2 id="4-LSTM相关问题解答"><a href="#4-LSTM相关问题解答" class="headerlink" title="4. LSTM相关问题解答"></a>4. LSTM相关问题解答</h2><ol><li><a href="https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell#comment83076885_37901047" target="_blank" rel="noopener">tensorflow中LSTM模型的n_hidden含义</a><h2 id="5-LSTM论文及资料"><a href="#5-LSTM论文及资料" class="headerlink" title="5. LSTM论文及资料"></a>5. LSTM论文及资料</h2><a href="http://suanfazu.com/t/rnn-lstm/13587" target="_blank" rel="noopener">rnn和lstm资源收集</a></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/RNN/LSTM/LSTM%20stateful%E7%9A%84%E7%90%86%E8%A7%A3/"/>
      <url>/2019/06/15/DeepLearning/RNN/LSTM/LSTM%20stateful%E7%9A%84%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="LSTM-stateful的理解"><a href="#LSTM-stateful的理解" class="headerlink" title="LSTM stateful的理解"></a>LSTM stateful的理解</h1><ol><li><a href="http://philipperemy.github.io/keras-stateful-lstm/" target="_blank" rel="noopener">Stateful LSTM in Keras</a></li><li><a href="https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/" target="_blank" rel="noopener">Understanding Stateful LSTM Recurrent Neural Networks in Python with Keras</a><blockquote><p>The LSTM networks are stateful. They should be able to learn the whole alphabet sequence, but by default the Keras implementation resets the network state after each training batch.</p></blockquote></li><li><a href="https://stackoverflow.com/questions/43882796/when-does-keras-reset-an-lstm-state" target="_blank" rel="noopener">https://stackoverflow.com/questions/43882796/when-does-keras-reset-an-lstm-state</a></li><li><a href="https://ahstat.github.io/RNN-Keras-time-series/" target="_blank" rel="noopener">https://ahstat.github.io/RNN-Keras-time-series/</a></li><li><a href="https://zhuanlan.zhihu.com/p/34495801#comment-465502125" target="_blank" rel="noopener">Keras之stateful LSTM全面解析+实例测试</a></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/RNN/RNN%E4%B8%80%E4%BA%9B%E6%96%B0%E7%9A%84%E5%8F%91%E5%B1%95%E6%96%B9%E5%90%91/"/>
      <url>/2019/06/15/DeepLearning/RNN/RNN%E4%B8%80%E4%BA%9B%E6%96%B0%E7%9A%84%E5%8F%91%E5%B1%95%E6%96%B9%E5%90%91/</url>
      
        <content type="html"><![CDATA[<h1 id="some-development-of-RNN"><a href="#some-development-of-RNN" class="headerlink" title="some development of RNN"></a>some development of RNN</h1><ol><li><p><a href="https://distill.pub/2016/augmented-rnns/#neural-turing-machines" target="_blank" rel="noopener">Attention and Augmented Recurrent Neural Networks</a></p></li><li><p><a href="https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/43795" target="_blank" rel="noopener">Web Traffic Time Series Forecasting Forecast future traffic to Wikipedia pages</a></p><ol><li><a href="https://github.com/Arturus/kaggle-web-traffic" target="_blank" rel="noopener">github project</a></li></ol></li><li><p><a href="https://github.com/ysn2233/attentioned-dual-stage-stock-prediction" target="_blank" rel="noopener">Pytorch dual attention model</a></p></li><li><p><a href="https://www.jianshu.com/p/0d1e9f0db887" target="_blank" rel="noopener">使用Attentioned Dual-Stage RNN模型预测股票(PyTorch)</a></p></li><li><p><a href="http://chandlerzuo.github.io/blog/2017/11/darnn" target="_blank" rel="noopener">A PyTorch Example to Use RNN for Financial Prediction</a></p><ol><li><a href="https://github.com/Seanny123/da-rnn" target="_blank" rel="noopener">github project</a></li></ol></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/other%20techniques/layer%20normalization/"/>
      <url>/2019/06/15/DeepLearning/other%20techniques/layer%20normalization/</url>
      
        <content type="html"><![CDATA[<h1 id="batch-amp-layer-normalization-参考资料"><a href="#batch-amp-layer-normalization-参考资料" class="headerlink" title="batch &amp; layer normalization 参考资料"></a>batch &amp; layer normalization 参考资料</h1><ol><li><a href="http://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/" target="_blank" rel="noopener">Weight Normalization and Layer Normalization Explained </a></li><li><a href="https://medium.com/@ilango100/batch-normalization-speed-up-neural-network-training-245e39a62f85" target="_blank" rel="noopener">Batch Normalization详解</a></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/NLP/Word2vec%E8%B5%84%E6%96%99/"/>
      <url>/2019/06/15/DeepLearning/NLP/Word2vec%E8%B5%84%E6%96%99/</url>
      
        <content type="html"><![CDATA[<h1 id="Word2vec相关资料"><a href="#Word2vec相关资料" class="headerlink" title="Word2vec相关资料"></a>Word2vec相关资料</h1><ol><li><a href="https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285" target="_blank" rel="noopener"><strong>Word embedding</strong></a></li><li><a href="https://www.cnblogs.com/pinard/p/7160330.html" target="_blank" rel="noopener">word2vec原理(一) CBOW与Skip-Gram模型基础</a></li><li><a href="https://www.cnblogs.com/pinard/p/7243513.html" target="_blank" rel="noopener">word2vec原理(二) 基于Hierarchical Softmax的模型</a></li><li><a href="https://www.cnblogs.com/pinard/p/7249903.html" target="_blank" rel="noopener">word2vec原理(三) 基于Negative Sampling的模型</a></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/NLP/NLP%E5%8F%91%E5%B1%95%E6%96%B0%E6%96%B9%E5%90%91/"/>
      <url>/2019/06/15/DeepLearning/NLP/NLP%E5%8F%91%E5%B1%95%E6%96%B0%E6%96%B9%E5%90%91/</url>
      
        <content type="html"><![CDATA[<h1 id="NLP新的发展方向"><a href="#NLP新的发展方向" class="headerlink" title="NLP新的发展方向"></a>NLP新的发展方向</h1><ol><li><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247493520&idx=1&sn=2b04c009ef75291ef3d19e8fe673aa36&chksm=96ea3810a19db10621e7a661974c796e8adeffc31625a769f8db1d87ba803cd58a30d40ad7ce&scene=21#wechat_redirect" target="_blank" rel="noopener">NLP的巨人肩膀上</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247493731&idx=1&sn=51206e4ca3983548436d889590ab5347&chksm=96ea37e3a19dbef5b6db3143eb9df822915126d3d8f61fe73ddb9f8fa329d568ec79a662acb1&mpshare=1&scene=23&srcid=12177Ua04Q6MGcDDUf5HSrL0#rd" target="_blank" rel="noopener">NLP的巨人肩膀下</a></li><li><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" target="_blank" rel="noopener">https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html</a></li><li><a href="https://zhuanlan.zhihu.com/p/54743941" target="_blank" rel="noopener">放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较</a></li><li><a href="https://mp.weixin.qq.com/s/TM5poGwSGi5C9szO13GYxg" target="_blank" rel="noopener">NLP中的attention机制</a></li><li><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247497461&idx=1&sn=2821912a51749ef50a46007249fec26a&chksm=96ea2975a19da0633f3286750088a22c78987628666a6cd2d42d1fbee74f9c189e073d111dc2&mpshare=1&scene=23&srcid=#rd" target="_blank" rel="noopener">ICLR 2019最佳论文 | ON-LSTM：用有序神经元表达层次结构</a></li><li><a href="https://zhuanlan.zhihu.com/p/68446772" target="_blank" rel="noopener">Bert时代的创新（应用篇）：Bert在NLP各领域的应用进展</a></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/LSTM/%E5%AF%B9%E4%BA%8ELSTM%E8%BE%93%E5%85%A5%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/"/>
      <url>/2019/06/15/DeepLearning/LSTM/%E5%AF%B9%E4%BA%8ELSTM%E8%BE%93%E5%85%A5%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="对于LSTM输入层、隐含层及输出层参数的理解"><a href="#对于LSTM输入层、隐含层及输出层参数的理解" class="headerlink" title="对于LSTM输入层、隐含层及输出层参数的理解"></a>对于LSTM输入层、隐含层及输出层参数的理解</h1><hr><p>LSTM输入层要求的维度是三维的，其中包含三个参数:batch_size, input_dim和time_step。隐含层有一个参数：n_hidden。输出层有两个参数：n_hidden和output_dim。下面举两个例子：<a href="https://blog.csdn.net/mebiuw/article/details/52705731" target="_blank" rel="noopener">利用LSTM识别MNIST手写数字集</a>和<a href="https://blog.csdn.net/a819825294/article/details/54376781" target="_blank" rel="noopener">LSTM时间序列分析</a>，谈谈个人对这些参数含义的理解。</p><h2 id="1-利用LSTM识别MNIST手写数字集"><a href="#1-利用LSTM识别MNIST手写数字集" class="headerlink" title="1.利用LSTM识别MNIST手写数字集"></a>1.利用LSTM识别MNIST手写数字集</h2><pre><code class="python">n_input = 28  # 输入层的n n_steps = 28  # 28长度 n_hidden = 128  # 隐含层的特征数 n_classes = 10  # 输出的数量，因为是分类问题，0~9个数字，这里一共有10个batch_size = 128  </code></pre><p><strong>输入层</strong>：首先说下batch_size。这个参数其实和其他神经网络的batch_size意义相同，都指一次性输入到神经网络中训练的个数。这里batch_size=128，含义是一次性将128个图像输入到LSTM中进行训练，完成一次参数计算和更新。再说说n_steps。n_steps实际上指的是构造的LSTM总共有多少个时间上的输入。在这里取n_step = 28，指的是按时间顺序依次输入28次，在同一时刻输入的个数为batch_size * n_input。在MNIST数据集中，一幅图片表示为28*28的矩阵，因此如果一次输入1行，那么要先后依次输入28行才能将一个图片的信息完全输入。那么同时input_dim（在此处为n_input）的含义也很清楚了，就是一次输入的数据维数，在这里就是1行的数据个数。因此，输入端的操作是，在t时刻输入128幅图片的第1行矩阵，t+1时刻输入128幅图片的第2行矩阵。以此类推直到输入完毕。<br><strong>隐含层</strong>：隐含层只有一个新的参数：n_hidden。这个参数表示的是用于记忆和储存过去状态的节点个数。<br><strong>输出层</strong>：输出层也只有一个新的参数：output_dim（在此处为n_classes）。这个参数的含义是输出结果维数。在MNIST数据集中，由于做的是0~9的分类，所以输出维度自然是10，类似于softmax分类。</p><h2 id="2-LSTM时间序列分析"><a href="#2-LSTM时间序列分析" class="headerlink" title="2.LSTM时间序列分析"></a>2.LSTM时间序列分析</h2><p><strong>输入层</strong>：在这个例子中，使用了Keras作为搭建LSTM工具。查看Keras的文档，得知其对输入数据的要求是</p><blockquote><p>形如（samples，timesteps，input_dim）的3D张量</p></blockquote><p>而第二个例子中对于输入数据做的处理为<br><code>x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))</code><br>因此不难比较得到：</p><pre><code class="python">batch_size = x_train.shape[0]time_steps = x_train.shape[1]input_dim = 1</code></pre><p>由于这个例子是给定一个已知序列，对该序列接下来的走势进行预测，因此自然而然想到把一个序列切成训练集和测试集，训练集再根据合适的时间长度分成t~(t+n)的训练集和t+n+1的测试集。那么batch_size的含义是一次性输入训练的序列数。time_step为取的一个时间序列的长度，也就是上一句话的n。在这个例子中，input_dim为1，说明在一个时间点，一个序列只输入1个点。隐含层和输出层类似，不再重复。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/LSTM/TimeDistributedDense()%E5%B1%82/"/>
      <url>/2019/06/15/DeepLearning/LSTM/TimeDistributedDense()%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<ol><li><a href="https://github.com/keras-team/keras/issues/2405" target="_blank" rel="noopener">https://github.com/keras-team/keras/issues/2405</a></li><li><a href="https://github.com/CanePunma/Stock_Price_Prediction_With_RNNs/issues/1" target="_blank" rel="noopener">https://github.com/CanePunma/Stock_Price_Prediction_With_RNNs/issues/1</a></li><li><a href="https://stackoverflow.com/questions/42398645/timedistributed-vs-timedistributeddense-keras" target="_blank" rel="noopener">https://stackoverflow.com/questions/42398645/timedistributed-vs-timedistributeddense-keras</a></li><li><a href="https://stackoverflow.com/questions/44611006/timedistributeddense-vs-dense-in-keras-same-number-of-parameters" target="_blank" rel="noopener">https://stackoverflow.com/questions/44611006/timedistributeddense-vs-dense-in-keras-same-number-of-parameters</a></li><li><a href="https://stackoverflow.com/questions/45631235/importerror-cannot-import-name-timedistributeddense-in-keras" target="_blank" rel="noopener">https://stackoverflow.com/questions/45631235/importerror-cannot-import-name-timedistributeddense-in-keras</a></li><li><a href="https://stackoverflow.com/questions/41947039/keras-rnn-with-lstm-cells-for-predicting-multiple-output-time-series-based-on-mu" target="_blank" rel="noopener">https://stackoverflow.com/questions/41947039/keras-rnn-with-lstm-cells-for-predicting-multiple-output-time-series-based-on-mu</a></li><li><a href="https://stackoverflow.com/questions/49661708/keras-lstm-multiple-input-multiple-output" target="_blank" rel="noopener">https://stackoverflow.com/questions/49661708/keras-lstm-multiple-input-multiple-output</a></li><li><a href="https://blog.csdn.net/oQiCheng1234567/article/details/73051251" target="_blank" rel="noopener">https://blog.csdn.net/oQiCheng1234567/article/details/73051251</a></li><li><a href="https://github.com/keras-team/keras/issues/1029" target="_blank" rel="noopener">https://github.com/keras-team/keras/issues/1029</a></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/LSTM/LSTM%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%BB%9E%E5%90%8E%E9%97%AE%E9%A2%98/"/>
      <url>/2019/06/15/DeepLearning/LSTM/LSTM%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%BB%9E%E5%90%8E%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="LSTM时序预测滞后问题"><a href="#LSTM时序预测滞后问题" class="headerlink" title="LSTM时序预测滞后问题"></a>LSTM时序预测滞后问题</h1><ol><li><a href="https://blog.csdn.net/aliceyangxi1987/article/details/73420583" target="_blank" rel="noopener">用 LSTM 做时间序列预测的一个小例子</a></li><li><a href="http://www.ilovematlab.cn/thread-165451-1-1.html" target="_blank" rel="noopener">时间序列预测—-预测的结果跟实际的时间序列值存在滞后</a></li><li><a href="https://blog.csdn.net/CS13522431352/article/details/77369300?locationNum=7" target="_blank" rel="noopener">代码干货 | 基于Keras的LSTM多变量时间序列预测</a></li><li><a href="https://www.jianshu.com/p/5d6d5aac4dbd" target="_blank" rel="noopener">但如果只用time series数据，你很有可能得到的就是滞后一天的趋势</a></li><li><a href="https://github.com/owoshch/time_series/blob/master/airline_prediction_one_lstm_layer_with_time_steps.ipynb" target="_blank" rel="noopener">一个带输出图像的LSTM预测</a></li><li><a href="https://www.douban.com/group/topic/102741080/" target="_blank" rel="noopener">用LSTM预测时间序列存在延迟现象？</a></li><li><a href="https://stats.stackexchange.com/questions/307340/role-of-delays-in-lstm-networks#comment587967_307340" target="_blank" rel="noopener">Stackexchange problem</a></li><li><a href="https://stackoverflow.com/questions/35563758/delay-issue-in-time-series-prediction" target="_blank" rel="noopener">Stackoverflow-Delay issue in time series prediction</a></li><li><a href="https://github.com/keras-team/keras/issues/2856" target="_blank" rel="noopener">LSTM for time series prediction</a></li><li><a href="http://www.jakob-aungiers.com/articles/a/LSTM-Neural-Network-for-Time-Series-Prediction" target="_blank" rel="noopener">LSTM Neural Network for Time Series Prediction</a></li><li><a href="https://www.zhihu.com/question/21229371" target="_blank" rel="noopener">知乎问题-Pyhong的答案</a></li><li><a href="https://dashee87.github.io/deep%20learning/python/predicting-cryptocurrency-prices-with-deep-learning/" target="_blank" rel="noopener">https://dashee87.github.io/deep%20learning/python/predicting-cryptocurrency-prices-with-deep-learning/</a></li><li><a href="https://medium.com/@siavash_37715/how-to-predict-bitcoin-and-ethereum-price-with-rnn-lstm-in-keras-a6d8ee8a5109" target="_blank" rel="noopener">https://medium.com/@siavash_37715/how-to-predict-bitcoin-and-ethereum-price-with-rnn-lstm-in-keras-a6d8ee8a5109</a></li><li><a href="https://zhuanlan.zhihu.com/p/31783805" target="_blank" rel="noopener">最全 LSTM 模型在量化交易中的应用汇总（代码+论文）</a></li><li><a href="https://www.kaggle.com/pablocastilla/predict-stock-prices-with-lstm" target="_blank" rel="noopener">https://www.kaggle.com/pablocastilla/predict-stock-prices-with-lstm</a></li><li><a href="https://stackoverflow.com/questions/48034625/keras-lstm-predicted-timeseries-squashed-and-shifted/48050810#48050810" target="_blank" rel="noopener">useful discussion</a></li><li><a href="https://stackoverflow.com/questions/49697457/lstm-nn-produces-shifted-forecast-low-quality-result/49700184#49700184" target="_blank" rel="noopener">another discussion</a></li><li><a href="https://stackoverflow.com/questions/39139446/keras-lstm-rnn-forecast-shifting-fitted-forecast-backward" target="_blank" rel="noopener">another another discussion</a></li><li><a href="https://jiasuhui.com/article/3855" target="_blank" rel="noopener">https://jiasuhui.com/article/3855</a></li><li><a href="https://www.zhihu.com/question/275040228" target="_blank" rel="noopener">知乎问题</a></li><li><a href="http://www.cnblogs.com/xuruilong100/p/8451790.html" target="_blank" rel="noopener">延时现象解决</a></li><li><a href="https://github.com/keras-team/keras/issues/2856" target="_blank" rel="noopener">github上的讨论</a></li><li><a href="https://www.researchgate.net/post/How_can_I_decrease_the_ANN_forecasting_delay" target="_blank" rel="noopener">ResearchGate上的讨论</a></li><li><strong>possible solution</strong><ol><li>randomize training samples in each batch, make sure they are not followed one by one</li><li>choose or design a better loss function other than MSE</li><li>extract some features from the input time series</li><li>manually limit the weight of x_{t-1}, x_{t-2}</li></ol></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/LSTM/LSTM/"/>
      <url>/2019/06/15/DeepLearning/LSTM/LSTM/</url>
      
        <content type="html"><![CDATA[<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><h2 id="1-LSTM原理"><a href="#1-LSTM原理" class="headerlink" title="1. LSTM原理"></a>1. LSTM原理</h2><ol><li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">English version：Understanding LSTM Networks</a></li><li><a href="https://www.jianshu.com/p/4b4701beba92" target="_blank" rel="noopener">中文版：如何简单的理解LSTM——其实没有那么复杂</a></li><li><a href="https://zybuluo.com/hanbingtao/note/581764" target="_blank" rel="noopener">作业部落</a></li><li><a href="https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714" target="_blank" rel="noopener">好看的图</a></li></ol><h2 id="2-LSTM基础应用"><a href="#2-LSTM基础应用" class="headerlink" title="2. LSTM基础应用"></a>2. LSTM基础应用</h2><ol><li><a href="https://yq.aliyun.com/articles/202939" target="_blank" rel="noopener">LSTM在MNIST数据集中的运用</a></li></ol><h2 id="3-LSTM实战"><a href="#3-LSTM实战" class="headerlink" title="3. LSTM实战"></a>3. LSTM实战</h2><ol><li><a href="https://blog.csdn.net/mylove0414/article/details/56969181" target="_blank" rel="noopener"># Tensorflow实例：利用LSTM预测股票每日最高价（二）</a></li><li><a href="https://blog.csdn.net/flying_sfeng/article/details/78852816" target="_blank" rel="noopener">使用tensorflow的lstm网络进行时间序列预测</a></li><li><a href="https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/" target="_blank" rel="noopener">基于单变量的LSTM时间序列预测</a></li><li><a href="https://yq.aliyun.com/articles/174270" target="_blank" rel="noopener">基于多变量的LSTM时间序列预测</a></li><li><a href="https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/" target="_blank" rel="noopener">基于LSTM的多步预测</a></li><li><a href="https://www.jianshu.com/p/5d6d5aac4dbd" target="_blank" rel="noopener">实用LSTM时间预测例子</a></li><li><a href="https://yq.aliyun.com/articles/68463" target="_blank" rel="noopener">多层LSTM网络：教你打造股市晴雨表——通过LSTM神经网络预测股市</a></li><li><a href="https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/" target="_blank" rel="noopener">LSTM英文句子预测问题</a></li><li><a href="http://www.willfleury.com/machine-learning/forecasting/lstm/2017/09/01/short-term-forceasting-lstm.html" target="_blank" rel="noopener">lstm时间序列预测程序</a></li></ol><h2 id="4-LSTM相关问题解答"><a href="#4-LSTM相关问题解答" class="headerlink" title="4. LSTM相关问题解答"></a>4. LSTM相关问题解答</h2><ol><li><a href="https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell#comment83076885_37901047" target="_blank" rel="noopener">tensorflow中LSTM模型的n_hidden含义</a><h2 id="5-LSTM论文及资料"><a href="#5-LSTM论文及资料" class="headerlink" title="5. LSTM论文及资料"></a>5. LSTM论文及资料</h2><a href="http://suanfazu.com/t/rnn-lstm/13587" target="_blank" rel="noopener">rnn和lstm资源收集</a></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/LSTM/LSTM%20stateful%E7%9A%84%E7%90%86%E8%A7%A3/"/>
      <url>/2019/06/15/DeepLearning/LSTM/LSTM%20stateful%E7%9A%84%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="LSTM-stateful的理解"><a href="#LSTM-stateful的理解" class="headerlink" title="LSTM stateful的理解"></a>LSTM stateful的理解</h1><ol><li><a href="http://philipperemy.github.io/keras-stateful-lstm/" target="_blank" rel="noopener">Stateful LSTM in Keras</a></li><li><a href="https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/" target="_blank" rel="noopener">Understanding Stateful LSTM Recurrent Neural Networks in Python with Keras</a><blockquote><p>The LSTM networks are stateful. They should be able to learn the whole alphabet sequence, but by default the Keras implementation resets the network state after each training batch.</p></blockquote></li><li><a href="https://stackoverflow.com/questions/43882796/when-does-keras-reset-an-lstm-state" target="_blank" rel="noopener">https://stackoverflow.com/questions/43882796/when-does-keras-reset-an-lstm-state</a></li><li><a href="https://ahstat.github.io/RNN-Keras-time-series/" target="_blank" rel="noopener">https://ahstat.github.io/RNN-Keras-time-series/</a></li><li><a href="https://zhuanlan.zhihu.com/p/34495801#comment-465502125" target="_blank" rel="noopener">Keras之stateful LSTM全面解析+实例测试</a></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/DBN/%E6%B7%B1%E5%BA%A6%E4%BF%A1%E5%BF%B5%E7%BD%91%E7%BB%9C(Deep%20Belief%20Network,DBN)/"/>
      <url>/2019/06/15/DeepLearning/DBN/%E6%B7%B1%E5%BA%A6%E4%BF%A1%E5%BF%B5%E7%BD%91%E7%BB%9C(Deep%20Belief%20Network,DBN)/</url>
      
        <content type="html"><![CDATA[<p>Tags: 深度学习</p><h1 id="深度信念网络-Deep-Belief-Network-DBN"><a href="#深度信念网络-Deep-Belief-Network-DBN" class="headerlink" title="深度信念网络(Deep Belief Network,DBN)"></a>深度信念网络(Deep Belief Network,DBN)</h1><h2 id="1-初识深度信念网络"><a href="#1-初识深度信念网络" class="headerlink" title="1.初识深度信念网络"></a>1.初识深度信念网络</h2><p>深度信念网络是一个概率生成模型，与传统的判别模型的神经网络相对，生成模型是建立一个观察数据和标签之间的联合分布，对$P(Observation|Label)$和 $P(Label|Observation)$都做了评估，而判别模型仅仅而已评估了后者，也就是$P(Label|Observation)$。<br>DBNs由多个限制玻尔兹曼机（Restricted Boltzmann Machines）层组成，一个典型的网络结构如图1所示。这些网络被“限制”为一个可视层和一个隐层，层间存在连接，但层内的单元间不存在连接。隐层单元被训练去捕捉在可视层表现出来的高阶数据的相关性。<br><img src="http://img.blog.csdn.net/20161213120114382?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTgxOTgyNTI5NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><h2 id="2-需要面对的问题"><a href="#2-需要面对的问题" class="headerlink" title="2.需要面对的问题"></a>2.需要面对的问题</h2><p>对于在深度神经网络应用传统的BP算法的时候，DBN遇到了以下问题：  </p><ol><li>需要为训练提供一个有标签的样本集；</li><li>学习过程较慢；</li><li>不适当的参数选择会导致学习收敛于局部最优解。</li></ol><p><strong>Solution：</strong><br>首先，先不考虑最顶构成一个联想记忆（associative memory）的两层，一个DBN的连接是通过自顶向下的生成权值来指导确定的，RBMs就像一个建筑块一样，相比传统和深度分层的sigmoid信念网络，它能易于连接权值的学习。<br>最开始的时候，通过一个非监督贪婪逐层方法去预训练获得生成模型的权值，非监督贪婪逐层方法被Hinton证明是有效的，并被其称为对比分歧（contrastive divergence）。<br><img src="http://img.blog.csdn.net/20161213123012466?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTgxOTgyNTI5NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br>在这个训练阶段，在可视层会产生一个向量v，通过它将值传递到隐层。反过来，可视层的输入会被随机的选择，以尝试去重构原始的输入信号。最后，这些新的可视的神经元激活单元将前向传递重构隐层激活单元，获得h（在训练过程中，首先将可视向量值映射给隐单元；然后可视单元由隐层单元重建；这些新可视单元再次映射给隐单元，这样就获取新的隐单元。执行这种反复步骤叫做吉布斯采样）。这些后退和前进的步骤就是我们熟悉的Gibbs采样，而隐层激活单元和可视层输入之间的相关性差别就作为权值更新的主要依据。<br>训练时间会显著的减少，因为只需要单个步骤就可以接近最大似然学习。增加进网络的每一层都会改进训练数据的对数概率，我们可以理解为越来越接近能量的真实表达。这个有意义的拓展，和无标签数据的使用，是任何一个深度学习应用的决定性的因素。<br>在最高两层，权值被连接到一起，这样更低层的输出将会提供一个参考的线索或者关联给顶层，这样顶层就会将其联系到它的记忆内容。而我们最关心的，最后想得到的就是判别性能，例如分类任务里面。<br>在预训练后，DBN可以通过利用带标签数据用BP算法去对判别性能做调整。在这里，一个标签集将被附加到顶层（推广联想记忆），通过一个自下向上的，学习到的识别权值获得一个网络的分类面。这个性能会比单纯的BP算法训练的网络好。这可以很直观的解释，DBNs的BP算法只需要对权值参数空间进行一个局部的搜索，这相比前向神经网络来说，训练是要快的，而且收敛的时间也少。</p><h2 id="3-详细训练算法流程"><a href="#3-详细训练算法流程" class="headerlink" title="3.详细训练算法流程"></a>3.详细训练算法流程</h2><p><img src="http://img.blog.csdn.net/20161213122524412?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTgxOTgyNTI5NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br>在训练时,Hinton采用了逐层无监督的方法来学习参数。如图3所示，首先把数据向量x和第一层隐藏层作为一个RBM, 训练出这个RBM的参数(连接x和h1的权重, x和h1各个节点的偏置等等), 然后固定这个RBM的参数, 把h1视作可见向量, 把h2视作隐藏向量, 训练第二个RBM, 得到其参数, 然后固定这些参数, 训练h2和h3构成的RBM, 具体的训练算法如下:<br><img src="http://img.blog.csdn.net/20161213123230571?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTgxOTgyNTI5NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br>  CD的训练过程中用到了Gibbs 采样，即在训练过程中，首先将可视向量值映射给隐单元，然后用隐层单元重建可视向量，接着再将可视向量值映射给隐单元……反复执行这种步骤。<br>　k-Gibbs的过程如下：<br><img src="http://img.blog.csdn.net/20161213123405362?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTgxOTgyNTI5NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br>其中，P是model distribution，$\hat{P}$是training set distribution.<br>DBN训练算法：<br><img src="http://img.blog.csdn.net/20161213123735570?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTgxOTgyNTI5NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br>DBN运用CD算法逐层进行训练，得到每一层的参数Wi和ci用于初始化DBN，之后再用监督学习算法对参数进行微调。<br><img src="http://img.blog.csdn.net/20161213123934323?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTgxOTgyNTI5NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><h2 id="4-经典网络结构"><a href="#4-经典网络结构" class="headerlink" title="4.经典网络结构"></a>4.经典网络结构</h2><p>经典的DBN网络结构是由若干层RBM和一层BP组成的一种深层神经网络, 结构如下图4所示。<br><img src="http://img.blog.csdn.net/20161213124220124?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTgxOTgyNTI5NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br>DBN 在训练模型的过程中主要分为两步:</p><ol><li>分别单独无监督地训练每一层 RBM 网络,确保特征向量映射到不同特征空间时,都尽可能多地保留特征信息;</li><li>在 DBN 的最后一层设置BP网络,接收RBM的输出特征向量作为它的输入特征向量,有监督地训练实体关系分类器.而且每一层 RBM 网络只能确保自身层内的 权值对该层特征向量映射达到最优,并不是对整个 DBN 的特征向量映射达到最优,所以反向传播网络还将错误信息自顶向下传播至每一层 RBM,微调整个 DBN 网络.RBM 网络训练模型的过程可以看作对一个深层 BP 网络权值参数的初始化,使DBN 克服了 BP 网络因随机初始化权值参数而容易陷入局部最优和训练时间长的缺点。<br>上述训练模型中第一步在深度学习的术语叫做预训练，第二步叫做微调。最上面有监督学习的那一层，根据具体的应用领域可以换成任何分类器模型，而不必是BP网络。</li></ol><h2 id="5-拓展"><a href="#5-拓展" class="headerlink" title="5.拓展"></a>5.拓展</h2><p>DBN的灵活性使得它的拓展比较容易。一个拓展就是卷积DBNs(Convolutional Deep Belief Networks(CDBN))。DBN并没有考虑到图像的2维结构信息，因为输入是简单的从一个图像矩阵一维向量化的。而CDBN就是考虑到了这个问题，它利用邻域像素的空域关系，通过一个称为卷积RBM的模型区达到生成模型的变换不变性，而且可以容易得变换到高维图像。DBN并没有明确地处理对观察变量的时间联系的学习上，虽然目前已经有这方面的研究，例如堆叠时间RBMs，以此为推广，有序列学习的dubbed temporal convolutionmachines，这种序列学习的应用，给语音信号处理问题带来了一个让人激动的未来研究方向。<br>目前，和DBN有关的研究包括堆叠自动编码器，它是通过用堆叠自动编码器来替换传统DBN里面的RBM。这就使得可以通过同样的规则来训练产生深度多层神经网络架构，但它缺少层的参数化的严格要求。与DBN不同，自动编码器使用判别模型，这样这个结构就很难采样输入采样空间，这就使得网络更难捕捉它的内部表达。但是，降噪自动编码器却能很好的避免这个问题，并且比传统的DBN更优。它通过在训练过程添加随机的污染并堆叠产生场泛化性能。训练单一的降噪自动编码器的过程和RBM训练生成模型的过程一样。</p><h2 id="5-更多资料"><a href="#5-更多资料" class="headerlink" title="5.更多资料"></a>5.更多资料</h2><ol><li><a href="http://blog.csdn.net/xbinworld/article/details/44901865" target="_blank" rel="noopener">深度学习方法：受限玻尔兹曼机RBM（一）基本概念</a></li><li><a href="http://blog.csdn.net/xbinworld/article/details/45013825" target="_blank" rel="noopener">深度学习方法：受限玻尔兹曼机RBM（二）网络模型</a></li><li><a href="http://blog.csdn.net/xbinworld/article/details/45128733" target="_blank" rel="noopener">深度学习方法：受限玻尔兹曼机RBM（三）模型求解，Gibbs sampling</a></li><li><a href="http://blog.csdn.net/xbinworld/article/details/45274289" target="_blank" rel="noopener">深度学习方法：受限玻尔兹曼机RBM（四）对比散度contrastive divergence，C</a></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/DBN/%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA%EF%BC%88RBM%EF%BC%89&amp;%E6%B7%B1%E5%BA%A6%E4%BF%A1%E5%BF%B5%E7%BD%91%E7%BB%9C%EF%BC%88DBN%EF%BC%89/"/>
      <url>/2019/06/15/DeepLearning/DBN/%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA%EF%BC%88RBM%EF%BC%89&amp;%E6%B7%B1%E5%BA%A6%E4%BF%A1%E5%BF%B5%E7%BD%91%E7%BB%9C%EF%BC%88DBN%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="受限玻尔兹曼机（RBM）-amp-深度信念网络（DBN）"><a href="#受限玻尔兹曼机（RBM）-amp-深度信念网络（DBN）" class="headerlink" title="受限玻尔兹曼机（RBM）&amp;深度信念网络（DBN）"></a>受限玻尔兹曼机（RBM）&amp;深度信念网络（DBN）</h1><p>标签（空格分隔）： 深度学习</p><hr><p>DBN调用：<a href="http://blog.csdn.net/m0_37924639/article/details/78962912" target="_blank" rel="noopener">http://blog.csdn.net/m0_37924639/article/details/78962912</a><br>DBN解释：<a href="http://blog.csdn.net/Losteng/article/details/51001247" target="_blank" rel="noopener">http://blog.csdn.net/Losteng/article/details/51001247</a><br>DBN实现：<a href="https://github.com/albertbup/deep-belief-network" target="_blank" rel="noopener">https://github.com/albertbup/deep-belief-network</a></p><p><a href="https://blog.csdn.net/mytestmy/article/details/9150213/" target="_blank" rel="noopener">RBM原理</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/DBN/Multi-step%20Time%20Series%20Forcasting/"/>
      <url>/2019/06/15/DeepLearning/DBN/Multi-step%20Time%20Series%20Forcasting/</url>
      
        <content type="html"><![CDATA[<h1 id="Multi-step-Time-Series-Forcasting"><a href="#Multi-step-Time-Series-Forcasting" class="headerlink" title="Multi-step Time Series Forcasting"></a>Multi-step Time Series Forcasting</h1><hr><p>There are at least four commonly used strategies for making multi-step forecasts.<br>They are:</p><h2 id="1-Direct-Multi-step-Forecast-Strategy"><a href="#1-Direct-Multi-step-Forecast-Strategy" class="headerlink" title="1. Direct Multi-step Forecast Strategy"></a>1. Direct Multi-step Forecast Strategy</h2><p>Create seperate prediction model for each ppint. Add computational add maintenance burden and there is <strong>no dependencies</strong> between points.</p><pre><code>prediction(t+1) = model1(obs(t-1), obs(t-2), ..., obs(t-n))prediction(t+2) = model2(obs(t-1), obs(t-3), ..., obs(t-n))</code></pre><p><strong>Attention:</strong> This kind of method just changes the network parameters. The input data remains the same for the whole prediction process.</p><h2 id="2-Recursive-Multi-step-Forecast"><a href="#2-Recursive-Multi-step-Forecast" class="headerlink" title="2. Recursive Multi-step Forecast"></a>2. Recursive Multi-step Forecast</h2><p>The recursive strategy involves using a one-step model multiple times where the prediction for the prior time step is used as an input for making a prediction on the following time step.<br>Because predictions are used in place of observations, the recursive strategy <strong>allows prediction errors to accumulate</strong> such that performance can quickly degrade as the prediction time horizon increases.<br><strong>Attention:</strong> This kind of method just changes the input data. The network parameters remain the same for the whole prediction process.</p><pre><code>prediction(t+1) = model(obs(t-1), obs(t-2), ..., obs(t-n))prediction(t+2) = model(prediction(t+1), obs(t-1), ..., obs(t-n))</code></pre><h2 id="3-Direct-Recursive-Hybrid-Multi-step-Forecast-Strategies"><a href="#3-Direct-Recursive-Hybrid-Multi-step-Forecast-Strategies" class="headerlink" title="3. Direct-Recursive Hybrid Multi-step Forecast Strategies"></a>3. Direct-Recursive Hybrid Multi-step Forecast Strategies</h2><p>The direct and recursive strategies can be combined to offer the benefits of both methods.<br>For example, a separate model can be constructed for each time step to be predicted, but each model may use the predictions made by models at prior time steps as input values.<br>This model learn a different model for each point using known data as well as prediction data.</p><pre><code>prediction(t+1) = model1(obs(t-1), obs(t-2), ..., obs(t-n))prediction(t+2) = model2(prediction(t+1), obs(t-1), ..., obs(t-n))</code></pre><h2 id="4-Multiple-Output-Forecast-Strategy"><a href="#4-Multiple-Output-Forecast-Strategy" class="headerlink" title="4. Multiple Output Forecast Strategy"></a>4. Multiple Output Forecast Strategy</h2><p>The multiple output strategy involves developing one model that is capable of predicting the entire forecast sequence in a one-shot manner.<br>In the case of predicting the temperature for the next two days, we would develop one model and use it to predict the next two days as one operation.<br>Multiple output models are more complex as they can learn the dependence structure between inputs and outputs as well as between outputs.<br>Being more complex may mean that they are slower to train and require more data to avoid overfitting the problem.</p><pre><code>prediction(t+1), prediction(t+2) = model(obs(t-1), obs(t-2), ..., obs(t-n))</code></pre><h2 id="Personal-View"><a href="#Personal-View" class="headerlink" title="Personal View"></a>Personal View</h2><p>Among the four methods, actually the first three methods are one-step ahead method. They realizes multi-step prediction by changing eather training datasets or network parameters. The fourth method is a multi-step network from the network structure aspect.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://machinelearningmastery.com/multi-step-time-series-forecasting/" target="_blank" rel="noopener">https://machinelearningmastery.com/multi-step-time-series-forecasting/</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/"/>
      <url>/2019/06/15/DeepLearning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/</url>
      
        <content type="html"><![CDATA[<p>Tags:深度学习</p><hr><p>#深度学习参考资料</p><ol><li><p>lstm实现<br><a href="http://blog.csdn.net/mydear_11000/article/details/52414342###" target="_blank" rel="noopener">http://blog.csdn.net/mydear_11000/article/details/52414342###</a><br><a href="http://blog.csdn.net/u013082989/article/details/73693392" target="_blank" rel="noopener">http://blog.csdn.net/u013082989/article/details/73693392</a></p></li><li><p>RNN和LSTM基础<br><a href="http://blog.csdn.net/xingzhedai/article/details/53144126" target="_blank" rel="noopener">http://blog.csdn.net/xingzhedai/article/details/53144126</a><br><a href="https://www.jianshu.com/p/9dc9f41f0b29" target="_blank" rel="noopener">https://www.jianshu.com/p/9dc9f41f0b29</a><br><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a><br><a href="https://www.zybuluo.com/hanbingtao/note/476663" target="_blank" rel="noopener">https://www.zybuluo.com/hanbingtao/note/476663</a></p></li><li><p>tensorflow 常用算子<br><a href="https://www.cnblogs.com/wuzhitj/p/6431381.html" target="_blank" rel="noopener">https://www.cnblogs.com/wuzhitj/p/6431381.html</a></p></li><li><p>DBN算法实现<br><a href="http://blog.csdn.net/zhanglu_wind/article/details/78949020" target="_blank" rel="noopener">http://blog.csdn.net/zhanglu_wind/article/details/78949020</a></p></li><li><p>数据分析必读书目<br><a href="http://www.cnblogs.com/charlotte77/p/5381681.html" target="_blank" rel="noopener">http://www.cnblogs.com/charlotte77/p/5381681.html</a></p></li><li><p>深度神经网络系列文章</p><ol><li><a href="https://www.zybuluo.com/hanbingtao/note/433855" target="_blank" rel="noopener">零基础入门深度学习(1) - 感知器</a></li><li><a href="https://www.zybuluo.com/hanbingtao/note/448086" target="_blank" rel="noopener">零基础入门深度学习(2) - 线性单元和梯度下降</a></li><li><a href="https://www.zybuluo.com/hanbingtao/note/476663" target="_blank" rel="noopener">零基础入门深度学习(3) - 神经网络和反向传播算法</a></li><li><a href="https://www.zybuluo.com/hanbingtao/note/485480" target="_blank" rel="noopener">零基础入门深度学习(4) - 卷积神经网络</a></li><li><a href="https://www.zybuluo.com/hanbingtao/note/541458" target="_blank" rel="noopener">零基础入门深度学习(5) - 循环神经网络</a></li><li><a href="https://zybuluo.com/hanbingtao/note/581764" target="_blank" rel="noopener">零基础入门深度学习(6) - 长短时记忆网络(LSTM)</a></li><li><a href="https://www.zybuluo.com/hanbingtao/note/626300" target="_blank" rel="noopener">零基础入门深度学习(7) - 递归神经网络</a></li></ol></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84batch%20size/"/>
      <url>/2019/06/15/DeepLearning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84batch%20size/</url>
      
        <content type="html"><![CDATA[<p>转载自:<a href="http://blog.csdn.net/ycheng_sjtu/article/details/49804041#reply" target="_blank" rel="noopener">http://blog.csdn.net/ycheng_sjtu/article/details/49804041#reply</a></p><h1 id="谈谈深度学习中的-Batch-Size"><a href="#谈谈深度学习中的-Batch-Size" class="headerlink" title="谈谈深度学习中的 Batch_Size"></a>谈谈深度学习中的 Batch_Size</h1><p>Batch_Size（批尺寸）是机器学习中一个重要参数，涉及诸多矛盾，下面逐一展开。</p><h2 id="首先，为什么需要有-Batch-Size-这个参数？"><a href="#首先，为什么需要有-Batch-Size-这个参数？" class="headerlink" title="首先，为什么需要有 Batch_Size 这个参数？"></a>首先，为什么需要有 Batch_Size 这个参数？</h2><p>Batch 的选择，首先决定的是下降的方向。如果数据集比较小，完全可以采用<strong>全数据集 （ Full Batch Learning ）</strong>的形式，这样做<strong>至少</strong>有 2 个好处：其一，由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。其二，由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 Full Batch Learning 可以使用 <strong>Rprop</strong> 只基于梯度符号并且针对性单独更新各权值。</p><p>对于更大的数据集，以上 2 个好处又变成了 2 个坏处：其一，随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。其二，以 Rprop 的方式迭代，会由于各个 Batch 之间的采样差异性，各次梯度修正值相互抵消，无法修正。这才有了后来 <strong>RMSProp</strong> 的妥协方案。</p><h2 id="既然-Full-Batch-Learning-并不适用大数据集，那么走向另一个极端怎么样？"><a href="#既然-Full-Batch-Learning-并不适用大数据集，那么走向另一个极端怎么样？" class="headerlink" title="既然 Full Batch Learning 并不适用大数据集，那么走向另一个极端怎么样？"></a>既然 Full Batch Learning 并不适用大数据集，那么走向另一个极端怎么样？</h2><p>所谓另一个极端，就是每次只训练一个样本，即 Batch_Size = 1。这就是<strong>在线学习（Online Learning）</strong>。线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元、非线性网络，在局部依然近似是抛物面。使用在线学习，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，<strong>难以达到收敛</strong>。<strong>如图所示</strong>：</p><p><img src="http://img.blog.csdn.net/20151112195814221" alt="这里写图片描述"> </p><h2 id="可不可以选择一个适中的-Batch-Size-值呢？"><a href="#可不可以选择一个适中的-Batch-Size-值呢？" class="headerlink" title="可不可以选择一个适中的 Batch_Size 值呢？"></a>可不可以选择一个适中的 Batch_Size 值呢？</h2><p>当然可以，这就是<strong>批梯度下降法（Mini-batches Learning）</strong>。因为如果数据集足够充分，那么用一半（<strong>甚至少得多</strong>）的数据训练算出来的梯度与用全部数据训练出来的梯度是<strong>几乎一样</strong>的。</p><h2 id="在合理范围内，增大-Batch-Size-有何好处？"><a href="#在合理范围内，增大-Batch-Size-有何好处？" class="headerlink" title="在合理范围内，增大 Batch_Size 有何好处？"></a>在合理范围内，增大 Batch_Size 有何好处？</h2><ul><li>内存利用率提高了，大矩阵乘法的并行化效率提高。</li><li>跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。</li><li>在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。</li></ul><h2 id="盲目增大-Batch-Size-有何坏处？"><a href="#盲目增大-Batch-Size-有何坏处？" class="headerlink" title="盲目增大 Batch_Size 有何坏处？"></a>盲目增大 Batch_Size 有何坏处？</h2><ul><li>内存利用率提高了，但是内存容量可能撑不住了。</li><li>跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。</li><li>Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。</li></ul><h2 id="调节-Batch-Size-对训练效果影响到底如何？"><a href="#调节-Batch-Size-对训练效果影响到底如何？" class="headerlink" title="调节 Batch_Size 对训练效果影响到底如何？"></a>调节 Batch_Size 对训练效果影响到底如何？</h2><p>这里跑一个 LeNet 在 MNIST 数据集上的效果。MNIST 是一个手写体标准库，我使用的是 <strong>Theano</strong> 框架。这是一个 Python 的深度学习库。<a href="http://deeplearning.net/software/theano/install.html#install" target="_blank" rel="noopener">安装方便</a>（几行命令而已），调试简单（自带 Profile），GPU / CPU 通吃，<a href="http://deeplearning.net/tutorial/contents.html" target="_blank" rel="noopener">官方教程相当完备</a>，支持模块十分丰富（除了 CNNs，更是支持 RBM / DBN / LSTM / RBM-RNN / SdA / MLPs）。在其上层有 <a href="http://keras.io/" target="_blank" rel="noopener">Keras</a> 封装，支持 GRU / JZS1, JZS2, JZS3 等较新结构，支持 Adagrad / Adadelta / RMSprop / Adam 等优化算法。<strong>如图所示</strong>：</p><p><img src="http://img.blog.csdn.net/20151112195829489" alt="这里写图片描述"></p><p><img src="http://img.blog.csdn.net/20151112195843957" alt="这里写图片描述"> </p><p>运行结果如上图所示，其中绝对时间做了标幺化处理。运行结果与上文分析相印证：</p><ul><li>Batch_Size 太小，算法在 200 epoches 内不收敛。</li><li>随着 Batch_Size 增大，处理相同数据量的速度越快。</li><li>随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。</li><li>由于上述两种因素的矛盾， Batch_Size 增大到<strong>某个</strong>时候，达到<strong>时间上</strong>的最优。</li><li>由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到<strong>某些</strong>时候，达到最终收敛<strong>精度上</strong>的最优。</li></ul><h2 id="关于batchsize和epoch的理解"><a href="#关于batchsize和epoch的理解" class="headerlink" title="关于batchsize和epoch的理解"></a>关于batchsize和epoch的理解</h2><p>batchsize指的是使超参数发生一次迭代输入的样本数，也就是完成一次反向传播、求导、参数更新过程所需要输入的样本数。epoch指完成将所有训练数据全部应用于参数更新的过程。在样本数据比较小时，batchsize可以取为训练样本数，这样完成一次epoch只需进行一次梯度下降和参数更新。将全部输入数据作为一个batch进行训练的方法称为<strong>批梯度下降法（Batch Gradient Descend）</strong>。与之相对应的是<strong>随机梯度下降法（Stochastic Gradient Descent）</strong>。这种梯度下降法每次只选取一个样本进行参数更新，因此完成一次epoch需要进行size(X)次迭代。这种方法梯度下降的方向一般不是最优的。将这两种方法折中的一种方法称为<strong>小批量梯度下降法（Mini-batch Gradient Descent）</strong>。这种方法每次将<strong>batchsize</strong>的数据用来训练网络，完成一次迭代。那么完成一次epoch需要进行的迭代次数为<code>np.ceil(size(x) / batchsize)</code>。在实际使用时，可以在一定范围内尽可能增大batchsize的值。当然，如果增大batch值，达到相同精度的epoch数会越来越多，因此需要在精度和运算速度这两方面来考虑选取的batchsize。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/DeepLearning/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"/>
      <url>/2019/06/15/DeepLearning/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/</url>
      
        <content type="html"><![CDATA[<h1 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h1><p>标签（空格分隔）： 机器学习</p><hr><p>#1.目的<br>有时候，直接估计条件概率密度函数很难，因此将概率密度估计问题转化为参数估计问题，极大似然法就是一种参数估计问题。</p><p>#2.重要前提<br>训练样本的分布能代表样本的<strong>真实分布</strong>。每个样本集中的样本都是所谓独立同分布的随机变量 (iid条件)，且有充分的训练样本。</p><p>#3.基本内容<br>极大似然估计的基本内容是：利用已知的样本结果，反推最有可能导致这样结果的参数值。似然函数指联合概率密度函数关于每个样本以及需要估计的参数的函数。一般形式为：<br>$$L(\theta) = f(x_{1}, x_{2}, … ,x_{n};\theta)$$</p><p>#4.一般步骤<br>求最大似然估计量的一般步骤：</p><ol><li>由总体分布导出样本的联合概率函数（或联合密度）；</li><li>把样本联合概率函数的自变量看成是已知常数，而把θθ看做是自变量，得到似然函数$L(\theta)$;</li><li>求似然函数的最大值（常常取对数，然后求驻点）；</li><li>用样本值带入得到参数的最大似然估计<br>最大似然估计的特点：</li><li>比其他估计方法更加简单；</li><li>收敛性：无偏或者渐近无偏，当样本数目增加时，收敛性质会更好；</li><li>如果假设的类条件概率模型正确，则通常能获得较好的结果。但如果假设模型出现偏差，将导致非常差的估计结果。</li></ol><p>#5.最大似然估计(MLE)和最小二乘法估计(LSE)的区别<br>最小二乘法估计的目的是让估计量最好地拟合样本数据，也就是让估计值与样本值之差的平方和最小<br>$$Q = \sum_{i=1}^{n}(Y_{i} - \hat{Y}_{i})^{2}$$</p><p>#参考<br><a href="http://blog.csdn.net/zengxiantao1994/article/details/72787849" target="_blank" rel="noopener">极大似然估计详解</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/Pytorch/Pytorch%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
      <url>/2019/06/15/Pytorch/Pytorch%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="Pytorch使用过程中遇到的问题"><a href="#Pytorch使用过程中遇到的问题" class="headerlink" title="Pytorch使用过程中遇到的问题"></a>Pytorch使用过程中遇到的问题</h1><h2 id="tensor-to-device-和module-to-device"><a href="#tensor-to-device-和module-to-device" class="headerlink" title="tensor.to(device)和module.to(device)"></a><code>tensor.to(device)</code>和<code>module.to(device)</code></h2><p><code>tensor.to(device)</code>不是inplace operation。因此在给一个tensor指定device时需要用：<code>tensor1 = tensor1.to(device)</code>。而<code>Module.to(device)</code>是inplace operation，因此直接使用<code>Module.to(device)</code>即可。</p><h2 id="Numpy和Tensor的数据转换"><a href="#Numpy和Tensor的数据转换" class="headerlink" title="Numpy和Tensor的数据转换"></a>Numpy和Tensor的数据转换</h2><p>在将Numpy的数据转化为Tensor时，需要注意一个问题。Numpy中默认浮点类型是<code>np.float64</code>，Pytorch中默认浮点类型是<code>torch.float32</code>。直接使用<code>torch.tensor(array)</code>得到的会是<code>torch.double</code>类型的数据。如果想得到<code>torch.float</code>类型的数据，需要对Numpy数据进行类型转换：<code>array = array.astype(np.float32)</code></p><h2 id="‘bool-value-of-Tensor-with-more-than-one-value-is-ambiguous’"><a href="#‘bool-value-of-Tensor-with-more-than-one-value-is-ambiguous’" class="headerlink" title="‘bool value of Tensor with more than one value is ambiguous’"></a>‘bool value of Tensor with more than one value is ambiguous’</h2><p>这种情况可能是损失函数声明时没有加括号。即应该是<code>loss_function=nn.MSELoss()</code>而不是<code>loss_function=nn.MSELoss</code></p><h2 id="Dataloader的数据格式"><a href="#Dataloader的数据格式" class="headerlink" title="Dataloader的数据格式"></a>Dataloader的数据格式</h2><p>在使用Dataloader导入训练数据时，会使用<code>for batch in dataloader</code>，<code>batch</code>实际上是一个包含所有data的列表。即使只有一组数据，也必须使用<code>batch[0]</code>对数据进行提取。</p><h2 id="在训练过程中loss不下降"><a href="#在训练过程中loss不下降" class="headerlink" title="在训练过程中loss不下降"></a>在训练过程中loss不下降</h2><p>一种可能原因是在计算<code>loss = torch.nn.functional.mseloss(output, real)</code>时，output和real的维数不完全一致。</p><h2 id="matplotlib和pytorch同时导入报错"><a href="#matplotlib和pytorch同时导入报错" class="headerlink" title="matplotlib和pytorch同时导入报错"></a>matplotlib和pytorch同时导入报错</h2><p>在同时导入matplotlib和pytorch.optim时报错：<code>Process finished with exit code -1073740791 (0xC0000409)</code>。解决方法：先导入pytorch再导入matplotlib，如下：</p><pre><code class="python">import torch.optim as optimport matplotlib.pyplot as plt</code></pre><h2 id="Pytorch显存不足"><a href="#Pytorch显存不足" class="headerlink" title="Pytorch显存不足"></a>Pytorch显存不足</h2><p>在使用Pytorch时，有时可能遇到显存不足的问题。即<code>RuntimeError: CUDA out of memory. Tried to allocate 11.88 MiB (GPU 0; 6.00 GiB total capacity; 4.52 GiB already allocated; 11.60 MiB free; 758.50 KiB cached)</code>。出现这种问题的原因可能是在保存结果时将过大的张量保存在GPU中，导致GPU存储空间不足。解决方法：检查所有存储在GPU中的张量，将其中一些较大的用<code>tensor.to(&#39;cpu&#39;)</code>转到内存中储存。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/Pytorch/Pytorch%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/06/15/Pytorch/Pytorch%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="Pytorch学习"><a href="#Pytorch学习" class="headerlink" title="Pytorch学习"></a>Pytorch学习</h1><h2 id="torch-nn和torch-nn-functional中神经网络层的区别"><a href="#torch-nn和torch-nn-functional中神经网络层的区别" class="headerlink" title="torch.nn和torch.nn.functional中神经网络层的区别"></a>torch.nn和torch.nn.functional中神经网络层的区别</h2><p>torch.nn中的层是类，torch.nn.functional中的层是函数。torch.nn中的forward()方法是调用torch.nn.functional实现。因此两者从原理上等价。torch.nn一般用于较复杂层的实现，torch.nn.functional一般用于简单层的实现</p><h2 id="Pytorch的层建立方式"><a href="#Pytorch的层建立方式" class="headerlink" title="Pytorch的层建立方式"></a>Pytorch的层建立方式</h2><ol><li><p>nn.Sequential().add_module(layer)</p><pre><code class="python">net1 = nn.Sequential()net1.add_module(&#39;conv&#39;, nn.Conv2d(3, 3, 3))net1.add_module(&#39;batchnorm&#39;, nn.BatchNorm2d(3))net1.add_module(&#39;activation_layer&#39;, nn.ReLU())</code></pre></li><li><p>nn.Sequential(layer)</p><pre><code class="python">net2 = nn.Sequential( nn.Conv2d(3, 3, 3), nn.BatchNorm2d(3), nn.ReLU() )</code></pre></li><li><p>nn.Sequential(OrderedDict([(multi layername, layer)]))</p><pre><code class="python">from collections import OrderedDictnet3 = nn.Sequential(OrderedDict([ (&#39;conv&#39;, nn.Conv2d(3, 3, 3)), (&#39;batchnorm&#39;, nn.BatchNorm2d(3)), (&#39;activation_layer&#39;, nn.ReLU())]))</code></pre></li></ol><pre><code>4. nn.ModuleList([layers])```pythonmodel1 = nn.ModuleList(    nn.Linear(10, 1) for _ in range(3))</code></pre><h2 id="Pytorch自定义层的编写"><a href="#Pytorch自定义层的编写" class="headerlink" title="Pytorch自定义层的编写"></a>Pytorch自定义层的编写</h2><p>下面是一个典型的Pytorch自定义层的实现方法</p><pre><code class="python">class ScaledDotProductAttention(nn.Module):    &quot;&quot;&quot; Scaled Dot-Product Attention &quot;&quot;&quot;    def __init__(self, temperature, attn_dropout=0.1):        &quot;&quot;&quot;        :param temperature: scale parameter in the equation            out = Q * K.T / temperature * V. Default is \sqrt d_k        :param attn_dropout: dropout rate in the self-attention            layer        &quot;&quot;&quot;        super(ScaledDotProductAttention, self).__init__()        self.temperature = temperature        self.dropout = nn.Dropout(attn_dropout)        self.softmax = nn.Softmax(dim=2)    def forward(self, q, k, v, mask=None):        &quot;&quot;&quot;        Calculate self-attention output        :param q: size: (batch_size, max_seq_len or input_len, d_k)        :param k: same as q        :param v: size: (batch_size, max_seq_len or input_len, d_k)        :param mask:        :return:        &quot;&quot;&quot;        attn = torch.bmm(q, k.transpose(1, 2))        attn = attn / self.temperature        if mask is not None:            attn = attn.masked_fill(mask, -np.inf)        attn = self.softmax(attn)        attn = self.dropout(attn)        output = torch.bmm(attn, v)        return output, attn</code></pre><p>在<code>self.__init__()</code>函数中，完成对层中要使用的Tensor和调用的层的产生和初始化。初始化可以直接写在<code>__init__()</code>方法中，也可以单独创建一个<code>self.reset_parameters()</code>方法，在<code>__init__()</code>方法中调用。初始化可以自定义，也可以使用<code>torch.nn.init</code>中提供的初始化方法。在该方法中调用的层只是对层的声明，并不是调用。</p><p>在<code>self.forward()</code>方法中，完成对调用该层时完成的功能的编写。层的输出写在return行。</p><h2 id="Pytorch中常见的层"><a href="#Pytorch中常见的层" class="headerlink" title="Pytorch中常见的层"></a>Pytorch中常见的层</h2><ol><li><p>LayerNorm<br>LayerNorm层是对数据的最后一维进行归一化。多用在深层RNN中。</p></li><li><p>Conv1d</p><pre><code class="python">Conv1dclass torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</code></pre><p>in_channels是输入数据第二维，out_channels是输出数据第二维，kernel_size是卷积占的列数。<br>输入：shape为(a, b, c)的Tensor。Conv1d在最后一维做卷积，卷积核的维数是(in_channels, out_channels, kernel_size)。输出的第二维是out_channels，说明用out_channels个不同的卷积做运算，输出第三维是c - kernel_size + 1,表示共卷积的次数。</p><pre><code class="python"># 一个Conv1d的例子m = nn.Conv1d(16, 33, 3, stride=2)input = torch.randn(20, 16, 50)output = m(input)print(output.shape)print(m.weight.shape)</code></pre><p>输出为torch.Size([20, 33, 48])和torch.Size([33, 16, 3])。原理为：Conv1d只在最后一维做卷积。第二维由in_channels变为out_channels，说明共有out_channels组卷积核，1组in_channels个，卷积核宽度为3，每个卷积核分别与输入数据第二维中的一行做卷积。</p></li><li><p>Conv2d</p><pre><code class="python">CLASS torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)# 一个Conv2d的例子：m = nn.Conv2d(16, 33, (3, 4))input = torch.randn(20, 16, 50, 100)output = m(input)print(output.size)print(m.weight.size)</code></pre><p>输出为torch.Size([16, 33, 48, 97])和torch.Size([33, 16, 3, 4])。原理为：Conv2d的in_channels和out_channels与Conv1d保持一致，不同的是kernel_size可以是两维的，也就是同时对input的最后两维进行卷积。其他部分与Conv1d相同。因此由Conv1d不难推出Conv2d的维数变换规律。</p></li></ol><h2 id="Pytorch中的数据处理"><a href="#Pytorch中的数据处理" class="headerlink" title="Pytorch中的数据处理"></a>Pytorch中的数据处理</h2><p>reference:<a href="https://zhuanlan.zhihu.com/p/30934236" target="_blank" rel="noopener">Pytorch数据读取(Dataset, DataLoader, DataLoaderIter)</a></p><h3 id="torch-utils-data-Dataset"><a href="#torch-utils-data-Dataset" class="headerlink" title="torch.utils.data.Dataset"></a><code>torch.utils.data.Dataset</code></h3><p>reference: <a href="http://pytorch.apachecn.org/cn/docs/0.3.0/data.html?highlight=dataloader#torch.utils.data.Dataset" target="_blank" rel="noopener">Pytorch cn doc</a><br>Dataset是一个抽象类，用于将数据封装成Dataset类。它是一个抽象类。在具体使用时需要继承Dataset类并实现其中的2个方法：</p><ul><li><code>__getitem__(self, index)</code><br> 用于决定每次如何取数据。比如对于形如(batch_size, input_length)的数据，每次读取第index行数据</li><li><code>__len__()</code><br> 用于获取数据的长度</li></ul><pre><code class="python">class DealDataset(Dataset):    &quot;&quot;&quot;        下载数据、初始5. Pytorch中的数据处理数据，都可以在这里完成    &quot;&quot;&quot;    def __init__(self):        xy = np.loadtxt(&#39;../dataSet/diabetes.csv.gz&#39;, delimiter=&#39;,&#39;, dtype=np.float32) # 使用numpy读取数据        self.x_data = torch.from_numpy(xy[:, 0:-1])        self.y_data = torch.from_numpy(xy[:, [-1]])        self.len = xy.shape[0]    def __getitem__(self, index):        return self.x_data[index], self.y_data[index]    def __len__(self):        return self.len</code></pre><h3 id="torch-utils-data-DataLoader"><a href="#torch-utils-data-DataLoader" class="headerlink" title="torch.utils.data.DataLoader"></a><code>torch.utils.data.DataLoader</code></h3><p>reference:<a href="http://pytorch.apachecn.org/cn/docs/0.3.0/data.html?highlight=dataloader#torch.utils.data.DataLoader" target="_blank" rel="noopener">Pytorch cn doc</a></p><p>用于定义从Dataset中读取数据的方式，包括batch_size, shuffle等</p><pre><code class="python税务总局发票">class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=&lt;function default_collate at 0x4316c08&gt;, pin_memory=False, drop_last=False)</code></pre><p>主要参数：</p><ul><li>dataset: dataset对象</li><li>batch_size: 每个 batch 加载多少个样本 (默认值: 1)</li><li>shuffle: 设置为 True 时, 会在每个 epoch 重新打乱数据 (默认值: False).</li></ul><pre><code class="python">train_loader = DataLoader(dataset=dealDataset, batch_size=32, shuffle=True)</code></pre><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><pre><code class="python">for epoch in epochs:    for i, batch in enumerate(train_loader):    print(&#39;the {}th batch: {}&#39;.format(i, batch))</code></pre><p><strong>注意:</strong>如果在dataloader中定义了多个返回值，那么在训练过程中每个batch都是一个list，使用batch[i]或者(train, test)来调用每个batch中的参数</p><h2 id="Pytorch中的数据类型及相互转换"><a href="#Pytorch中的数据类型及相互转换" class="headerlink" title="Pytorch中的数据类型及相互转换"></a>Pytorch中的数据类型及相互转换</h2><p>Pytorch中dtype是tensor的一个属性，使用<code>tensor.dtype</code>获取一个张量的数据类型。主要包括以下几类：</p><table><thead><tr><th>Data type</th><th>dtype</th><th>Tensor types</th></tr></thead><tbody><tr><td>32-bit floating point</td><td><code>torch.float32</code> or <code>torch.float</code></td><td><code>torch.*.FloatTensor</code></td></tr><tr><td>64-bit floating point</td><td><code>torch.float64</code> or <code>torch.double</code></td><td><code>torch.*.DoubleTensor</code></td></tr><tr><td>16-bit floating point</td><td><code>torch.float16</code> or <code>torch.half</code></td><td><code>torch.*.HalfTensor</code></td></tr><tr><td>8-bit integer (unsigned)</td><td><code>torch.uint8</code></td><td><code>torch.*.ByteTensor</code></td></tr><tr><td>8-bit integer (signed)</td><td><code>torch.int8</code></td><td><code>torch.*.CharTensor</code></td></tr><tr><td>16-bit integer (signed)</td><td><code>torch.int16</code> or <code>torch.short</code></td><td><code>torch.*.ShortTensor</code></td></tr><tr><td>32-bit integer (signed)</td><td><code>torch.int32</code> or <code>torch.int</code></td><td><code>torch.*.IntTensor</code></td></tr><tr><td>64-bit integer (signed)</td><td><code>torch.int64</code> or <code>torch.long</code></td><td><code>torch.*.LongTensor</code></td></tr></tbody></table><p>数据类型的查看：数据类型之间的转换使用<code>Tensor.long()</code>或<code>dtype=torch.long</code>实现。转换函数有：<code>long(), int(), double(), float(), byte()</code></p><p><strong>注意：</strong> </p><ol><li>Pytorch中的一些层对输入的tensor类型有要求。如Embedding层要求输入的tensor为<code>torch.long</code>类型。</li><li>如果Pytorch的数据来源是numpy，要十分注意numpy和pytorch的数据类型匹配。在numpy中，默认数据类型是<code>float</code>，但<code>float</code>与<code>np.float64</code>等价；在pytorch中，整数默认数据类型是<code>torch.long</code>，小数默认数据类型是<code>torch.float</code>，但<code>float</code>与<code>torch.float32</code>等价。也就是说，如果不加转换地使用<code>torch.from_numpy</code>，numpy中的数组将会被转换成pytorch中的<code>torch.double</code>类型。数据类型的不匹配将造成网络无法正确搭建。解决方法：在numpy端将数据转换为<code>np.float32</code>类型，即<code>.astype(np.float32)</code><pre><code class="python">a = np.sin([i for i in range(10)]).astype(np.float)print(a.dtype)</code></pre></li></ol><h2 id="Pytorch中的数据运算位置及相互转换"><a href="#Pytorch中的数据运算位置及相互转换" class="headerlink" title="Pytorch中的数据运算位置及相互转换"></a>Pytorch中的数据运算位置及相互转换</h2><p>Pytorch中device是tensor的一个属性，使用<code>tensor.device</code>获取一个张量的运算位置。</p><ul><li>从cpu转换到gpu：<code>tensor.to(&#39;cuda&#39;)</code>或<code>tensor.cuda()</code></li><li>从gpu转换到cpu：<code>tensor.to(&#39;cpu&#39;)</code>或<code>tensor.cpu()</code></li></ul><p>此外，还要注意tensor和numpy数组之间的转换只能在cpu上完成。即要先使用<code>tensor.to(&#39;cpu&#39;)</code>后才能使用<code>tensor.numpy()</code></p><h2 id="Pytorch展示模型结构"><a href="#Pytorch展示模型结构" class="headerlink" title="Pytorch展示模型结构"></a>Pytorch展示模型结构</h2><ol><li>展示模型所有层：<code>print(modelname)</code></li><li>展示模型所有参数：<code>print(list(model.named_parameters()))</code></li></ol><h2 id="Pytorch学习率调整"><a href="#Pytorch学习率调整" class="headerlink" title="Pytorch学习率调整"></a>Pytorch学习率调整</h2><p>使用的类：torch.optim.lr_scheduler。这个类的optimizer为常用的优化方法。如果使用scheduler，则在训练过程中只需写scheduler.step()而不需写optimizer.step()。</p><p>常用的学习率调整方法：</p><ol><li><p><code>torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)</code> </p><p>每过step_size将learning_rate调整为gamma * learning_rate.</p></li><li><p><code>torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1)</code></p><p>每到milestones中包含的训练次数时把learning_rate调整为gamma * learning_rate.</p><p>milestones:包含迭代次数的列表，必须递增。</p></li><li><p><code>torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=&#39;min&#39;, factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode=&#39;rel&#39;, cooldown=0, min_lr=0, eps=1e-08)</code></p><p>学习率递减函数。当模型性能不再提升时将学习率减少。</p><p>mode: ‘min’或’max’。在’min’模式下，衡量标准不再下降时学习率减小。</p><p>factor: 学习率更新系数</p><p>patience: 学习率更新前指标不再下降/上升的迭代次数</p><p>verbose: 每次更新学习率时是否打印信息</p></li></ol><h2 id="Pytorch设置随机数种子"><a href="#Pytorch设置随机数种子" class="headerlink" title="Pytorch设置随机数种子"></a>Pytorch设置随机数种子</h2><pre><code class="python">torch.manual_seed(args.seed)torch.cuda.manual_seed(args.seed)</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/Pytorch/Pytorch%E5%85%A5%E9%97%A8/"/>
      <url>/2019/06/15/Pytorch/Pytorch%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="Pytorch教程"><a href="#Pytorch教程" class="headerlink" title="Pytorch教程"></a>Pytorch教程</h2><ol><li><a href="http://pytorch.apachecn.org/cn/docs/0.3.0/" target="_blank" rel="noopener">Pytorch中文文档</a></li><li><a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">Pytorch英文文档</a></li><li><a href="http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener">英文版：http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html</a></li><li><a href="https://www.jianshu.com/p/889dbc684622" target="_blank" rel="noopener">中文版：https://www.jianshu.com/p/889dbc684622</a></li><li><a href="https://zhuanlan.zhihu.com/p/28475866" target="_blank" rel="noopener">Pytorch github项目整理</a></li></ol><h2 id="Pytorch安装"><a href="#Pytorch安装" class="headerlink" title="Pytorch安装"></a>Pytorch安装</h2><ol><li>建议使用Anaconda安装。清华大学anaconda pytorch地址： <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</a></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/Pytorch/Pytorch%E4%B8%AD%E5%B8%B8%E7%94%A8tensor%E6%93%8D%E4%BD%9C/"/>
      <url>/2019/06/15/Pytorch/Pytorch%E4%B8%AD%E5%B8%B8%E7%94%A8tensor%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="1-torch-cat"><a href="#1-torch-cat" class="headerlink" title="1 torch.cat"></a>1 torch.cat</h1><p>cat指的是对多个Tensor在<strong>原有某一维度</strong>进行拼接，拼接的结果是Tensor的总维数不变，其中用于拼接的那一维等于各分量维数之和。示例：</p><pre><code class="python">x = torch.rand(2, 3)y = torch.rand(4, 3)z = torch.cat((x, y), 0)</code></pre><p>即x和y沿着axis=0的维度进行拼接，得到的结果是一个(6*3)的Tensor<br>此外，还有如下用法：</p><ol><li><code>torch.cat((x, x), 0)</code></li><li><code>torch.cat([torch.rand(3, 4), torch.rand(5, 4)], dim=0)</code> </li></ol><h1 id="2-torch-chunk"><a href="#2-torch-chunk" class="headerlink" title="2 torch.chunk"></a>2 torch.chunk</h1><p><code>torch.chunk(tensor, chunks, dim=0)</code><br>chunk可以看成cat的逆操作，即将一个矩阵沿着某一维分割开。chunks为分割的份数， dim为分割的维度。例子：</p><pre><code class="python">x = torch.zeros(2, 3, 4)y = torch.chunk(x, 3, 1)</code></pre><p>得到的y是一个tuple，每一个的维度为(2, 1, 4)<br>还可使用Tensor.chunks(chunks, dim=0)效果与上相同。</p><h1 id="3-torch-stack"><a href="#3-torch-stack" class="headerlink" title="3 torch.stack"></a>3 torch.stack</h1><p>stack指的是在<strong>新的维度上</strong>进行拼接，这个操作会增加维度。示例：</p><pre><code class="python">x = torch.ones(1, 3)y = torch.ones(1, 3)z1 = torch.stack((x, y), 0)z2 = torch.stack((x, y), 1)z3 = torch.stack((x, y), 2)</code></pre><p>输出维数规律：除拼接的维数外别的维数保持不变，拼接的维数等于输入个数之和。则上述三个输出的维数分别为：<br>z1:(2, 1, 3), z2:(1, 2, 3), z3:(1, 3, 2)。<br><strong>注意：</strong> torch.stack的输入tensor的维数必须一致，这样才能保证在能够在新的维度进行拼接操作。</p><h1 id="4-torch-transpose"><a href="#4-torch-transpose" class="headerlink" title="4 torch.transpose"></a>4 torch.transpose</h1><p>transpose指的是将Tensor的某两个维度进行交换。示例：</p><pre><code class="python">x = torch.zeros(2, 3)y = torch.transpose(x, 0, 1)</code></pre><h1 id="5-permute-amp-reshape"><a href="#5-permute-amp-reshape" class="headerlink" title="5 permute &amp; reshape"></a>5 permute &amp; reshape</h1><p>permute是适合于多维度的维数交换。使用方法：输入希望产生的维度即可。例子：</p><pre><code class="python">x = torch.zeros(2, 3, 4)y = x.permute(2, 1, 0)</code></pre><p>输出y的维度为:(4, 3, 2)</p><p>reshape和permute功能类似，不过reshape一般用于连续维度的改变，如</p><pre><code class="python">x = torch.zeros(2, 3, 4)y = x.reshape(2, 6, 2)</code></pre><p>如果用于reshape的维度不是连续的，会出现数据改变的情况。</p><h1 id="6-squeeze"><a href="#6-squeeze" class="headerlink" title="6 squeeze"></a>6 squeeze</h1><p>squeeze是将某一个维度为1的维去除。使用方法：x.squeeze()。<br>例子：</p><pre><code class="python">x = torch.zeros(3, 1, 2)y = x.squeeze(1)</code></pre><p>得到的y的维度为(3, 2)。如果选择的维数不为1，那么得到的结果的维数与原Tensor的维数一致</p><h1 id="7-unsqueeze"><a href="#7-unsqueeze" class="headerlink" title="7 unsqueeze"></a>7 unsqueeze</h1><p>unsqueeze是增加一个维度，维度位置为dim。使用方法：a.unsqueeze(dim)。<br>例子：</p><pre><code class="python">x = torch.zeros(3, 2)y = x.unsqueeze(2)</code></pre><p>得到的y的维数为：(3, 2, 1)<br>use repeat – this will copy each vector 28 times.</p><p>X = torch.randn(100, 700)<br>X = X.unsqueeze(2).repeat(1, 1, 28)</p><h1 id="8-torch-masked-fill"><a href="#8-torch-masked-fill" class="headerlink" title="8 torch.masked_fill"></a>8 torch.masked_fill</h1><p>masked_fill将一个tensor中为1的元素用指定的值填充。例如：</p><pre><code class="python">a = torch.ones(3, 3)</code></pre><h1 id="9-torch-view"><a href="#9-torch-view" class="headerlink" title="9 torch.view"></a>9 torch.view</h1><p>view将一个tensor变换维度，但其中的数值保持不变</p><h1 id="10-torch-bmm"><a href="#10-torch-bmm" class="headerlink" title="10 torch.bmm"></a>10 torch.bmm</h1><p>bmm即batch_matmul，作用是不考虑batch维度将两个矩阵相乘。</p><pre><code class="python">a = torch.ones(128, 4, 3)b = torch.ones(128, 3, 10)result = torch.bmm(a, b)print(result.shape)&gt;&gt;&gt; (128, 4, 10)</code></pre><h1 id="11-expand-amp-repeat"><a href="#11-expand-amp-repeat" class="headerlink" title="11 expand&amp;repeat"></a>11 expand&amp;repeat</h1><p>expand和repeat都用于扩展Tensor的维度。<strong>使用前提：原矩阵的维度和扩展后矩阵后的维度一致。因此通常先进行squeeze(dim)或unsqueeze(dim)操作</strong>。expand的输入参数是扩展后Tensor的维度，repeat的输入参数是扩展后Tensor相对于原Tensor扩展的倍数。<strong>此外，注意expand仅限于维数为1的扩展，否则会报类型不匹配错误。</strong> 例如：</p><pre><code class="python">a = torch.Tensor([1, 2, 3])# 最终维数为(3,12 )b = a.unsqueeze(1).expand(3, 5)# 13 在第一维扩展1次，在第二维扩展5次c = a.unsqueeze(1).repeat(1, 5)# 14 output:tensor([[1., 1., 1., 1., 1.],        [2., 2., 2., 2., 2.],        [3., 3., 3., 3., 3.]])# RuntimeError: The expanded size of the tensor (6) must match the existing size (3) at non-singleton dimension15 Target sizes: [1,15 ,15 ].  Tensor sizes: [1,15 ,15 ]a.expand(1, 4, 6)</code></pre><p>expand不会复制数组内存，节省空间。repeat会复制所有数据</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/CProgrammingLanguage/%E8%81%94%E5%90%88%E4%BD%93%EF%BC%88Union%EF%BC%89/"/>
      <url>/2019/06/15/CProgrammingLanguage/%E8%81%94%E5%90%88%E4%BD%93%EF%BC%88Union%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="联合体（Union）"><a href="#联合体（Union）" class="headerlink" title="联合体（Union）"></a>联合体（Union）</h1><p>Tags：C语言<br>原文地址：<a href="http://c.biancheng.net/cpp/html/2932.html" target="_blank" rel="noopener">http://c.biancheng.net/cpp/html/2932.html</a></p><p>##1. 共用体的简介<br>在C语言中，还有另外一种和结构体非常类似的语法，叫做共用体（Union），它的定义格式为：</p><pre><code class="c">union 共用体名{    成员列表};</code></pre><p>结构体和共用体的区别在于：结构体的各个成员会占用不同的内存，互相之间没有影响；<strong><em>而共用体的所有成员占用同一段内存，所有成员的存储首地址相同。</em></strong>因此更改一个成员的值可能影响所有成员的值。结构体占用的内存大于等于所有成员占用的内存的总和（成员之间可能会存在缝隙），<strong><em>共用体占用的内存等于最长的成员占用的内存。</em></strong>共用体使用了内存覆盖技术，同一时刻只能保存一个成员的值，如果对新的成员赋值，就会把原来成员的值覆盖掉。<br>共用体也是一种自定义类型，可以通过它来创建变量，例如：</p><pre><code class="c">union data{    int n;    char ch;    double f;};union data a, b, c;</code></pre><p>上面是先定义共用体，再创建变量，也可以在定义共用体的同时创建变量：</p><pre><code class="c">union data{    int n;    char ch;    double f;} a, b, c;</code></pre><p>如果不再定义新的变量，也可以将共用体的名字省略：</p><pre><code class="c">union{    int n;    char ch;    double f;} a, b, c;</code></pre><p>共用体 data 中，成员 f 占用的内存最多，为 8 个字节，所以 data 类型的变量（也就是 a、b、c）也占用 8 个字节的内存，请看下面的演示：</p><pre><code class="c">#include &lt;stdio.h&gt;union data{    int n;  //int类型占用4个字节    char ch;  //char类型占用1个字节    short m;  //short类型占用2个字节};int main(){    union data a;    printf(&quot;%d, %d\n&quot;, sizeof(a), sizeof(union data) );    a.n = 0x40;    printf(&quot;%X, %c, %hX\n&quot;, a.n, a.ch, a.m);    a.ch = &#39;9&#39;;    printf(&quot;%X, %c, %hX\n&quot;, a.n, a.ch, a.m);    a.m = 0x2059;    printf(&quot;%X, %c, %hX\n&quot;, a.n, a.ch, a.m);    a.n = 0x3E25AD54;    printf(&quot;%X, %c, %hX\n&quot;, a.n, a.ch, a.m);    return 0;}</code></pre><p>运行结果：</p><blockquote><p>4, 4<br>40, @, 40  //‘@’是ASCII码等于0x40的字符<br>39, 9, 39  //0x39是’9’的ASCII码<br>2059, Y, 2059  //‘Y’是ASCII码等于0x59的字符<br>3E25AD54, T, AD54</p></blockquote><p>这段代码不但验证了共用体的长度，还说明共用体成员之间会相互影响，修改一个成员的值会影响其他成员。<br>要想理解上面的输出结果，弄清成员之间究竟是如何相互影响的，就得了解各个成员在内存中的分布。以上面的 data 为例，各个成员在内存中的分布如下：</p><p><img src="http://c.biancheng.net/cpp/uploads/allimg/160811/1-160Q1152HRM.jpg" alt></p><p>成员 n、ch、m 在内存中“对齐”到一头，对 ch 赋值修改的是前一个字节，对 m 赋值修改的是前两个字节，对 n 赋值修改的是全部字节。也就是说，ch、m 会影响到 n 的一部分数据，而 n 会影响到 ch、m 的全部数据。<br>上图是在绝大多数 PC机上的内存分布情况，如果是51单片机，情况就会有所不同：</p><p><img src="http://c.biancheng.net/cpp/uploads/allimg/160811/1-160Q116311LV.jpg" alt></p><p>为什么不同的机器会有不同的分布情况呢？这跟机器的存储模式有关。</p><p>##2. 共用体的应用<br>共用体在一般的编程中应用较少，在单片机中应用较多。对于PC机，经常使用到的一个实例是：现有一张关于学生信息和教师信息的表格。学生信息包括姓名、编号、性别、职业、分数，教师的信息包括姓名、编号、性别、职业、教学科目。请看下面的表格：<br>|Name|Num|Sex|Profession|Score / Course|<br>|:———-|:———-|:——|:——|:——|<br>|HanXiaoXiao|501|f|s|89.5|<br>|YanWeiMin    |1011|m|t|math|<br>|LiuZhenTao    |109|f|t|English|<br>|ZhaoFeiYan    |982|m|s|95.0|</p><p>f 和 m 分别表示女性和男性，s表示学生，t表示教师。可以看出，学生和教师所包含的数据是不同的。现在要求把这些信息放在同一个表格中，并设计程序输入人员信息然后输出。<br>如果把每个人的信息都看作一个结构体变量的话，那么教师和学生的前4个成员变量是一样的，第 5个成员变量可能是score或者course。当第4个成员变量的值是 s 的时候，第 5 个成员变量就是score；当第4个成员变量的值是t的时候，第 5 个成员变量就是 course。<br>经过上面的分析，我们可以设计一个包含共用体的结构体，请看下面的代码：</p><pre><code class="c">#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define TOTAL 4  //人员总数struct{    char name[20];    int num;    char sex;    char profession;    union{        float score;        char course[20];    } sc;} bodys[TOTAL];int main(){    int i;    //输入人员信息    for(i=0; i&lt;TOTAL; i++){        printf(&quot;Input info: &quot;);        scanf(&quot;%s %d %c %c&quot;, bodys[i].name, &amp;(bodys[i].num), &amp;(bodys[i].sex), &amp;(bodys[i].profession));        if(bodys[i].profession == &#39;s&#39;){  //如果是学生            scanf(&quot;%f&quot;, &amp;bodys[i].sc.score);        }else{  //如果是老师            scanf(&quot;%s&quot;, bodys[i].sc.course);        }        fflush(stdin);    }    //输出人员信息    printf(&quot;\nName\t\tNum\tSex\tProfession\tScore / Course\n&quot;);    for(i=0; i&lt;TOTAL; i++){        if(bodys[i].profession == &#39;s&#39;){  //如果是学生            printf(&quot;%s\t%d\t%c\t%c\t\t%f\n&quot;, bodys[i].name, bodys[i].num, bodys[i].sex, bodys[i].profession, bodys[i].sc.score);        }else{  //如果是老师            printf(&quot;%s\t%d\t%c\t%c\t\t%s\n&quot;, bodys[i].name, bodys[i].num, bodys[i].sex, bodys[i].profession, bodys[i].sc.course);        }    }    return 0;}</code></pre><p>运行结果：</p><blockquote><p>Input info: HanXiaoXiao 501 f s 89.5<br>Input info: YanWeiMin 1011 m t math<br>Input info: LiuZhenTao 109 f t English<br>Input info: ZhaoFeiYan 982 m s 95.0</p></blockquote><blockquote><p>Name            Num     Sex     Profession      Score / Course<br>HanXiaoXiao     501     f       s               89.500000<br>YanWeiMin       1011    m       t               math<br>LiuZhenTao      109     f       t               English<br>ZhaoFeiYan      982     m       s               95.000000</p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/CProgrammingLanguage/%E3%80%8AC%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%AF%AD%E8%A8%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%E5%87%BD%E6%95%B0%E4%B8%8E%E7%A8%8B%E5%BA%8F%E7%BB%93%E6%9E%84/"/>
      <url>/2019/06/15/CProgrammingLanguage/%E3%80%8AC%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%AF%AD%E8%A8%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%E5%87%BD%E6%95%B0%E4%B8%8E%E7%A8%8B%E5%BA%8F%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<p>Tags:《C语言程序设计》读书笔记</p><p>#《C程序设计语言》读书笔记（四）——函数与程序结构</p><h2 id="book"><a href="#book" class="headerlink" title="book"></a>book</h2><p>###4.1</p><ul><li>找出某一字符串在一段文本内的位置并返回</li></ul><pre><code class="c">int strindex(char s[], char t[]){    int i,j,k;    for(i = 0;s[i] != &#39;\0&#39;;i++)  //遍历初始位置    {        for (j = i,k = 0;t[k] != &#39;\0&#39; &amp;&amp; s[j] == t[k];j++,k++)            ;        if(k &gt; 0 &amp;&amp; t[k] == &#39;\0&#39;)  //t[k] == &#39;\0&#39;表示完全匹配            return i;    }    return -1;  //遍历完成后没有找到匹配的}</code></pre><p>###4.4</p><ul><li>外部变量或函数的作用域从声明它的地方开始，到其所在的（待编译的的）文件的末尾为结束。如果要在外部变量的定义之前使用该变量，或者外部变量的定义与变量的使用不在同一源文件中，则必须在相应的变量声明中强制性地使用关键字extern.</li><li>外部变量定义与声明<br>外部变量的声明英语说明变量的属性（主要是变量的类型），而那两定义除此之外还将引起存储器的分配。一个外部变量只能某个文件中定义一次，而其他文件可以通过extern声明访问它；下面给出变量定理和声明的例子：<pre><code class="c">//变量定义int sp;double val[MAXVAL];  //要说明数组的大小</code></pre></li></ul><p>//变量声明<br>extern int sp;<br>extern double val[];</p><pre><code>### 4.5- 头文件的作用由于一些共同使用的外部变量和函数在不同文件中被使用的时候需要被重新声明，所以可以将这些需要共同使用的变量和函数的 *声明* 放在同一个文件中，在需要用到这些函数的文件中只要将这个文件包含进来就可以直接使用，不必再在每个文件重新声明外部变量和函数。这个包含外部变量和共同使用的函数的文件就叫做头文件，包含头文件的操作叫`#include&lt;a.h&gt;`。一般头文件只会存放外部变量和函数的声明，定义在各个.c文件中实现，因为函数的声明只能出现一次，如果定义放在头文件中，而这个头文件又被多个文件引用，那么容易出现重复定义的问题。### 4.6- 静态变量声明一个变量为静态变量`static`的作用在于限定该变量的作用区域。如对一个外部变量声明为静态变量表示该变量只能在声明的文件中使用，在其他文件中不能使用该外部变量。对一个函数中的变量声明为静态变量表示该变量只能在该函数的范围内使用。**注意**：在函数中声明的静态变量和函数内部变量的区别是静态变量始终存在于内存中，下次调用时存放在里面的值是上次调用完毕后的值；而内部变量在函数开始时被分配内存，在函数结束后内存被收回，下次调用重新分配内存。### 4.9- 初始化在不进行人为初始化的情况下，外部变量和静态变量都被初始化为0，而内部变量和寄存器变量没有定义。外部变量和静态变量的初始化必须是 *常量* 表达式。### 4.11.2- 宏替换宏定义可以带参数，不过要十分注意括号的添加，如`#define square(x) (x)*(x)`中的括号不可省略，否则在计算square(a+1)时会发生错误### 4.11.3- 条件包含 #ifdef在头文件的编写中，为了防止一个头文件被重复包含在同一个.c文件中，最好使用条件包含格式。头文件被重复包含情况：假设 a.h 和 b.h 中有 \#include &quot;c.h&quot;语句，而在 main.c 中出现了\#include &quot;a.h&quot; 和\#include &quot;b.h&quot; 那么 v.h就属于在一个.c 文件中重复引用的情况。这时候就会用到如下格式的语句：``` c#ifndef B_H#define B_H//declarations#endif</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/CProgrammingLanguage/%E3%80%8AC%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%AF%AD%E8%A8%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94%E7%BB%93%E6%9E%84/"/>
      <url>/2019/06/15/CProgrammingLanguage/%E3%80%8AC%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%AF%AD%E8%A8%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<p>Tags:《C语言程序设计》读书笔记</p><h1 id="《C程序设计语言》读书笔记（六）——结构"><a href="#《C程序设计语言》读书笔记（六）——结构" class="headerlink" title="《C程序设计语言》读书笔记（六）——结构"></a>《C程序设计语言》读书笔记（六）——结构</h1><h2 id="book"><a href="#book" class="headerlink" title="book"></a>book</h2><h3 id="6-1"><a href="#6-1" class="headerlink" title="6.1"></a>6.1</h3><p>结构的初始化只能紧跟在结构类型的变量声明后面，必须对所有结构成员进行初始化，且初始化的值必须为常量。比如</p><pre><code class="c">    struct abc    {        int a;        int b;    } x = {100,200};</code></pre><p>或者</p><pre><code class="c">struct abc y = {200,390};</code></pre><p>或者</p><pre><code class="c">struct abc z;z.a = 200;z.b = 300;</code></pre><p>而不允许</p><pre><code class="c">struct abc y;y = {200,300};</code></pre><h3 id="6-2-结构与函数、指针"><a href="#6-2-结构与函数、指针" class="headerlink" title="6.2 结构与函数、指针"></a>6.2 结构与函数、指针</h3><h4 id="1"><a href="#1" class="headerlink" title="1."></a>1.</h4><p>在函数中可以调用结构，也可以在一个函数中返回一个结构。这里要注意的是，在函数的声明中如果出现结构的形式，其类型是struct + 结构标记，如<code>struct abc</code>。下面给出课本上两个使用结构的函数：</p><pre><code class="c">struct point makepoint (int x,int y)  //返回类型为struct point{    struct point temp;    temp.x = x;    temp.y = y;    return temp;  //返回结构时只写结构名即可}struct point addpoint(struct point p1,struct point p2)  //形参为两个struct point类型，返回为struct point类型{    p1.x += p2.x;    p1.y += p2.y;    return p1;}</code></pre><h4 id="2-结构指针"><a href="#2-结构指针" class="headerlink" title="2. 结构指针"></a>2. 结构指针</h4><p>在结构很大的条件下，使用指针方式的结构比普通的结构效率更高。结构指针的定义方式和普通指针类似：<code>struct point *pp;</code>。调用结构成员时可使用两种方法：<code>(*pp).a</code>或<code>pp-&gt;a</code>。个人感觉第二种方法不容易出错，因为运算符<code>.</code>的优先级高于<code>*</code>，所以第一种方法中圆括号不可少，而第二种方法简单直观。实际上，通过查找2.12的运算符优先级表可以看到，<code>-&gt;</code>和<code>.</code>的优先级是最高的，所以在类似于<code>++</code>、<code>*</code>等操作时一定注意括号的使用。</p><h3 id="6-5-自引用结构"><a href="#6-5-自引用结构" class="headerlink" title="6.5 自引用结构"></a>6.5 自引用结构</h3><p>在结构的声明中不能包括结构本身，但是可以包括指向本结构的指针。在下面这个课本例子对于结构的声明中我们可以看到这一点。</p><pre><code class="c">/*例6.5 统计单词出现的次数完整版代码*/#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;malloc.h&gt;#define MAXWORDNUM 100  //输入字符串最大数目#define MAXWORDLEN 10  //输入每个字符串最大长度struct tnode{    char *tword;    int num;    struct tnode *left;    struct tnode *right;};struct tnode * Addtree(struct tnode *p, char *word)  //判断一个单词是否在二叉树中并添加其到合适位置，返回根节点{    if (p == NULL)    {        p = (struct tnode *)malloc(sizeof(struct tnode));  //分配struct tnode大小的空间        p-&gt;tword = word;        p-&gt;num = 1;        p-&gt;left = NULL;        p-&gt;right = NULL;    }    else if ((strcmp(p-&gt;tword, word)) == 0)        p-&gt;num++;    else if ((strcmp(p-&gt;tword, word)) &gt; 0)        p-&gt;left = Addtree(p-&gt;left, word);  //递归调用    else p-&gt;right = Addtree(p-&gt;right, word);    return p;}int Findtree(struct tnode *p,char *word)  //查找一个单词是否在二叉树中，找到返回1，否则返回0{    int result = 0;    if(p != NULL)    {        result |= Findtree(p-&gt;left,word);  //如果在左子树中找到，左子树的Findtree返回1，和总结果做或运算即可        if(strcmp(p-&gt;tword,word) == 0)            result = 1;        result |= Findtree(p-&gt;right,word);    }    return result;}void Printtree(struct tnode *p)  //打印整个二叉树{    if (p != NULL)  //这里不能使用while，否则会一直循环    {        Printtree(p-&gt;left);  //递归调用        printf(&quot;word:%10s\t\tnumber:%2d\n&quot;, p-&gt;tword, p-&gt;num);        Printtree(p-&gt;right);    }}int main(int argc, char const *argv[]){    char *wordarray[MAXWORDNUM];    int i = 0, j,result;    char *temp,*tempword;    printf(&quot;Please enter all words:\n&quot;);    //将所有单词输入进一个字符串指针数组中    do    {        wordarray[i] = (char*)malloc(MAXWORDLEN * sizeof(char));  //使用gets()输入字符串        gets(wordarray[i]);        i++;    } while (*wordarray[i - 1] != &#39;\0&#39; &amp;&amp; i &lt; MAXWORDNUM);    struct tnode *root;    root = NULL;    for (j = 0; j &lt; ((*wordarray[i - 1] == &#39;\0&#39; ? i - 1 : i); j++)        root = Addtree(root, wordarray[j]);  //创建整棵二叉树。注意每一个Addtree的调用返回的是它本身，所以root并不会因此    printf(&quot;Press 1 to find a specific word,press 2 to print all words existed:\n&quot;);    temp = (char *)malloc(sizeof(char));    gets(temp);    switch(*temp)    {        case &#39;1&#39;:  //选择1：查找某个单词是否在二叉树中        printf(&quot;Please enter the word you want to find:\n&quot;);        tempword = (char*)malloc(MAXWORDLEN*sizeof(char));        gets(tempword);        result = Findtree(root,tempword);        result &gt; 0 ? puts(&quot;Found!&quot;):puts(&quot;Not found!&quot;);        break;        case &#39;2&#39;:  //选择2：打印整棵二叉树        Printtree(root);        printf(&quot;\nPrint complete!\n&quot;);        break;        default:        printf(&quot;Input error!\n&quot;);        break;    }return 0;}</code></pre><h3 id="6-6-表查找"><a href="#6-6-表查找" class="headerlink" title="6.6 表查找"></a>6.6 表查找</h3><p>给出一个课本例题的稍微改动和添加版本，其中包含了hash表的初始化、添加数据、删除和查找功能，也包括使用malloc()和get_s()为字符串指针申请内存以及一些编程中容易出错的地方（花了我快一天啊啊啊效率太低了(╯°口°)╯(┴—┴）</p><pre><code class="c">//hash表的初始化、添加、删除和查找练习// ConsoleApplication2.cpp : 定义控制台应用程序的入口点。//#include &quot;stdafx.h&quot;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;malloc.h&gt;#define MAXHASHSIZE 20#define MAXCHARSIZE 20#define SUCCESS 1#define FAIL -1typedef struct tnode{    char *tword;    int count;}Hashtable;int InitHashtable(Hashtable *[]);int AddToHashtable(Hashtable *[], char*);int DeleteHashtable(Hashtable *[], char *);int FindInHashtable(Hashtable *[], char *);int InitHashtable(Hashtable *root[]);unsigned int CalcHashValue(char *);int InitHashtable(Hashtable *root[])  //初始化hash表，所有字符串指针指向NULL，计数为0{    int i;    for (i = 0; i &lt; MAXHASHSIZE; i++)    {        root[i] = (Hashtable*)malloc(sizeof(Hashtable));  //为字符串指针分配地址        if ((*(root + i))-&gt;tword == NULL)  //存储空间满            return FAIL;        (*(root + i))-&gt;tword = NULL;        (*(root + i))-&gt;count = 0;    }    return SUCCESS;}int AddToHashtable(Hashtable *root[], char *data)  //将一个字符串加入hash表{    unsigned int hashvalue, i = 0;    hashvalue = CalcHashValue(data);    while ((*(root + hashvalue))-&gt;tword != NULL &amp;&amp; i &lt; MAXHASHSIZE)  //欲放入的位置不为空    {        if (strcmp((*(root + hashvalue))-&gt;tword, data) == 0)        {            (*(root + hashvalue))-&gt;count++;            break;        }        else            hashvalue = (hashvalue + i) % MAXHASHSIZE;  //存放位置加1        i++;    }    if (i == MAXHASHSIZE)    {        printf(&quot;Stack full!&quot;);        return FAIL;    }    else    {        (*(root + hashvalue))-&gt;tword = data;  //放入字符串        (*(root + hashvalue))-&gt;count = 1;  //计数        return SUCCESS;    }}int DeleteHashtable(Hashtable *root[], char *word)  //从hash表中删除一个字符串数据{    unsigned int hashvalue;    int temp;    temp = FindInHashtable(root, word);    if (temp == FAIL)    {        printf(&quot;Cannot delete!\n&quot;);        return FAIL;    }    else    {        (*(root + temp))-&gt;tword = NULL;  //字符串指针指向NULL        (*(root + temp))-&gt;count = 0;        return SUCCESS;    }}int FindInHashtable(Hashtable *root[], char *word)  //在hash表中查找某个字符串{    unsigned int hashvalue;    int i;    hashvalue = CalcHashValue(word);    for (i = 0;(*(root + hashvalue))-&gt;tword != NULL &amp;&amp; strcmp((*(root + hashvalue))-&gt;tword, word) != 0 &amp;&amp; i &lt; MAXHASHSIZE; i++)        hashvalue = (hashvalue + i) % MAXHASHSIZE;    if (i &lt; MAXHASHSIZE &amp;&amp; (*(root + hashvalue))-&gt;tword != NULL)  //找到的时候没有循环一次而且不为空指针，说明已经找到        return (hashvalue + i) % MAXHASHSIZE;  //返回hash表中位置    else return FAIL;}unsigned int CalcHashValue(char *data)  //计算一个字符串hash的值{    int i = 0;    unsigned int result = 0;    while (*data != &#39;\0&#39;)        result = result * 31 + *data++;    return result % MAXHASHSIZE;}int main(int argc, char const *argv[]){    char *temp[MAXHASHSIZE];    char *wordtemp;    int i = 0, j = 0, k, l;    char m;    Hashtable *root[MAXHASHSIZE];    //初始化过程    printf(&quot;Please enter all words:\n&quot;);    do    {        *(temp + i) = (char *)malloc(MAXCHARSIZE * sizeof(char));  //为字符串指针分配存储空间        gets_s(*(temp + i),MAXCHARSIZE);        i++;    } while (**(temp + i - 1) != &#39;\0&#39; &amp;&amp; i &lt; MAXHASHSIZE);    if (InitHashtable(root) == SUCCESS)        printf(&quot;Initial complete!\n&quot;);    else    {        printf(&quot;Initial fail!\n&quot;);        return FAIL;    }    //添加字符串数据过程    for (j = 0; j &lt; (**(temp + i - 1) == &#39;\0&#39; ? i - 1 : i); j++)    {        if (l = AddToHashtable(root, *(temp + j)) != SUCCESS)        {            printf(&quot;Add fail!\n&quot;);            return FAIL;        }    }    printf(&quot;Add complete!\n&quot;);    //查找，删除或退出    while (1)    {        printf(&quot;Press 1 to delete a word,press 2 to find a word,press 3 to break:\n&quot;);        m = getchar();        getchar();  /*注意注意注意在getchar()获取一个字符后一定要加一个getchar()清楚缓存区，        否则下面的gets_s()会直接从缓存区读取回车号，造成的后果是读取的直接为空字符_(:3」∠)_*/        switch (m)        {            case &#39;1&#39;:  //删除            printf(&quot;Please enter the word you want to delete:\n&quot;);            wordtemp = (char*)malloc(MAXCHARSIZE * sizeof(char));            gets_s(wordtemp, MAXCHARSIZE);            k = DeleteHashtable(root, wordtemp);            if (k == SUCCESS)                printf(&quot;Delete successful!\n&quot;);            else                printf(&quot;Delete fail!\n&quot;);            break;            case &#39;2&#39;:  //查找            printf(&quot;Please enter the word you want to find:\n&quot;);            wordtemp = (char*)malloc(MAXCHARSIZE * sizeof(char));            gets_s(wordtemp, MAXCHARSIZE);            k = FindInHashtable(root, wordtemp);            if (k &gt;= 0 &amp;&amp; k &lt; MAXHASHSIZE)                printf(&quot;Found!%s is in the %d place.\n&quot;, (*(root + k))-&gt;tword, k);            else if (k == FAIL)                printf(&quot;Not found!\n&quot;);            else                printf(&quot;Error!\n&quot;);            break;            case &#39;3&#39;:            return 0;        }    }    return 0;}</code></pre><p>###6.7 类型定义<br>格式：typedef 原类型名 新类型名<br>例子：</p><ol><li><code>typedef int length;</code></li><li><code>typedef char *string;</code> 定义string为字符串指针类型</li><li><code>typedef struct tnode{} Tree;</code>  定义Tree为struct tnode类型，因此Tree tnode1等价于struct tnode tnode1;</li><li><code>typedef struct tnode *pTree;</code>  定义pTree为指向struct tnode类型的指针，因此可用：<code>pTree pTree1; pTree1-&gt;member</code>访问成员变量</li><li><code>typedef int (*Func1)(char*,char*);</code>  定义一个指向函数的指针，这个函数返回char类型，因此可用：<code>Func1 func1; a = (*func1)(str1,str2);</code>来调用这个函数</li></ol><p>###6.8 共用体<br><a href="http://blog.csdn.net/yyb19951015/article/details/71036444" target="_blank" rel="noopener">共用体（Union）简介</a></p><h3 id="6-9-位字段"><a href="#6-9-位字段" class="headerlink" title="6.9 位字段"></a>6.9 位字段</h3><p>位字段的作用是将多个对象保存在同一个机器字中，机器字是指计算机一次运算能够同时处理的最大位数，通常所说的32/64位电脑就是指机器字的长度。位字段在一个结构中声明，方式为：</p><pre><code class="c">//代码引用自：http://blog.csdn.net/lovecodeless/article/details/23270911struct  {    unsigned int a  : 1;     //冒号“:”后的数字为该位字段所占的bit位数    unsigned int b  : 3;          unsigned int c  : 5;}flags;</code></pre><p>因此，在对每个字段进行赋值的时候，要注意不能超过定义的最大范围，在上例中范围为0~2^(n-1)<br>更详细的介绍请参考：<a href="http://blog.csdn.net/lovecodeless/article/details/23270911" target="_blank" rel="noopener">http://blog.csdn.net/lovecodeless/article/details/23270911</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/CProgrammingLanguage/%E3%80%8AC%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%AF%AD%E8%A8%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%E6%8C%87%E9%92%88%E4%B8%8E%E6%95%B0%E7%BB%84/"/>
      <url>/2019/06/15/CProgrammingLanguage/%E3%80%8AC%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%AF%AD%E8%A8%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%E6%8C%87%E9%92%88%E4%B8%8E%E6%95%B0%E7%BB%84/</url>
      
        <content type="html"><![CDATA[<p>Tags:《C语言程序设计》读书笔记</p><h1 id="《C程序设计语言》读书笔记（五）——指针与数组"><a href="#《C程序设计语言》读书笔记（五）——指针与数组" class="headerlink" title="《C程序设计语言》读书笔记（五）——指针与数组"></a>《C程序设计语言》读书笔记（五）——指针与数组</h1><h2 id="book"><a href="#book" class="headerlink" title="book"></a>book</h2><h3 id="5-1-指针的概念"><a href="#5-1-指针的概念" class="headerlink" title="5.1 指针的概念"></a>5.1 指针的概念</h3><ol><li><p>声明：<code>int *ip;  \\一个指向int类型的指针</code></p></li><li><p>基本操作：<code>ip = &amp;x;  \\将ip指向x，ip中存放x的地址</code>  <code>b = *ip; \\将ip指向地址的变量中的值取出赋给b</code><br>取地址符号的优先级高于+-*/和+=等，但与++和–优先级相同，运算顺序从右向左，因此在表示取ip指向的地址里的值并自加时应表示为：<code>(*ip)++</code></p><h3 id="5-3-指针与数组"><a href="#5-3-指针与数组" class="headerlink" title="5.3 指针与数组"></a>5.3 指针与数组</h3><ol><li><p>当一个指针指向数组的某一个元素时，指针数值加一必然指向数组的下一个元素，无论数组中元素的类型数组的长度是多少。</p></li><li><p>数组名与指向该数组第一个元素的指针含义相同，即<code>int a[10]; a = &amp;a[0]; *(a+i) = a[i];</code>。但是注意指针是变量，值可更改，数组名是常量，值不可更改。</p></li><li><p>当数组作为参数传入一个函数中时，实际传入的是指向该数组的指针，因此之前在传入数组的函数定义中使用的a[]等价于*a。</p><h3 id="5-4-指针用于地址分配"><a href="#5-4-指针用于地址分配" class="headerlink" title="5.4 指针用于地址分配"></a>5.4 指针用于地址分配</h3></li><li><p>在进行存储空间的分配和释放时，对于外部来说存放数据的数组名字和容量并不需要知道，只需设置一个静态数组和一个指向数组栈顶的指针，返回指向存储起点的指针即可。</p><h3 id="5-5"><a href="#5-5" class="headerlink" title="5.5"></a>5.5</h3></li><li><p>字符指针<br><code>char *s</code>为指向一个字符串常量的指针，其实际指向为该字符串首个字符的地址址，可以通过<code>*(s + 1)</code>来访问字符串中的每一个字符，但<strong>试图通过s来修改字符串是没有意义的</strong>，因为指针指向的字符串是作为常量保存在静态存储区的。如果想要通过类似于<code>*(s + 1) = &#39;A&#39;;</code>的方法来修改字符串，只能通过前面介绍的字符串数组实现。<br>ex:</p><pre><code class="c">void Strcpy(char *ori,char *des){while(*des++ = *ori++);  //先计算++，再赋值并与&#39;\0&#39;比较，最后des和des自加}</code></pre><h3 id="5-6-指针数组和指向指针的指针"><a href="#5-6-指针数组和指向指针的指针" class="headerlink" title="5.6 指针数组和指向指针的指针"></a>5.6 指针数组和指向指针的指针</h3></li><li><p>形如<code>char *a[]</code>格式的称为指针数组，数组中的每一个元素为一个char类型的指针，可指向一个字符串或一个字符串数组。以<code>char *a[]</code>为例，下面给出一些指针数组的用法：</p><pre><code class="c">a[0] = &quot;abcd&quot;;  //将指针数组中的第一个元素——一个char类型的指针指向一个字符串&quot;abcd&quot;putchar(*(a[0]+1));  //打印a[0]指向的字符串中的第二个字符</code></pre></li><li><p>指向指针的指针<br>下面是一个简单的指向指针的指针的例子：</p><pre><code>int a = 1,b = 2;int *p1 = &amp;a;int *p2 = &amp;b;int *pp1 = &amp;p1;printf(&quot;%d %d %d&quot;,a,*p1,**pp1);  //利用三种方法打印变量a的值，结果相同</code></pre><p>由指针的知识我们知道变量p1中存放的是变量a的地址，变量p2中存放的是变量b的地址。如果我们想用一个变量存放p1或p2的地址，就需要对p1或p2取地址，再放到一个变量里，这种变量就称为指向指针的指针，在上例中为pp1。那么，自然有<code>*pp1 = p1</code>和<code>pp1 = &amp;p1</code>。如果令<code>*pp1 = p2;</code>，完成的操作是将p2中存放的地址拷贝给p1，这时候p1和p2均指向b，自然有<code>*p1 == *p2</code>，且该值为2。<strong>注意</strong>：此时pp1仍然指向p1，也就是仍有<code>pp1 = &amp;p1</code>成立，改变的是存储在p1中的值。如果想改变pp1的指向，应该用<code>pp1 = &amp;p2;</code>来表示。在使用这些指向指针的指针时，记住一句话：<strong><em>要想改变一个指针指向的变量，等号左边必须是指针名字；要想改变指针指向变量的值而不是改变指向关系，等号左边必须是对指针取值，即\</em>号。*</strong></p><h3 id="5-7-多维数组"><a href="#5-7-多维数组" class="headerlink" title="5.7 多维数组"></a>5.7 多维数组</h3></li><li><p>个人感觉多维数组和指针数组以及指向指针的指针有异曲同工之处。还是拿一个简单的例子来说说。设有一个二维数组<code>char a[3][10]</code>。容易联想到，这个数组实际上包含3个元素，而每一个元素又是一个长度为10的字符串数组。我们分两个层次分析这个数组：</p></li><li><p>联想到刚才的指针数组以及前面所学的<strong><em>数组名实际上是一个指向数组首地址的指针</em></strong>，我们发现a[3]实际上就是一个指针数组，a[0]~a[2]是分别指向三个字符串的首地址的指针。这样一来，想要打印出第i个字符串中的第j个字符的代码就显而易见了，有以下三种方法：</p><pre><code class="c">putchar(a[i - 1][j - 1]);  //直接取数组元素，最容易想到的方法putchar(*(a[i - 1] + j - 1))  //利用指针来取元素，利用数组来取指针putchar(*(*(a + i - 1) + j - 1)) //这里用指针来取指针数组的元素，感觉是最容易把人绕晕的方法_(:з」∠)_</code></pre></li><li><p>再来看a[3]这个指针数组，重复使用刚才提到的<strong><em>数组名实际上是一个指向数组首地址的指针</em></strong>，可以看出a实际上是一个指向指针数组首个元素a[0]的指针，那么我们要想访问这个指针数组中的第i个元素，可以使用*(a + i - 1)完成，也就有了上面的第三种最容易把人绕晕的方法。<br>接下来提供两个有关多维数组和指向指针的指针的例子（写的不完善，仅供参考）<br>ex1：</p><pre><code class="c">//一个简单的利用指针进行字符串排序的程序#include&lt;stdio.h&gt;#include&lt;string.h&gt;#define MAXSTRNUM 4   //最大输入数组数#define MAXSTRLEN 10  //每个数组最大长度void StringComp(char *[]);int main(){char s[MAXSTRNUM][MAXSTRLEN];char *q[MAXSTRNUM];int i;printf(&quot;Please enter %d strings:\n&quot;, MAXSTRNUM);for (i = 0;i &lt; MAXSTRNUM;i++){scanf(&quot;%s&quot;,*(s + i));*(q + i) = *(s + i);   /*指针数组q的每一个元素指向二维数组的每一个字符串，这里不能直接用s[i]当做指向字符串的指针，因为后面涉及到指针指向地址的交换，s[i]不具有交换功能*/}StringComp(q);for(i = 0;i &lt; MAXSTRNUM;i++)printf(&quot;%s\n&quot;,*(q + i));return 0;}</code></pre></li></ol></li></ol><p>void StringComp(char <em>l[])<br>{    int j,k;<br>    char *temp;<br>    for(j = 0;j &lt; MAXSTRNUM - 1;j++)  //冒泡排序法<br>    {<br>        for(k = 0;k &lt; MAXSTRNUM - 1 - j;k++)<br>        if(strcmp(</em>(l + k),*(l + k + 1)) &gt; 0 )<br>        {<br>            temp = *(l + k + 1);  // 交换指针指向的对象<br>            *(l + k + 1) = *(l + k);<br>            *(l + k) = temp;<br>        }</p><pre><code>}</code></pre><p>}</p><pre><code>ex2：``` c//一个可以得到某年第几天的具体日期或某天是当年第几天的程序#include &lt;stdio.h&gt;int day_of_year(int,int *,int *);void month_day(int,int,int *,int *);int main(int argc, char const *argv[]){    int year,yearday,i,c,monthtemp,daytemp;    int temp[4];    int *month,*day;    month = &amp;monthtemp;    day = &amp;daytemp;    printf(&quot;Please enter the year and yearday or the year,month and day:\n&quot;);    scanf(&quot;%d,%d,%d,%d&quot;,temp,temp + 1,temp +2,temp + 3);    if(*(temp + 3) == 1)    {        year = temp[0];        yearday = temp[1];        month_day(year,yearday,month,day);        printf(&quot;year:%d month:%d day:%d\n&quot;,year,*month,*day );    }    else if(*(temp + 3) == 2)    {        year = temp[0];        month = &amp;temp[1];        day = &amp;temp[2];        printf(&quot;year:%d yearday:%d\n&quot;,year,day_of_year(year,month,day));    }    else printf(&quot;Enter wrong~\n&quot;);    return 0;}static char daytab[2][13] ={    {0,29,28,31,30,31,30,31,31,30,31,30,31},    {0,31,29,31,30,31,30,31,31,30,31,30,31}};  //daytab[][0]被设为0是为了方便计算月份的时候从1开始，符合习惯int day_of_year(int year,int *month,int *day){    int i,leap,yearday;    yearday = *day;    leap = year % 4 == 0 &amp;&amp; year % 100 != 0 || year % 400 == 0;  //选择是否为闰年，闰年leap = 1    for(i = 1;i &lt; *month;i++)        yearday += daytab[leap][i]; //总天数等于当月天数day加上该月份之前的天数之和    return yearday;}void month_day(int year,int yearday,int *month,int *day){    int i,leap;    leap = year % 4 == 0 &amp;&amp; year % 100 != 0 || year % 400 == 0;  //选择是否为闰年，闰年leap = 1    for(i = 1;yearday &gt; daytab[leap][i];i++)  //判断天数是否还大于一个月，若不大于，则返回结果        yearday -= daytab[leap][i];    *month = i;    *day = yearday;}</code></pre><h3 id="5-10-命令行参数"><a href="#5-10-命令行参数" class="headerlink" title="5.10 命令行参数"></a>5.10 命令行参数</h3><ol><li><p>在c程序开始执行时，可以向程序传递命令行参数，我们经常见到的<code>int main(int argc, char const *argv[])</code>就是命令行参数的表示。可以看到，命令行参数包括两个参数：int 类型的argc和char *[]类型的argv。argc用于存放命令行参数的数目，argv是一个指向一系列字符串的数组，用于存放参数。由c语言规定，*argv指向的是启动该程序的程序名，因此argc的值最小为1，自定义参数的存放也从*(argv + 1)开始。同时，argv[argc]的值规定为null，也就是空指针，因此能够存放命令行参数的就只有argv[1]到argv[argc - 1]。<br>在编译时，可以通过IDE选择传递到程序的参数，在Dev-C++中设置方法为菜单栏Execute-&gt;Parameters。相邻参数使用空格分隔。比如，传入a b c，则有argc = 4,*(*argv + 1) = “a”,*(*argv + 2) = “b”,*(*argv + 3) = “c”。</p></li><li><p>课本中的改写后的find函数实际上就是将一些指定的参数（如-n，-x）传进程序，从而查找并输出相应的行。<code>while(--argc &gt; 0 &amp;&amp; (*++argv)[0] == &#39;-&#39;</code>的含义是：遍历每一个输入参数的第一个字符，判断是否等于’-‘。<code>(*++argv)[0]</code>以二维数组的形式改写就是argv[i + 1][0]。下一行<code>c = *++argv[0]</code>的意思是取下一个字符，由此判断要查找的类型。由于’*‘和’++’是同级运算符，结合顺序是由右向左，而argv[0]是指向一个字符串（第一个字符）的指针，因此自加后该指针指向第二个字符，再用*取出该字符。</p><h3 id="5-11-指向函数的指针"><a href="#5-11-指向函数的指针" class="headerlink" title="5.11 指向函数的指针"></a>5.11 指向函数的指针</h3></li><li><p>这一节出现了一个看上去很奇怪的函数声明：<br><code>void  qsort(void *lineptr[],int left,int right,int (*comp)(void    *,void *)</code>.这里面的(*comp)其实就是一个指向函数的指针，这个函数传入两个通用指针类型void *,返回int类型。</p></li><li><p>函数指针变量的声明、初始化和使用：<br>函数指针的声明和普通函数的声明几乎一样，与普通函数声明唯一区别之处在于函数名是(*comp)而不是comp，这点应该很好理解，因为声明的是一个指向函数的指针。在声明完成后，要让这个函数指针指向一个具体的函数。很显然指向的函数必须与函数指针的输入和返回参数的类型完全一致，这样才能匹配。初始化方法为:<code>comp = &amp;compare</code>，compare为函数名。而使用函数指针的方法为：<code>*comp(a,b)</code>原理很简单，相当于取出指针指向的函数。<br>下面给出一个简单的使用函数指针变量和函数指针类型的程序：</p><pre><code class="c">#include &lt;stdio.h&gt;void Change(int,int);int main(int argc, char const *argv[]){ int a = 1,b = 2; /*1.定义一个指向函数的指针变量*/ void (*Func1)(int,int); printf(&quot;Exercise1:\n&quot;); Func1 = &amp;Change;  //函数指针初始化 (*Func1)(a,b);  //注意此处不可以写成*Func1(a,b) /*2.定义一个指向函数的指针变量类型*/ typedef void(*Func2)(int,int);  //定义了一个名为Func2的函数指针类型 Func2 func2;  //定义一个名为func2的函数指针 func2 = &amp;Change; printf(&quot;Exercise2:\n&quot;); (*func2)(a,b); /*3.将一个指向函数的指针变量传递进一个函数*/ printf(&quot;Exercise3:\n&quot;); void Func3(void (*func3)(int,int),int c,int d)  //注意这里传入Func3的函数指针func3相当于声明，不用写形参 {     (*func3)(c,d); } Func3(Func1,a,b); /*4.在一个函数中定义一个指向函数的指针类型*/ printf(&quot;Exercise4:\n&quot;); void Func4(Func2 func4,int a, int b) {     (*func4)(a,b); } Func4(Func1,a,b); return 0;}void Change(int a,int b)  //交换a,b的值并打印{ int i; i = a; a = b; b = i; printf(&quot;a = %d, b = %d\n&quot;,a,b );}</code></pre></li></ol><pre><code></code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/CProgrammingLanguage/%E3%80%8AC%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%AF%AD%E8%A8%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89%E2%80%94%E2%80%94%E8%BE%93%E5%85%A5%E4%B8%8E%E8%BE%93%E5%87%BA/"/>
      <url>/2019/06/15/CProgrammingLanguage/%E3%80%8AC%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%AF%AD%E8%A8%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89%E2%80%94%E2%80%94%E8%BE%93%E5%85%A5%E4%B8%8E%E8%BE%93%E5%87%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="《C程序设计语言》读书笔记（七）——输入与输出"><a href="#《C程序设计语言》读书笔记（七）——输入与输出" class="headerlink" title="《C程序设计语言》读书笔记（七）——输入与输出"></a>《C程序设计语言》读书笔记（七）——输入与输出</h1><p>Tags:《C语言程序设计》读书笔记</p><h2 id="book"><a href="#book" class="headerlink" title="book"></a>book</h2><h3 id="7-2-格式化输出"><a href="#7-2-格式化输出" class="headerlink" title="7.2 格式化输出"></a>7.2 格式化输出</h3><p>利用printf控制输出时，每个控制语句由’%’号8开始，由转换字符结束，在’%’号和控制字符之间可以有下列组成成分：</p><ul><li>负号：表示输出内容左对齐</li><li>a.b：a表示最小字段宽度，对于右对齐来说，如果最小字段宽度大于输出的字符数，则在左端用相应数量的空格填充，对于右对齐则正好相反；b表示精度，用于指定字符串中要打印的最大字符数和浮点数小数点后的位数<br>例如： <code>char *s = &quot;Hello world!&quot;; printf(&quot;%15.8s&quot;,s);</code>将打印出：<code>（7个空格）Hello wo</code></li><li>h或l：h表示将整数作为short类型打印，l表示将整数作为long类型打印</li></ul><p>下面列出一些不常见到的转换字符，常见的不再举例：</p><ul><li>u：无符号十进制数</li><li>e/E：double类型的科学计数法</li><li>g/G：如果置数小于-4或大于等于精度，则使用%e格式输出，否则使用%f格式输出</li><li>p：指针类型</li></ul><p>sprintf函数：将输出结果存放到字符数组中，成功则返回字符串的长度。其语法为：<code>int sprintf(char *string, char *format, arg1,arg2);</code></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/06/15/CProgrammingLanguage/C%E8%AF%AD%E8%A8%80%E4%B8%AD%E6%95%B0%E7%BB%84%E5%90%8D%E4%BD%9C%E4%B8%BA%E5%8F%82%E6%95%B0%E8%BF%9B%E8%A1%8C%E5%87%BD%E6%95%B0%E4%BC%A0%E9%80%92/"/>
      <url>/2019/06/15/CProgrammingLanguage/C%E8%AF%AD%E8%A8%80%E4%B8%AD%E6%95%B0%E7%BB%84%E5%90%8D%E4%BD%9C%E4%B8%BA%E5%8F%82%E6%95%B0%E8%BF%9B%E8%A1%8C%E5%87%BD%E6%95%B0%E4%BC%A0%E9%80%92/</url>
      
        <content type="html"><![CDATA[<h1 id="C语言中数组名作为参数进行函数传递"><a href="#C语言中数组名作为参数进行函数传递" class="headerlink" title="C语言中数组名作为参数进行函数传递"></a>C语言中数组名作为参数进行函数传递</h1><p>Tags:C语言<br>在普通变量或下标变量作函数参数时，形参变量和实参变量是由编译系统分配的两个不同的内存单元。在函数调用时发生的值传送是把实参变量的值赋予形参变量。在用数组名作函数参数时，不是进行值的传送，即不是把实参数组的每一个元素的值都赋予形参数组的各个元素。因为实际上形参数组并不存在，编译系统不为形参数组分配内存。那么，数据的传送是如何实现的呢?在我们曾介绍过，数组名就是数组的首地址。<strong><em>因此在数组名作函数参数时所进行的传送只是地址的传送，也就是说把实参数组的首地址赋予形参数组名。</em></strong>形参数组名取得该首地址之后，也就等于有了实在的数组。实际上是形参数组和实参数组为同一数组，共同拥有一段内存空间。<strong><em>因此当形参数组发生变化时，实参数组也随之变化。</em></strong><br><img src="http://c.biancheng.net/cpp/uploads/allimg/120129/343dfgdgd645ccvnbm.gif?_=5778318" alt="图片1"><br>上图说明了这种情形。图中设a为实参数组，类型为整型。a占有以2000为首地址的一块内存区。b为形参数组名。当发生函数调用时，进行地址传送，把实参数组a的首地址传送给形参数组名b，于是b也取得该地址2000。于是a，b两数组共同占有以2000为首地址的一段连续内存单元。从图中还可以看出a和b下标相同的元素实际上也占相同的两个内存单元（整型数组每个元素占二字节）。例如a[0]和b[0]都占用2000和2001单元，当然a[0]等于b[0]。类推则有a[i]等于b[i]。<br>多维数组也可以作为函数的参数。在函数定义时对形参数组可以指定每一维的长度，也可省去第一维的长度。因此，以下写法都是合法的：<br><code>int MA(int a[3][10]);</code>或<code>int MA(int a[][10]);</code></p><p>下面给出两个将数组传递进函数的例子：<br>ex1:</p><pre><code class="c">//求5名学生的平均成绩#include &lt;stdio.h&gt;float aver(float a[5])  //此处函数的定义中数组的元素个数可以省略，由传入的数组决定{    int i;    float av,s=a[0];    for(i=1;i&lt;5;i++)        s=s+a[i];    av=s/5;    return av;}int main(void){    float sco[5],av;    int i;    printf(&quot;\ninput 5 scores:\n&quot;);    for(i=0;i&lt;5;i++)        scanf(&quot;%f&quot;,&amp;sco[i]);    av=aver(sco);  //此处将数组传递进函数时使用的是数组名称，没有括号    printf(&quot;average score is %5.2f&quot;,av);    return 0;}</code></pre><p>ex2:</p><pre><code class="c">//将数组中小于0的元素置成0#include &lt;stdio.h&gt;void nzp(int a[8]){    int i;    printf(&quot;\nvalues of array are:\n&quot;);    for(i=0;i&lt;8;i++)    {        if(a[i]&lt;0)            a[i]=0;        printf(&quot;%d &quot;,a[i]);    }}int main(void){    int b[5],i;    printf(&quot;\ninput 5 numbers:\n&quot;);    for(i=0;i&lt;5;i++)        scanf(&quot;%d&quot;,&amp;b[i]);    printf(&quot;initial values of array b are:\n&quot;);    for(i=0;i&lt;5;i++)        printf(&quot;%d &quot;,b[i]);    nzp(b);  //将有5个元素的数组传递进一个有8个元素的数组的函数，编译通过    printf(&quot;\nlast values of array b are:\n&quot;);    for(i=0;i&lt;5;i++)        printf(&quot;%d &quot;,b[i]);    return 0;}</code></pre>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
