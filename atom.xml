<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Eric_fish&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://yyb1995.github.io/"/>
  <updated>2019-03-20T00:37:13.870Z</updated>
  <id>https://yyb1995.github.io/</id>
  
  <author>
    <name>Eric_fish</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/RNN/Seq2Seq/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/RNN/Seq2Seq/注意力机制/</id>
    <published>2019-06-15T15:54:34.416Z</published>
    <updated>2019-03-20T00:37:13.870Z</updated>
    
    <content type="html"><![CDATA[<h1 id="RNN中的注意力机制"><a href="#RNN中的注意力机制" class="headerlink" title="RNN中的注意力机制"></a>RNN中的注意力机制</h1><h2 id="1-一文解读NLP中的注意力机制"><a href="#1-一文解读NLP中的注意力机制" class="headerlink" title="1. 一文解读NLP中的注意力机制"></a>1. <a href="https://mp.weixin.qq.com/s/TM5poGwSGi5C9szO13GYxg" target="_blank" rel="noopener">一文解读NLP中的注意力机制</a></h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;RNN中的注意力机制&quot;&gt;&lt;a href=&quot;#RNN中的注意力机制&quot; class=&quot;headerlink&quot; title=&quot;RNN中的注意力机制&quot;&gt;&lt;/a&gt;RNN中的注意力机制&lt;/h1&gt;&lt;h2 id=&quot;1-一文解读NLP中的注意力机制&quot;&gt;&lt;a href=&quot;#1-一文解
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/RNN/Seq2Seq/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/RNN/Seq2Seq/参考资料/</id>
    <published>2019-06-15T15:54:34.414Z</published>
    <updated>2019-02-25T02:05:48.387Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Sequence-to-Sequence参考资料"><a href="#Sequence-to-Sequence参考资料" class="headerlink" title="Sequence to Sequence参考资料"></a>Sequence to Sequence参考资料</h1><h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><ol><li><a href="https://arxiv.org/pdf/1409.3215.pdf" target="_blank" rel="noopener">Sequence to Sequence Learning with Neural Networks</a></li><li><a href="https://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a></li><li><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Neural Machine Translation By Jointly Learning To Align and Translate</a></li></ol><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><ol><li><a href="https://blog.csdn.net/Jerr__y/article/details/53749693" target="_blank" rel="noopener">seq2seq学习笔记</a></li><li><a href="http://jacoxu.com/encoder_decoder/" target="_blank" rel="noopener">漫谈四种神经网络序列解码模型</a></li></ol><h2 id="实践应用"><a href="#实践应用" class="headerlink" title="实践应用"></a>实践应用</h2><ol><li><a href="https://blog.csdn.net/tensorflowshizhan/article/details/69230070" target="_blank" rel="noopener">TensorFlow文本摘要生成 - 基于注意力的序列到序列模型</a></li><li><a href="https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/" target="_blank" rel="noopener">How to Develop an Encoder-Decoder Model for Sequence-to-Sequence Prediction in Keras</a></li><li><a href="https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/" target="_blank" rel="noopener">How to Define an Encoder-Decoder Sequence-to-Sequence Model for Neural Machine Translation in Keras</a></li><li><a href="https://zhuanlan.zhihu.com/p/27608348" target="_blank" rel="noopener">知乎sequence_to_sequence项目</a></li></ol><h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><ol><li><a href="https://blog.csdn.net/malefactor/article/details/50550211" target="_blank" rel="noopener">自然语言处理中的Attention Model：是什么及为什么</a></li><li><a href="https://www.jianshu.com/p/ed058614b73d?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation" target="_blank" rel="noopener">NLP中的Attention Model</a></li><li><a href="http://www.cnblogs.com/robert-dlut/p/8638283.html" target="_blank" rel="noopener">自然语言处理中的自注意力机制（Self-attention Mechanism）</a></li></ol><h2 id="Attention-is-all-you-need"><a href="#Attention-is-all-you-need" class="headerlink" title="Attention is all you need"></a>Attention is all you need</h2><ol><li><p><strong><a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">详细图解attention is all you need</a></strong></p></li><li><p><strong><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">详细解释加pytorch实现</a></strong></p></li><li><p><a href="http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/" target="_blank" rel="noopener">另一份详细解释</a></p></li><li><p><a href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/" target="_blank" rel="noopener">attention is all you need解读</a></p></li><li><p><a href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/" target="_blank" rel="noopener">The Transformer – Attention is all you need.</a></p></li><li><p><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">pytorch实现</a></p></li><li><p><a href="https://juejin.im/post/5b9f1af0e51d450e425eb32d" target="_blank" rel="noopener">中文资料加pytorch实现</a></p></li><li><p><a href="https://github.com/tensorflow/models/tree/master/official/transformer/model" target="_blank" rel="noopener">Google官方tensorflow实现</a></p></li><li><p><a href="https://segmentfault.com/a/1190000015575985" target="_blank" rel="noopener">中文资料</a></p></li><li><p><a href="https://www.cnblogs.com/guoyaohua/p/transformer.html" target="_blank" rel="noopener">Transformer各层图示</a></p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Sequence-to-Sequence参考资料&quot;&gt;&lt;a href=&quot;#Sequence-to-Sequence参考资料&quot; class=&quot;headerlink&quot; title=&quot;Sequence to Sequence参考资料&quot;&gt;&lt;/a&gt;Sequence to Se
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/RNN/LSTM/%E5%AF%B9%E4%BA%8ELSTM%E8%BE%93%E5%85%A5%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/RNN/LSTM/对于LSTM输入层的理解/</id>
    <published>2019-06-15T15:54:34.409Z</published>
    <updated>2019-02-25T02:01:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="对于LSTM输入层、隐含层及输出层参数的理解"><a href="#对于LSTM输入层、隐含层及输出层参数的理解" class="headerlink" title="对于LSTM输入层、隐含层及输出层参数的理解"></a>对于LSTM输入层、隐含层及输出层参数的理解</h1><hr><p>LSTM输入层要求的维度是三维的，其中包含三个参数:batch_size, input_dim和time_step。隐含层有一个参数：n_hidden。输出层有两个参数：n_hidden和output_dim。下面举两个例子：<a href="https://blog.csdn.net/mebiuw/article/details/52705731" target="_blank" rel="noopener">利用LSTM识别MNIST手写数字集</a>和<a href="https://blog.csdn.net/a819825294/article/details/54376781" target="_blank" rel="noopener">LSTM时间序列分析</a>，谈谈个人对这些参数含义的理解。</p><h2 id="1-利用LSTM识别MNIST手写数字集"><a href="#1-利用LSTM识别MNIST手写数字集" class="headerlink" title="1.利用LSTM识别MNIST手写数字集"></a>1.利用LSTM识别MNIST手写数字集</h2><pre><code class="python">n_input = 28  # 输入层的n n_steps = 28  # 28长度 n_hidden = 128  # 隐含层的特征数 n_classes = 10  # 输出的数量，因为是分类问题，0~9个数字，这里一共有10个batch_size = 128  </code></pre><p><strong>输入层</strong>：首先说下batch_size。这个参数其实和其他神经网络的batch_size意义相同，都指一次性输入到神经网络中训练的个数。这里batch_size=128，含义是一次性将128个图像输入到LSTM中进行训练，完成一次参数计算和更新。再说说n_steps。n_steps实际上指的是构造的LSTM总共有多少个时间上的输入。在这里取n_step = 28，指的是按时间顺序依次输入28次，在同一时刻输入的个数为batch_size * n_input。在MNIST数据集中，一幅图片表示为28*28的矩阵，因此如果一次输入1行，那么要先后依次输入28行才能将一个图片的信息完全输入。那么同时input_dim（在此处为n_input）的含义也很清楚了，就是一次输入的数据维数，在这里就是1行的数据个数。因此，输入端的操作是，在t时刻输入128幅图片的第1行矩阵，t+1时刻输入128幅图片的第2行矩阵。以此类推直到输入完毕。<br><strong>隐含层</strong>：隐含层只有一个新的参数：n_hidden。这个参数表示的是用于记忆和储存过去状态的节点个数。<br><strong>输出层</strong>：输出层也只有一个新的参数：output_dim（在此处为n_classes）。这个参数的含义是输出结果维数。在MNIST数据集中，由于做的是0~9的分类，所以输出维度自然是10，类似于softmax分类。</p><h2 id="2-LSTM时间序列分析"><a href="#2-LSTM时间序列分析" class="headerlink" title="2.LSTM时间序列分析"></a>2.LSTM时间序列分析</h2><p><strong>输入层</strong>：在这个例子中，使用了Keras作为搭建LSTM工具。查看Keras的文档，得知其对输入数据的要求是</p><blockquote><p>形如（samples，timesteps，input_dim）的3D张量</p></blockquote><p>而第二个例子中对于输入数据做的处理为<br><code>x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))</code><br>因此不难比较得到：</p><pre><code class="python">batch_size = x_train.shape[0]time_steps = x_train.shape[1]input_dim = 1</code></pre><p>由于这个例子是给定一个已知序列，对该序列接下来的走势进行预测，因此自然而然想到把一个序列切成训练集和测试集，训练集再根据合适的时间长度分成t~(t+n)的训练集和t+n+1的测试集。那么batch_size的含义是一次性输入训练的序列数。time_step为取的一个时间序列的长度，也就是上一句话的n。在这个例子中，input_dim为1，说明在一个时间点，一个序列只输入1个点。隐含层和输出层类似，不再重复。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;对于LSTM输入层、隐含层及输出层参数的理解&quot;&gt;&lt;a href=&quot;#对于LSTM输入层、隐含层及输出层参数的理解&quot; class=&quot;headerlink&quot; title=&quot;对于LSTM输入层、隐含层及输出层参数的理解&quot;&gt;&lt;/a&gt;对于LSTM输入层、隐含层及输出层参数的
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/RNN/LSTM/TimeDistributedDense()%E5%B1%82/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/RNN/LSTM/TimeDistributedDense()层/</id>
    <published>2019-06-15T15:54:34.406Z</published>
    <updated>2019-02-25T02:01:44.000Z</updated>
    
    <content type="html"><![CDATA[<ol><li><a href="https://github.com/keras-team/keras/issues/2405" target="_blank" rel="noopener">https://github.com/keras-team/keras/issues/2405</a></li><li><a href="https://github.com/CanePunma/Stock_Price_Prediction_With_RNNs/issues/1" target="_blank" rel="noopener">https://github.com/CanePunma/Stock_Price_Prediction_With_RNNs/issues/1</a></li><li><a href="https://stackoverflow.com/questions/42398645/timedistributed-vs-timedistributeddense-keras" target="_blank" rel="noopener">https://stackoverflow.com/questions/42398645/timedistributed-vs-timedistributeddense-keras</a></li><li><a href="https://stackoverflow.com/questions/44611006/timedistributeddense-vs-dense-in-keras-same-number-of-parameters" target="_blank" rel="noopener">https://stackoverflow.com/questions/44611006/timedistributeddense-vs-dense-in-keras-same-number-of-parameters</a></li><li><a href="https://stackoverflow.com/questions/45631235/importerror-cannot-import-name-timedistributeddense-in-keras" target="_blank" rel="noopener">https://stackoverflow.com/questions/45631235/importerror-cannot-import-name-timedistributeddense-in-keras</a></li><li><a href="https://stackoverflow.com/questions/41947039/keras-rnn-with-lstm-cells-for-predicting-multiple-output-time-series-based-on-mu" target="_blank" rel="noopener">https://stackoverflow.com/questions/41947039/keras-rnn-with-lstm-cells-for-predicting-multiple-output-time-series-based-on-mu</a></li><li><a href="https://stackoverflow.com/questions/49661708/keras-lstm-multiple-input-multiple-output" target="_blank" rel="noopener">https://stackoverflow.com/questions/49661708/keras-lstm-multiple-input-multiple-output</a></li><li><a href="https://blog.csdn.net/oQiCheng1234567/article/details/73051251" target="_blank" rel="noopener">https://blog.csdn.net/oQiCheng1234567/article/details/73051251</a></li><li><a href="https://github.com/keras-team/keras/issues/1029" target="_blank" rel="noopener">https://github.com/keras-team/keras/issues/1029</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/keras-team/keras/issues/2405&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/keras-team/keras/issues
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/RNN/LSTM/LSTM%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%BB%9E%E5%90%8E%E9%97%AE%E9%A2%98/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/RNN/LSTM/LSTM时序预测滞后问题/</id>
    <published>2019-06-15T15:54:34.404Z</published>
    <updated>2019-02-25T02:01:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LSTM时序预测滞后问题"><a href="#LSTM时序预测滞后问题" class="headerlink" title="LSTM时序预测滞后问题"></a>LSTM时序预测滞后问题</h1><ol><li><p><a href="https://blog.csdn.net/aliceyangxi1987/article/details/73420583" target="_blank" rel="noopener">用 LSTM 做时间序列预测的一个小例子</a></p></li><li><p><a href="http://www.ilovematlab.cn/thread-165451-1-1.html" target="_blank" rel="noopener">时间序列预测—-预测的结果跟实际的时间序列值存在滞后</a></p></li><li><p><a href="https://blog.csdn.net/CS13522431352/article/details/77369300?locationNum=7" target="_blank" rel="noopener">代码干货 | 基于Keras的LSTM多变量时间序列预测</a></p></li><li><p><a href="https://www.jianshu.com/p/5d6d5aac4dbd" target="_blank" rel="noopener">但如果只用time series数据，你很有可能得到的就是滞后一天的趋势</a></p></li><li><p><a href="https://github.com/owoshch/time_series/blob/master/airline_prediction_one_lstm_layer_with_time_steps.ipynb" target="_blank" rel="noopener">一个带输出图像的LSTM预测</a></p></li><li><p><a href="https://www.douban.com/group/topic/102741080/" target="_blank" rel="noopener">用LSTM预测时间序列存在延迟现象？</a></p></li><li><p><a href="https://stats.stackexchange.com/questions/307340/role-of-delays-in-lstm-networks#comment587967_307340" target="_blank" rel="noopener">Stackexchange problem</a></p></li><li><p><a href="https://stackoverflow.com/questions/35563758/delay-issue-in-time-series-prediction" target="_blank" rel="noopener">Stackoverflow-Delay issue in time series prediction</a></p></li><li><p><a href="https://github.com/keras-team/keras/issues/2856" target="_blank" rel="noopener">LSTM for time series prediction</a></p></li><li><p><a href="http://www.jakob-aungiers.com/articles/a/LSTM-Neural-Network-for-Time-Series-Prediction" target="_blank" rel="noopener">LSTM Neural Network for Time Series Prediction</a></p></li><li><p><a href="https://www.zhihu.com/question/21229371" target="_blank" rel="noopener">知乎问题-Pyhong的答案</a></p></li><li><p><a href="https://dashee87.github.io/deep%20learning/python/predicting-cryptocurrency-prices-with-deep-learning/" target="_blank" rel="noopener">https://dashee87.github.io/deep%20learning/python/predicting-cryptocurrency-prices-with-deep-learning/</a></p></li><li><p><a href="https://medium.com/@siavash_37715/how-to-predict-bitcoin-and-ethereum-price-with-rnn-lstm-in-keras-a6d8ee8a5109" target="_blank" rel="noopener">https://medium.com/@siavash_37715/how-to-predict-bitcoin-and-ethereum-price-with-rnn-lstm-in-keras-a6d8ee8a5109</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/31783805" target="_blank" rel="noopener">最全 LSTM 模型在量化交易中的应用汇总（代码+论文）</a></p></li><li><p><a href="https://www.kaggle.com/pablocastilla/predict-stock-prices-with-lstm" target="_blank" rel="noopener">https://www.kaggle.com/pablocastilla/predict-stock-prices-with-lstm</a></p></li><li><p><a href="https://stackoverflow.com/questions/48034625/keras-lstm-predicted-timeseries-squashed-and-shifted/48050810#48050810" target="_blank" rel="noopener">useful discussion</a></p></li><li><p><a href="https://stackoverflow.com/questions/49697457/lstm-nn-produces-shifted-forecast-low-quality-result/49700184#49700184" target="_blank" rel="noopener">another discussion</a></p></li><li><p><a href="https://stackoverflow.com/questions/39139446/keras-lstm-rnn-forecast-shifting-fitted-forecast-backward" target="_blank" rel="noopener">another another discussion</a></p></li><li><p><a href="https://jiasuhui.com/article/3855" target="_blank" rel="noopener">https://jiasuhui.com/article/3855</a></p></li><li><p><a href="https://www.zhihu.com/question/275040228" target="_blank" rel="noopener">知乎问题</a></p></li><li><p><a href="http://www.cnblogs.com/xuruilong100/p/8451790.html" target="_blank" rel="noopener">延时现象解决</a></p></li><li><p><a href="https://github.com/keras-team/keras/issues/2856" target="_blank" rel="noopener">github上的讨论</a></p></li><li><p><a href="https://www.researchgate.net/post/How_can_I_decrease_the_ANN_forecasting_delay" target="_blank" rel="noopener">ResearchGate上的讨论</a></p></li><li><p><strong>possible solution</strong></p><ol><li>randomize training samples in each batch, make sure they are not followed one by one</li><li>choose or design a better loss function other than MSE</li><li>extract some features from the input time series</li><li>manually limit the weight of x_{t-1}, x_{t-2}</li></ol></li><li><p><a href="https://www.datacamp.com/community/tutorials/lstm-python-stock-market" target="_blank" rel="noopener">https://www.datacamp.com/community/tutorials/lstm-python-stock-market</a></p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;LSTM时序预测滞后问题&quot;&gt;&lt;a href=&quot;#LSTM时序预测滞后问题&quot; class=&quot;headerlink&quot; title=&quot;LSTM时序预测滞后问题&quot;&gt;&lt;/a&gt;LSTM时序预测滞后问题&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://blog
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/RNN/LSTM/LSTM/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/RNN/LSTM/LSTM/</id>
    <published>2019-06-15T15:54:34.402Z</published>
    <updated>2019-02-25T02:01:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><h2 id="1-LSTM原理"><a href="#1-LSTM原理" class="headerlink" title="1. LSTM原理"></a>1. LSTM原理</h2><ol><li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">English version：Understanding LSTM Networks</a></li><li><a href="https://www.jianshu.com/p/4b4701beba92" target="_blank" rel="noopener">中文版：如何简单的理解LSTM——其实没有那么复杂</a></li><li><a href="https://zybuluo.com/hanbingtao/note/581764" target="_blank" rel="noopener">作业部落</a></li><li><a href="https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714" target="_blank" rel="noopener">好看的图</a></li></ol><h2 id="2-LSTM基础应用"><a href="#2-LSTM基础应用" class="headerlink" title="2. LSTM基础应用"></a>2. LSTM基础应用</h2><ol><li><a href="https://yq.aliyun.com/articles/202939" target="_blank" rel="noopener">LSTM在MNIST数据集中的运用</a></li></ol><h2 id="3-LSTM实战"><a href="#3-LSTM实战" class="headerlink" title="3. LSTM实战"></a>3. LSTM实战</h2><ol><li><a href="https://blog.csdn.net/mylove0414/article/details/56969181" target="_blank" rel="noopener"># Tensorflow实例：利用LSTM预测股票每日最高价（二）</a></li><li><a href="https://blog.csdn.net/flying_sfeng/article/details/78852816" target="_blank" rel="noopener">使用tensorflow的lstm网络进行时间序列预测</a></li><li><a href="https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/" target="_blank" rel="noopener">基于单变量的LSTM时间序列预测</a></li><li><a href="https://yq.aliyun.com/articles/174270" target="_blank" rel="noopener">基于多变量的LSTM时间序列预测</a></li><li><a href="https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/" target="_blank" rel="noopener">基于LSTM的多步预测</a></li><li><a href="https://www.jianshu.com/p/5d6d5aac4dbd" target="_blank" rel="noopener">实用LSTM时间预测例子</a></li><li><a href="https://yq.aliyun.com/articles/68463" target="_blank" rel="noopener">多层LSTM网络：教你打造股市晴雨表——通过LSTM神经网络预测股市</a></li><li><a href="https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/" target="_blank" rel="noopener">LSTM英文句子预测问题</a></li><li><a href="http://www.willfleury.com/machine-learning/forecasting/lstm/2017/09/01/short-term-forceasting-lstm.html" target="_blank" rel="noopener">lstm时间序列预测程序</a></li></ol><h2 id="4-LSTM相关问题解答"><a href="#4-LSTM相关问题解答" class="headerlink" title="4. LSTM相关问题解答"></a>4. LSTM相关问题解答</h2><ol><li><a href="https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell#comment83076885_37901047" target="_blank" rel="noopener">tensorflow中LSTM模型的n_hidden含义</a><h2 id="5-LSTM论文及资料"><a href="#5-LSTM论文及资料" class="headerlink" title="5. LSTM论文及资料"></a>5. LSTM论文及资料</h2><a href="http://suanfazu.com/t/rnn-lstm/13587" target="_blank" rel="noopener">rnn和lstm资源收集</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;LSTM&quot;&gt;&lt;a href=&quot;#LSTM&quot; class=&quot;headerlink&quot; title=&quot;LSTM&quot;&gt;&lt;/a&gt;LSTM&lt;/h1&gt;&lt;h2 id=&quot;1-LSTM原理&quot;&gt;&lt;a href=&quot;#1-LSTM原理&quot; class=&quot;headerlink&quot; title=&quot;1
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/RNN/LSTM/LSTM%20stateful%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/RNN/LSTM/LSTM stateful的理解/</id>
    <published>2019-06-15T15:54:34.400Z</published>
    <updated>2019-02-25T02:01:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LSTM-stateful的理解"><a href="#LSTM-stateful的理解" class="headerlink" title="LSTM stateful的理解"></a>LSTM stateful的理解</h1><ol><li><a href="http://philipperemy.github.io/keras-stateful-lstm/" target="_blank" rel="noopener">Stateful LSTM in Keras</a></li><li><a href="https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/" target="_blank" rel="noopener">Understanding Stateful LSTM Recurrent Neural Networks in Python with Keras</a><blockquote><p>The LSTM networks are stateful. They should be able to learn the whole alphabet sequence, but by default the Keras implementation resets the network state after each training batch.</p></blockquote></li><li><a href="https://stackoverflow.com/questions/43882796/when-does-keras-reset-an-lstm-state" target="_blank" rel="noopener">https://stackoverflow.com/questions/43882796/when-does-keras-reset-an-lstm-state</a></li><li><a href="https://ahstat.github.io/RNN-Keras-time-series/" target="_blank" rel="noopener">https://ahstat.github.io/RNN-Keras-time-series/</a></li><li><a href="https://zhuanlan.zhihu.com/p/34495801#comment-465502125" target="_blank" rel="noopener">Keras之stateful LSTM全面解析+实例测试</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;LSTM-stateful的理解&quot;&gt;&lt;a href=&quot;#LSTM-stateful的理解&quot; class=&quot;headerlink&quot; title=&quot;LSTM stateful的理解&quot;&gt;&lt;/a&gt;LSTM stateful的理解&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;a href=
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/RNN/RNN%E4%B8%80%E4%BA%9B%E6%96%B0%E7%9A%84%E5%8F%91%E5%B1%95%E6%96%B9%E5%90%91/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/RNN/RNN一些新的发展方向/</id>
    <published>2019-06-15T15:54:34.396Z</published>
    <updated>2019-02-25T02:03:58.442Z</updated>
    
    <content type="html"><![CDATA[<h1 id="some-development-of-RNN"><a href="#some-development-of-RNN" class="headerlink" title="some development of RNN"></a>some development of RNN</h1><ol><li><p><a href="https://distill.pub/2016/augmented-rnns/#neural-turing-machines" target="_blank" rel="noopener">Attention and Augmented Recurrent Neural Networks</a></p></li><li><p><a href="https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/43795" target="_blank" rel="noopener">Web Traffic Time Series Forecasting Forecast future traffic to Wikipedia pages</a></p><ol><li><a href="https://github.com/Arturus/kaggle-web-traffic" target="_blank" rel="noopener">github project</a></li></ol></li><li><p><a href="https://github.com/ysn2233/attentioned-dual-stage-stock-prediction" target="_blank" rel="noopener">Pytorch dual attention model</a></p></li><li><p><a href="https://www.jianshu.com/p/0d1e9f0db887" target="_blank" rel="noopener">使用Attentioned Dual-Stage RNN模型预测股票(PyTorch)</a></p></li><li><p><a href="http://chandlerzuo.github.io/blog/2017/11/darnn" target="_blank" rel="noopener">A PyTorch Example to Use RNN for Financial Prediction</a></p><ol><li><a href="https://github.com/Seanny123/da-rnn" target="_blank" rel="noopener">github project</a></li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;some-development-of-RNN&quot;&gt;&lt;a href=&quot;#some-development-of-RNN&quot; class=&quot;headerlink&quot; title=&quot;some development of RNN&quot;&gt;&lt;/a&gt;some development 
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/other%20techniques/layer%20normalization/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/other techniques/layer normalization/</id>
    <published>2019-06-15T15:54:34.389Z</published>
    <updated>2019-02-25T02:01:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="batch-amp-layer-normalization-参考资料"><a href="#batch-amp-layer-normalization-参考资料" class="headerlink" title="batch &amp; layer normalization 参考资料"></a>batch &amp; layer normalization 参考资料</h1><ol><li><a href="http://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/" target="_blank" rel="noopener">Weight Normalization and Layer Normalization Explained </a></li><li><a href="https://medium.com/@ilango100/batch-normalization-speed-up-neural-network-training-245e39a62f85" target="_blank" rel="noopener">Batch Normalization详解</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;batch-amp-layer-normalization-参考资料&quot;&gt;&lt;a href=&quot;#batch-amp-layer-normalization-参考资料&quot; class=&quot;headerlink&quot; title=&quot;batch &amp;amp; layer normal
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/NLP/Word2vec%E8%B5%84%E6%96%99/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/NLP/Word2vec资料/</id>
    <published>2019-06-15T15:54:34.378Z</published>
    <updated>2019-02-21T09:08:09.181Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Word2vec相关资料"><a href="#Word2vec相关资料" class="headerlink" title="Word2vec相关资料"></a>Word2vec相关资料</h1><ol><li><a href="https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285" target="_blank" rel="noopener"><strong>Word embedding</strong></a></li><li><a href="https://www.cnblogs.com/pinard/p/7160330.html" target="_blank" rel="noopener">word2vec原理(一) CBOW与Skip-Gram模型基础</a></li><li><a href="https://www.cnblogs.com/pinard/p/7243513.html" target="_blank" rel="noopener">word2vec原理(二) 基于Hierarchical Softmax的模型</a></li><li><a href="https://www.cnblogs.com/pinard/p/7249903.html" target="_blank" rel="noopener">word2vec原理(三) 基于Negative Sampling的模型</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Word2vec相关资料&quot;&gt;&lt;a href=&quot;#Word2vec相关资料&quot; class=&quot;headerlink&quot; title=&quot;Word2vec相关资料&quot;&gt;&lt;/a&gt;Word2vec相关资料&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://medium.
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/NLP/NLP%E5%8F%91%E5%B1%95%E6%96%B0%E6%96%B9%E5%90%91/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/NLP/NLP发展新方向/</id>
    <published>2019-06-15T15:54:34.375Z</published>
    <updated>2019-06-11T01:51:29.028Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NLP新的发展方向"><a href="#NLP新的发展方向" class="headerlink" title="NLP新的发展方向"></a>NLP新的发展方向</h1><ol><li><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247493520&idx=1&sn=2b04c009ef75291ef3d19e8fe673aa36&chksm=96ea3810a19db10621e7a661974c796e8adeffc31625a769f8db1d87ba803cd58a30d40ad7ce&scene=21#wechat_redirect" target="_blank" rel="noopener">NLP的巨人肩膀上</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247493731&idx=1&sn=51206e4ca3983548436d889590ab5347&chksm=96ea37e3a19dbef5b6db3143eb9df822915126d3d8f61fe73ddb9f8fa329d568ec79a662acb1&mpshare=1&scene=23&srcid=12177Ua04Q6MGcDDUf5HSrL0#rd" target="_blank" rel="noopener">NLP的巨人肩膀下</a></li><li><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" target="_blank" rel="noopener">https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html</a></li><li><a href="https://zhuanlan.zhihu.com/p/54743941" target="_blank" rel="noopener">放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较</a></li><li><a href="https://mp.weixin.qq.com/s/TM5poGwSGi5C9szO13GYxg" target="_blank" rel="noopener">NLP中的attention机制</a></li><li><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247497461&idx=1&sn=2821912a51749ef50a46007249fec26a&chksm=96ea2975a19da0633f3286750088a22c78987628666a6cd2d42d1fbee74f9c189e073d111dc2&mpshare=1&scene=23&srcid=#rd" target="_blank" rel="noopener">ICLR 2019最佳论文 | ON-LSTM：用有序神经元表达层次结构</a></li><li><a href="https://zhuanlan.zhihu.com/p/68446772" target="_blank" rel="noopener">Bert时代的创新（应用篇）：Bert在NLP各领域的应用进展</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;NLP新的发展方向&quot;&gt;&lt;a href=&quot;#NLP新的发展方向&quot; class=&quot;headerlink&quot; title=&quot;NLP新的发展方向&quot;&gt;&lt;/a&gt;NLP新的发展方向&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/LSTM/%E5%AF%B9%E4%BA%8ELSTM%E8%BE%93%E5%85%A5%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/LSTM/对于LSTM输入层的理解/</id>
    <published>2019-06-15T15:54:34.366Z</published>
    <updated>2018-03-29T04:33:46.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="对于LSTM输入层、隐含层及输出层参数的理解"><a href="#对于LSTM输入层、隐含层及输出层参数的理解" class="headerlink" title="对于LSTM输入层、隐含层及输出层参数的理解"></a>对于LSTM输入层、隐含层及输出层参数的理解</h1><hr><p>LSTM输入层要求的维度是三维的，其中包含三个参数:batch_size, input_dim和time_step。隐含层有一个参数：n_hidden。输出层有两个参数：n_hidden和output_dim。下面举两个例子：<a href="https://blog.csdn.net/mebiuw/article/details/52705731" target="_blank" rel="noopener">利用LSTM识别MNIST手写数字集</a>和<a href="https://blog.csdn.net/a819825294/article/details/54376781" target="_blank" rel="noopener">LSTM时间序列分析</a>，谈谈个人对这些参数含义的理解。</p><h2 id="1-利用LSTM识别MNIST手写数字集"><a href="#1-利用LSTM识别MNIST手写数字集" class="headerlink" title="1.利用LSTM识别MNIST手写数字集"></a>1.利用LSTM识别MNIST手写数字集</h2><pre><code class="python">n_input = 28  # 输入层的n n_steps = 28  # 28长度 n_hidden = 128  # 隐含层的特征数 n_classes = 10  # 输出的数量，因为是分类问题，0~9个数字，这里一共有10个batch_size = 128  </code></pre><p><strong>输入层</strong>：首先说下batch_size。这个参数其实和其他神经网络的batch_size意义相同，都指一次性输入到神经网络中训练的个数。这里batch_size=128，含义是一次性将128个图像输入到LSTM中进行训练，完成一次参数计算和更新。再说说n_steps。n_steps实际上指的是构造的LSTM总共有多少个时间上的输入。在这里取n_step = 28，指的是按时间顺序依次输入28次，在同一时刻输入的个数为batch_size * n_input。在MNIST数据集中，一幅图片表示为28*28的矩阵，因此如果一次输入1行，那么要先后依次输入28行才能将一个图片的信息完全输入。那么同时input_dim（在此处为n_input）的含义也很清楚了，就是一次输入的数据维数，在这里就是1行的数据个数。因此，输入端的操作是，在t时刻输入128幅图片的第1行矩阵，t+1时刻输入128幅图片的第2行矩阵。以此类推直到输入完毕。<br><strong>隐含层</strong>：隐含层只有一个新的参数：n_hidden。这个参数表示的是用于记忆和储存过去状态的节点个数。<br><strong>输出层</strong>：输出层也只有一个新的参数：output_dim（在此处为n_classes）。这个参数的含义是输出结果维数。在MNIST数据集中，由于做的是0~9的分类，所以输出维度自然是10，类似于softmax分类。</p><h2 id="2-LSTM时间序列分析"><a href="#2-LSTM时间序列分析" class="headerlink" title="2.LSTM时间序列分析"></a>2.LSTM时间序列分析</h2><p><strong>输入层</strong>：在这个例子中，使用了Keras作为搭建LSTM工具。查看Keras的文档，得知其对输入数据的要求是</p><blockquote><p>形如（samples，timesteps，input_dim）的3D张量</p></blockquote><p>而第二个例子中对于输入数据做的处理为<br><code>x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))</code><br>因此不难比较得到：</p><pre><code class="python">batch_size = x_train.shape[0]time_steps = x_train.shape[1]input_dim = 1</code></pre><p>由于这个例子是给定一个已知序列，对该序列接下来的走势进行预测，因此自然而然想到把一个序列切成训练集和测试集，训练集再根据合适的时间长度分成t~(t+n)的训练集和t+n+1的测试集。那么batch_size的含义是一次性输入训练的序列数。time_step为取的一个时间序列的长度，也就是上一句话的n。在这个例子中，input_dim为1，说明在一个时间点，一个序列只输入1个点。隐含层和输出层类似，不再重复。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;对于LSTM输入层、隐含层及输出层参数的理解&quot;&gt;&lt;a href=&quot;#对于LSTM输入层、隐含层及输出层参数的理解&quot; class=&quot;headerlink&quot; title=&quot;对于LSTM输入层、隐含层及输出层参数的理解&quot;&gt;&lt;/a&gt;对于LSTM输入层、隐含层及输出层参数的
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/LSTM/TimeDistributedDense()%E5%B1%82/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/LSTM/TimeDistributedDense()层/</id>
    <published>2019-06-15T15:54:34.363Z</published>
    <updated>2018-04-16T07:15:50.000Z</updated>
    
    <content type="html"><![CDATA[<ol><li><a href="https://github.com/keras-team/keras/issues/2405" target="_blank" rel="noopener">https://github.com/keras-team/keras/issues/2405</a></li><li><a href="https://github.com/CanePunma/Stock_Price_Prediction_With_RNNs/issues/1" target="_blank" rel="noopener">https://github.com/CanePunma/Stock_Price_Prediction_With_RNNs/issues/1</a></li><li><a href="https://stackoverflow.com/questions/42398645/timedistributed-vs-timedistributeddense-keras" target="_blank" rel="noopener">https://stackoverflow.com/questions/42398645/timedistributed-vs-timedistributeddense-keras</a></li><li><a href="https://stackoverflow.com/questions/44611006/timedistributeddense-vs-dense-in-keras-same-number-of-parameters" target="_blank" rel="noopener">https://stackoverflow.com/questions/44611006/timedistributeddense-vs-dense-in-keras-same-number-of-parameters</a></li><li><a href="https://stackoverflow.com/questions/45631235/importerror-cannot-import-name-timedistributeddense-in-keras" target="_blank" rel="noopener">https://stackoverflow.com/questions/45631235/importerror-cannot-import-name-timedistributeddense-in-keras</a></li><li><a href="https://stackoverflow.com/questions/41947039/keras-rnn-with-lstm-cells-for-predicting-multiple-output-time-series-based-on-mu" target="_blank" rel="noopener">https://stackoverflow.com/questions/41947039/keras-rnn-with-lstm-cells-for-predicting-multiple-output-time-series-based-on-mu</a></li><li><a href="https://stackoverflow.com/questions/49661708/keras-lstm-multiple-input-multiple-output" target="_blank" rel="noopener">https://stackoverflow.com/questions/49661708/keras-lstm-multiple-input-multiple-output</a></li><li><a href="https://blog.csdn.net/oQiCheng1234567/article/details/73051251" target="_blank" rel="noopener">https://blog.csdn.net/oQiCheng1234567/article/details/73051251</a></li><li><a href="https://github.com/keras-team/keras/issues/1029" target="_blank" rel="noopener">https://github.com/keras-team/keras/issues/1029</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/keras-team/keras/issues/2405&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/keras-team/keras/issues
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/LSTM/LSTM%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%BB%9E%E5%90%8E%E9%97%AE%E9%A2%98/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/LSTM/LSTM时序预测滞后问题/</id>
    <published>2019-06-15T15:54:34.361Z</published>
    <updated>2018-07-19T07:01:30.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LSTM时序预测滞后问题"><a href="#LSTM时序预测滞后问题" class="headerlink" title="LSTM时序预测滞后问题"></a>LSTM时序预测滞后问题</h1><ol><li><a href="https://blog.csdn.net/aliceyangxi1987/article/details/73420583" target="_blank" rel="noopener">用 LSTM 做时间序列预测的一个小例子</a></li><li><a href="http://www.ilovematlab.cn/thread-165451-1-1.html" target="_blank" rel="noopener">时间序列预测—-预测的结果跟实际的时间序列值存在滞后</a></li><li><a href="https://blog.csdn.net/CS13522431352/article/details/77369300?locationNum=7" target="_blank" rel="noopener">代码干货 | 基于Keras的LSTM多变量时间序列预测</a></li><li><a href="https://www.jianshu.com/p/5d6d5aac4dbd" target="_blank" rel="noopener">但如果只用time series数据，你很有可能得到的就是滞后一天的趋势</a></li><li><a href="https://github.com/owoshch/time_series/blob/master/airline_prediction_one_lstm_layer_with_time_steps.ipynb" target="_blank" rel="noopener">一个带输出图像的LSTM预测</a></li><li><a href="https://www.douban.com/group/topic/102741080/" target="_blank" rel="noopener">用LSTM预测时间序列存在延迟现象？</a></li><li><a href="https://stats.stackexchange.com/questions/307340/role-of-delays-in-lstm-networks#comment587967_307340" target="_blank" rel="noopener">Stackexchange problem</a></li><li><a href="https://stackoverflow.com/questions/35563758/delay-issue-in-time-series-prediction" target="_blank" rel="noopener">Stackoverflow-Delay issue in time series prediction</a></li><li><a href="https://github.com/keras-team/keras/issues/2856" target="_blank" rel="noopener">LSTM for time series prediction</a></li><li><a href="http://www.jakob-aungiers.com/articles/a/LSTM-Neural-Network-for-Time-Series-Prediction" target="_blank" rel="noopener">LSTM Neural Network for Time Series Prediction</a></li><li><a href="https://www.zhihu.com/question/21229371" target="_blank" rel="noopener">知乎问题-Pyhong的答案</a></li><li><a href="https://dashee87.github.io/deep%20learning/python/predicting-cryptocurrency-prices-with-deep-learning/" target="_blank" rel="noopener">https://dashee87.github.io/deep%20learning/python/predicting-cryptocurrency-prices-with-deep-learning/</a></li><li><a href="https://medium.com/@siavash_37715/how-to-predict-bitcoin-and-ethereum-price-with-rnn-lstm-in-keras-a6d8ee8a5109" target="_blank" rel="noopener">https://medium.com/@siavash_37715/how-to-predict-bitcoin-and-ethereum-price-with-rnn-lstm-in-keras-a6d8ee8a5109</a></li><li><a href="https://zhuanlan.zhihu.com/p/31783805" target="_blank" rel="noopener">最全 LSTM 模型在量化交易中的应用汇总（代码+论文）</a></li><li><a href="https://www.kaggle.com/pablocastilla/predict-stock-prices-with-lstm" target="_blank" rel="noopener">https://www.kaggle.com/pablocastilla/predict-stock-prices-with-lstm</a></li><li><a href="https://stackoverflow.com/questions/48034625/keras-lstm-predicted-timeseries-squashed-and-shifted/48050810#48050810" target="_blank" rel="noopener">useful discussion</a></li><li><a href="https://stackoverflow.com/questions/49697457/lstm-nn-produces-shifted-forecast-low-quality-result/49700184#49700184" target="_blank" rel="noopener">another discussion</a></li><li><a href="https://stackoverflow.com/questions/39139446/keras-lstm-rnn-forecast-shifting-fitted-forecast-backward" target="_blank" rel="noopener">another another discussion</a></li><li><a href="https://jiasuhui.com/article/3855" target="_blank" rel="noopener">https://jiasuhui.com/article/3855</a></li><li><a href="https://www.zhihu.com/question/275040228" target="_blank" rel="noopener">知乎问题</a></li><li><a href="http://www.cnblogs.com/xuruilong100/p/8451790.html" target="_blank" rel="noopener">延时现象解决</a></li><li><a href="https://github.com/keras-team/keras/issues/2856" target="_blank" rel="noopener">github上的讨论</a></li><li><a href="https://www.researchgate.net/post/How_can_I_decrease_the_ANN_forecasting_delay" target="_blank" rel="noopener">ResearchGate上的讨论</a></li><li><strong>possible solution</strong><ol><li>randomize training samples in each batch, make sure they are not followed one by one</li><li>choose or design a better loss function other than MSE</li><li>extract some features from the input time series</li><li>manually limit the weight of x_{t-1}, x_{t-2}</li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;LSTM时序预测滞后问题&quot;&gt;&lt;a href=&quot;#LSTM时序预测滞后问题&quot; class=&quot;headerlink&quot; title=&quot;LSTM时序预测滞后问题&quot;&gt;&lt;/a&gt;LSTM时序预测滞后问题&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.cs
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/LSTM/LSTM/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/LSTM/LSTM/</id>
    <published>2019-06-15T15:54:34.358Z</published>
    <updated>2018-05-23T00:52:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><h2 id="1-LSTM原理"><a href="#1-LSTM原理" class="headerlink" title="1. LSTM原理"></a>1. LSTM原理</h2><ol><li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">English version：Understanding LSTM Networks</a></li><li><a href="https://www.jianshu.com/p/4b4701beba92" target="_blank" rel="noopener">中文版：如何简单的理解LSTM——其实没有那么复杂</a></li><li><a href="https://zybuluo.com/hanbingtao/note/581764" target="_blank" rel="noopener">作业部落</a></li><li><a href="https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714" target="_blank" rel="noopener">好看的图</a></li></ol><h2 id="2-LSTM基础应用"><a href="#2-LSTM基础应用" class="headerlink" title="2. LSTM基础应用"></a>2. LSTM基础应用</h2><ol><li><a href="https://yq.aliyun.com/articles/202939" target="_blank" rel="noopener">LSTM在MNIST数据集中的运用</a></li></ol><h2 id="3-LSTM实战"><a href="#3-LSTM实战" class="headerlink" title="3. LSTM实战"></a>3. LSTM实战</h2><ol><li><a href="https://blog.csdn.net/mylove0414/article/details/56969181" target="_blank" rel="noopener"># Tensorflow实例：利用LSTM预测股票每日最高价（二）</a></li><li><a href="https://blog.csdn.net/flying_sfeng/article/details/78852816" target="_blank" rel="noopener">使用tensorflow的lstm网络进行时间序列预测</a></li><li><a href="https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/" target="_blank" rel="noopener">基于单变量的LSTM时间序列预测</a></li><li><a href="https://yq.aliyun.com/articles/174270" target="_blank" rel="noopener">基于多变量的LSTM时间序列预测</a></li><li><a href="https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/" target="_blank" rel="noopener">基于LSTM的多步预测</a></li><li><a href="https://www.jianshu.com/p/5d6d5aac4dbd" target="_blank" rel="noopener">实用LSTM时间预测例子</a></li><li><a href="https://yq.aliyun.com/articles/68463" target="_blank" rel="noopener">多层LSTM网络：教你打造股市晴雨表——通过LSTM神经网络预测股市</a></li><li><a href="https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/" target="_blank" rel="noopener">LSTM英文句子预测问题</a></li><li><a href="http://www.willfleury.com/machine-learning/forecasting/lstm/2017/09/01/short-term-forceasting-lstm.html" target="_blank" rel="noopener">lstm时间序列预测程序</a></li></ol><h2 id="4-LSTM相关问题解答"><a href="#4-LSTM相关问题解答" class="headerlink" title="4. LSTM相关问题解答"></a>4. LSTM相关问题解答</h2><ol><li><a href="https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell#comment83076885_37901047" target="_blank" rel="noopener">tensorflow中LSTM模型的n_hidden含义</a><h2 id="5-LSTM论文及资料"><a href="#5-LSTM论文及资料" class="headerlink" title="5. LSTM论文及资料"></a>5. LSTM论文及资料</h2><a href="http://suanfazu.com/t/rnn-lstm/13587" target="_blank" rel="noopener">rnn和lstm资源收集</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;LSTM&quot;&gt;&lt;a href=&quot;#LSTM&quot; class=&quot;headerlink&quot; title=&quot;LSTM&quot;&gt;&lt;/a&gt;LSTM&lt;/h1&gt;&lt;h2 id=&quot;1-LSTM原理&quot;&gt;&lt;a href=&quot;#1-LSTM原理&quot; class=&quot;headerlink&quot; title=&quot;1
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/LSTM/LSTM%20stateful%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/LSTM/LSTM stateful的理解/</id>
    <published>2019-06-15T15:54:34.356Z</published>
    <updated>2018-06-10T09:08:42.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LSTM-stateful的理解"><a href="#LSTM-stateful的理解" class="headerlink" title="LSTM stateful的理解"></a>LSTM stateful的理解</h1><ol><li><a href="http://philipperemy.github.io/keras-stateful-lstm/" target="_blank" rel="noopener">Stateful LSTM in Keras</a></li><li><a href="https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/" target="_blank" rel="noopener">Understanding Stateful LSTM Recurrent Neural Networks in Python with Keras</a><blockquote><p>The LSTM networks are stateful. They should be able to learn the whole alphabet sequence, but by default the Keras implementation resets the network state after each training batch.</p></blockquote></li><li><a href="https://stackoverflow.com/questions/43882796/when-does-keras-reset-an-lstm-state" target="_blank" rel="noopener">https://stackoverflow.com/questions/43882796/when-does-keras-reset-an-lstm-state</a></li><li><a href="https://ahstat.github.io/RNN-Keras-time-series/" target="_blank" rel="noopener">https://ahstat.github.io/RNN-Keras-time-series/</a></li><li><a href="https://zhuanlan.zhihu.com/p/34495801#comment-465502125" target="_blank" rel="noopener">Keras之stateful LSTM全面解析+实例测试</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;LSTM-stateful的理解&quot;&gt;&lt;a href=&quot;#LSTM-stateful的理解&quot; class=&quot;headerlink&quot; title=&quot;LSTM stateful的理解&quot;&gt;&lt;/a&gt;LSTM stateful的理解&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;a href=
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/DBN/%E6%B7%B1%E5%BA%A6%E4%BF%A1%E5%BF%B5%E7%BD%91%E7%BB%9C(Deep%20Belief%20Network,DBN)/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/DBN/深度信念网络(Deep Belief Network,DBN)/</id>
    <published>2019-06-15T15:54:34.346Z</published>
    <updated>2019-02-25T02:01:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>Tags: 深度学习</p><h1 id="深度信念网络-Deep-Belief-Network-DBN"><a href="#深度信念网络-Deep-Belief-Network-DBN" class="headerlink" title="深度信念网络(Deep Belief Network,DBN)"></a>深度信念网络(Deep Belief Network,DBN)</h1><h2 id="1-初识深度信念网络"><a href="#1-初识深度信念网络" class="headerlink" title="1.初识深度信念网络"></a>1.初识深度信念网络</h2><p>深度信念网络是一个概率生成模型，与传统的判别模型的神经网络相对，生成模型是建立一个观察数据和标签之间的联合分布，对$P(Observation|Label)$和 $P(Label|Observation)$都做了评估，而判别模型仅仅而已评估了后者，也就是$P(Label|Observation)$。<br>DBNs由多个限制玻尔兹曼机（Restricted Boltzmann Machines）层组成，一个典型的网络结构如图1所示。这些网络被“限制”为一个可视层和一个隐层，层间存在连接，但层内的单元间不存在连接。隐层单元被训练去捕捉在可视层表现出来的高阶数据的相关性。<br><img src="http://img.blog.csdn.net/20161213120114382?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTgxOTgyNTI5NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><h2 id="2-需要面对的问题"><a href="#2-需要面对的问题" class="headerlink" title="2.需要面对的问题"></a>2.需要面对的问题</h2><p>对于在深度神经网络应用传统的BP算法的时候，DBN遇到了以下问题：  </p><ol><li>需要为训练提供一个有标签的样本集；</li><li>学习过程较慢；</li><li>不适当的参数选择会导致学习收敛于局部最优解。</li></ol><p><strong>Solution：</strong><br>首先，先不考虑最顶构成一个联想记忆（associative memory）的两层，一个DBN的连接是通过自顶向下的生成权值来指导确定的，RBMs就像一个建筑块一样，相比传统和深度分层的sigmoid信念网络，它能易于连接权值的学习。<br>最开始的时候，通过一个非监督贪婪逐层方法去预训练获得生成模型的权值，非监督贪婪逐层方法被Hinton证明是有效的，并被其称为对比分歧（contrastive divergence）。<br><img src="http://img.blog.csdn.net/20161213123012466?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTgxOTgyNTI5NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br>在这个训练阶段，在可视层会产生一个向量v，通过它将值传递到隐层。反过来，可视层的输入会被随机的选择，以尝试去重构原始的输入信号。最后，这些新的可视的神经元激活单元将前向传递重构隐层激活单元，获得h（在训练过程中，首先将可视向量值映射给隐单元；然后可视单元由隐层单元重建；这些新可视单元再次映射给隐单元，这样就获取新的隐单元。执行这种反复步骤叫做吉布斯采样）。这些后退和前进的步骤就是我们熟悉的Gibbs采样，而隐层激活单元和可视层输入之间的相关性差别就作为权值更新的主要依据。<br>训练时间会显著的减少，因为只需要单个步骤就可以接近最大似然学习。增加进网络的每一层都会改进训练数据的对数概率，我们可以理解为越来越接近能量的真实表达。这个有意义的拓展，和无标签数据的使用，是任何一个深度学习应用的决定性的因素。<br>在最高两层，权值被连接到一起，这样更低层的输出将会提供一个参考的线索或者关联给顶层，这样顶层就会将其联系到它的记忆内容。而我们最关心的，最后想得到的就是判别性能，例如分类任务里面。<br>在预训练后，DBN可以通过利用带标签数据用BP算法去对判别性能做调整。在这里，一个标签集将被附加到顶层（推广联想记忆），通过一个自下向上的，学习到的识别权值获得一个网络的分类面。这个性能会比单纯的BP算法训练的网络好。这可以很直观的解释，DBNs的BP算法只需要对权值参数空间进行一个局部的搜索，这相比前向神经网络来说，训练是要快的，而且收敛的时间也少。</p><h2 id="3-详细训练算法流程"><a href="#3-详细训练算法流程" class="headerlink" title="3.详细训练算法流程"></a>3.详细训练算法流程</h2><p><img src="http://img.blog.csdn.net/20161213122524412?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTgxOTgyNTI5NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br>在训练时,Hinton采用了逐层无监督的方法来学习参数。如图3所示，首先把数据向量x和第一层隐藏层作为一个RBM, 训练出这个RBM的参数(连接x和h1的权重, x和h1各个节点的偏置等等), 然后固定这个RBM的参数, 把h1视作可见向量, 把h2视作隐藏向量, 训练第二个RBM, 得到其参数, 然后固定这些参数, 训练h2和h3构成的RBM, 具体的训练算法如下:<br><img src="http://img.blog.csdn.net/20161213123230571?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTgxOTgyNTI5NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br>  CD的训练过程中用到了Gibbs 采样，即在训练过程中，首先将可视向量值映射给隐单元，然后用隐层单元重建可视向量，接着再将可视向量值映射给隐单元……反复执行这种步骤。<br>　k-Gibbs的过程如下：<br><img src="http://img.blog.csdn.net/20161213123405362?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTgxOTgyNTI5NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br>其中，P是model distribution，$\hat{P}$是training set distribution.<br>DBN训练算法：<br><img src="http://img.blog.csdn.net/20161213123735570?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTgxOTgyNTI5NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br>DBN运用CD算法逐层进行训练，得到每一层的参数Wi和ci用于初始化DBN，之后再用监督学习算法对参数进行微调。<br><img src="http://img.blog.csdn.net/20161213123934323?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTgxOTgyNTI5NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><h2 id="4-经典网络结构"><a href="#4-经典网络结构" class="headerlink" title="4.经典网络结构"></a>4.经典网络结构</h2><p>经典的DBN网络结构是由若干层RBM和一层BP组成的一种深层神经网络, 结构如下图4所示。<br><img src="http://img.blog.csdn.net/20161213124220124?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYTgxOTgyNTI5NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br>DBN 在训练模型的过程中主要分为两步:</p><ol><li>分别单独无监督地训练每一层 RBM 网络,确保特征向量映射到不同特征空间时,都尽可能多地保留特征信息;</li><li>在 DBN 的最后一层设置BP网络,接收RBM的输出特征向量作为它的输入特征向量,有监督地训练实体关系分类器.而且每一层 RBM 网络只能确保自身层内的 权值对该层特征向量映射达到最优,并不是对整个 DBN 的特征向量映射达到最优,所以反向传播网络还将错误信息自顶向下传播至每一层 RBM,微调整个 DBN 网络.RBM 网络训练模型的过程可以看作对一个深层 BP 网络权值参数的初始化,使DBN 克服了 BP 网络因随机初始化权值参数而容易陷入局部最优和训练时间长的缺点。<br>上述训练模型中第一步在深度学习的术语叫做预训练，第二步叫做微调。最上面有监督学习的那一层，根据具体的应用领域可以换成任何分类器模型，而不必是BP网络。</li></ol><h2 id="5-拓展"><a href="#5-拓展" class="headerlink" title="5.拓展"></a>5.拓展</h2><p>DBN的灵活性使得它的拓展比较容易。一个拓展就是卷积DBNs(Convolutional Deep Belief Networks(CDBN))。DBN并没有考虑到图像的2维结构信息，因为输入是简单的从一个图像矩阵一维向量化的。而CDBN就是考虑到了这个问题，它利用邻域像素的空域关系，通过一个称为卷积RBM的模型区达到生成模型的变换不变性，而且可以容易得变换到高维图像。DBN并没有明确地处理对观察变量的时间联系的学习上，虽然目前已经有这方面的研究，例如堆叠时间RBMs，以此为推广，有序列学习的dubbed temporal convolutionmachines，这种序列学习的应用，给语音信号处理问题带来了一个让人激动的未来研究方向。<br>目前，和DBN有关的研究包括堆叠自动编码器，它是通过用堆叠自动编码器来替换传统DBN里面的RBM。这就使得可以通过同样的规则来训练产生深度多层神经网络架构，但它缺少层的参数化的严格要求。与DBN不同，自动编码器使用判别模型，这样这个结构就很难采样输入采样空间，这就使得网络更难捕捉它的内部表达。但是，降噪自动编码器却能很好的避免这个问题，并且比传统的DBN更优。它通过在训练过程添加随机的污染并堆叠产生场泛化性能。训练单一的降噪自动编码器的过程和RBM训练生成模型的过程一样。</p><h2 id="5-更多资料"><a href="#5-更多资料" class="headerlink" title="5.更多资料"></a>5.更多资料</h2><ol><li><a href="http://blog.csdn.net/xbinworld/article/details/44901865" target="_blank" rel="noopener">深度学习方法：受限玻尔兹曼机RBM（一）基本概念</a></li><li><a href="http://blog.csdn.net/xbinworld/article/details/45013825" target="_blank" rel="noopener">深度学习方法：受限玻尔兹曼机RBM（二）网络模型</a></li><li><a href="http://blog.csdn.net/xbinworld/article/details/45128733" target="_blank" rel="noopener">深度学习方法：受限玻尔兹曼机RBM（三）模型求解，Gibbs sampling</a></li><li><a href="http://blog.csdn.net/xbinworld/article/details/45274289" target="_blank" rel="noopener">深度学习方法：受限玻尔兹曼机RBM（四）对比散度contrastive divergence，C</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Tags: 深度学习&lt;/p&gt;
&lt;h1 id=&quot;深度信念网络-Deep-Belief-Network-DBN&quot;&gt;&lt;a href=&quot;#深度信念网络-Deep-Belief-Network-DBN&quot; class=&quot;headerlink&quot; title=&quot;深度信念网络(Deep Be
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/DBN/%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA%EF%BC%88RBM%EF%BC%89&amp;%E6%B7%B1%E5%BA%A6%E4%BF%A1%E5%BF%B5%E7%BD%91%E7%BB%9C%EF%BC%88DBN%EF%BC%89/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/DBN/受限玻尔兹曼机（RBM）&amp;深度信念网络（DBN）/</id>
    <published>2019-06-15T15:54:34.343Z</published>
    <updated>2019-02-25T02:01:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="受限玻尔兹曼机（RBM）-amp-深度信念网络（DBN）"><a href="#受限玻尔兹曼机（RBM）-amp-深度信念网络（DBN）" class="headerlink" title="受限玻尔兹曼机（RBM）&amp;深度信念网络（DBN）"></a>受限玻尔兹曼机（RBM）&amp;深度信念网络（DBN）</h1><p>标签（空格分隔）： 深度学习</p><hr><p>DBN调用：<a href="http://blog.csdn.net/m0_37924639/article/details/78962912" target="_blank" rel="noopener">http://blog.csdn.net/m0_37924639/article/details/78962912</a><br>DBN解释：<a href="http://blog.csdn.net/Losteng/article/details/51001247" target="_blank" rel="noopener">http://blog.csdn.net/Losteng/article/details/51001247</a><br>DBN实现：<a href="https://github.com/albertbup/deep-belief-network" target="_blank" rel="noopener">https://github.com/albertbup/deep-belief-network</a></p><p><a href="https://blog.csdn.net/mytestmy/article/details/9150213/" target="_blank" rel="noopener">RBM原理</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;受限玻尔兹曼机（RBM）-amp-深度信念网络（DBN）&quot;&gt;&lt;a href=&quot;#受限玻尔兹曼机（RBM）-amp-深度信念网络（DBN）&quot; class=&quot;headerlink&quot; title=&quot;受限玻尔兹曼机（RBM）&amp;amp;深度信念网络（DBN）&quot;&gt;&lt;/a&gt;受限
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/DBN/Multi-step%20Time%20Series%20Forcasting/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/DBN/Multi-step Time Series Forcasting/</id>
    <published>2019-06-15T15:54:34.341Z</published>
    <updated>2019-02-25T02:01:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Multi-step-Time-Series-Forcasting"><a href="#Multi-step-Time-Series-Forcasting" class="headerlink" title="Multi-step Time Series Forcasting"></a>Multi-step Time Series Forcasting</h1><hr><p>There are at least four commonly used strategies for making multi-step forecasts.<br>They are:</p><h2 id="1-Direct-Multi-step-Forecast-Strategy"><a href="#1-Direct-Multi-step-Forecast-Strategy" class="headerlink" title="1. Direct Multi-step Forecast Strategy"></a>1. Direct Multi-step Forecast Strategy</h2><p>Create seperate prediction model for each ppint. Add computational add maintenance burden and there is <strong>no dependencies</strong> between points.</p><pre><code>prediction(t+1) = model1(obs(t-1), obs(t-2), ..., obs(t-n))prediction(t+2) = model2(obs(t-1), obs(t-3), ..., obs(t-n))</code></pre><p><strong>Attention:</strong> This kind of method just changes the network parameters. The input data remains the same for the whole prediction process.</p><h2 id="2-Recursive-Multi-step-Forecast"><a href="#2-Recursive-Multi-step-Forecast" class="headerlink" title="2. Recursive Multi-step Forecast"></a>2. Recursive Multi-step Forecast</h2><p>The recursive strategy involves using a one-step model multiple times where the prediction for the prior time step is used as an input for making a prediction on the following time step.<br>Because predictions are used in place of observations, the recursive strategy <strong>allows prediction errors to accumulate</strong> such that performance can quickly degrade as the prediction time horizon increases.<br><strong>Attention:</strong> This kind of method just changes the input data. The network parameters remain the same for the whole prediction process.</p><pre><code>prediction(t+1) = model(obs(t-1), obs(t-2), ..., obs(t-n))prediction(t+2) = model(prediction(t+1), obs(t-1), ..., obs(t-n))</code></pre><h2 id="3-Direct-Recursive-Hybrid-Multi-step-Forecast-Strategies"><a href="#3-Direct-Recursive-Hybrid-Multi-step-Forecast-Strategies" class="headerlink" title="3. Direct-Recursive Hybrid Multi-step Forecast Strategies"></a>3. Direct-Recursive Hybrid Multi-step Forecast Strategies</h2><p>The direct and recursive strategies can be combined to offer the benefits of both methods.<br>For example, a separate model can be constructed for each time step to be predicted, but each model may use the predictions made by models at prior time steps as input values.<br>This model learn a different model for each point using known data as well as prediction data.</p><pre><code>prediction(t+1) = model1(obs(t-1), obs(t-2), ..., obs(t-n))prediction(t+2) = model2(prediction(t+1), obs(t-1), ..., obs(t-n))</code></pre><h2 id="4-Multiple-Output-Forecast-Strategy"><a href="#4-Multiple-Output-Forecast-Strategy" class="headerlink" title="4. Multiple Output Forecast Strategy"></a>4. Multiple Output Forecast Strategy</h2><p>The multiple output strategy involves developing one model that is capable of predicting the entire forecast sequence in a one-shot manner.<br>In the case of predicting the temperature for the next two days, we would develop one model and use it to predict the next two days as one operation.<br>Multiple output models are more complex as they can learn the dependence structure between inputs and outputs as well as between outputs.<br>Being more complex may mean that they are slower to train and require more data to avoid overfitting the problem.</p><pre><code>prediction(t+1), prediction(t+2) = model(obs(t-1), obs(t-2), ..., obs(t-n))</code></pre><h2 id="Personal-View"><a href="#Personal-View" class="headerlink" title="Personal View"></a>Personal View</h2><p>Among the four methods, actually the first three methods are one-step ahead method. They realizes multi-step prediction by changing eather training datasets or network parameters. The fourth method is a multi-step network from the network structure aspect.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://machinelearningmastery.com/multi-step-time-series-forecasting/" target="_blank" rel="noopener">https://machinelearningmastery.com/multi-step-time-series-forecasting/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Multi-step-Time-Series-Forcasting&quot;&gt;&lt;a href=&quot;#Multi-step-Time-Series-Forcasting&quot; class=&quot;headerlink&quot; title=&quot;Multi-step Time Series For
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yyb1995.github.io/2019/06/15/DeepLearning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/"/>
    <id>https://yyb1995.github.io/2019/06/15/DeepLearning/深度学习参考资料/</id>
    <published>2019-06-15T15:54:34.336Z</published>
    <updated>2019-02-25T02:01:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>Tags:深度学习</p><hr><p>#深度学习参考资料</p><ol><li><p>lstm实现<br><a href="http://blog.csdn.net/mydear_11000/article/details/52414342###" target="_blank" rel="noopener">http://blog.csdn.net/mydear_11000/article/details/52414342###</a><br><a href="http://blog.csdn.net/u013082989/article/details/73693392" target="_blank" rel="noopener">http://blog.csdn.net/u013082989/article/details/73693392</a></p></li><li><p>RNN和LSTM基础<br><a href="http://blog.csdn.net/xingzhedai/article/details/53144126" target="_blank" rel="noopener">http://blog.csdn.net/xingzhedai/article/details/53144126</a><br><a href="https://www.jianshu.com/p/9dc9f41f0b29" target="_blank" rel="noopener">https://www.jianshu.com/p/9dc9f41f0b29</a><br><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a><br><a href="https://www.zybuluo.com/hanbingtao/note/476663" target="_blank" rel="noopener">https://www.zybuluo.com/hanbingtao/note/476663</a></p></li><li><p>tensorflow 常用算子<br><a href="https://www.cnblogs.com/wuzhitj/p/6431381.html" target="_blank" rel="noopener">https://www.cnblogs.com/wuzhitj/p/6431381.html</a></p></li><li><p>DBN算法实现<br><a href="http://blog.csdn.net/zhanglu_wind/article/details/78949020" target="_blank" rel="noopener">http://blog.csdn.net/zhanglu_wind/article/details/78949020</a></p></li><li><p>数据分析必读书目<br><a href="http://www.cnblogs.com/charlotte77/p/5381681.html" target="_blank" rel="noopener">http://www.cnblogs.com/charlotte77/p/5381681.html</a></p></li><li><p>深度神经网络系列文章</p><ol><li><a href="https://www.zybuluo.com/hanbingtao/note/433855" target="_blank" rel="noopener">零基础入门深度学习(1) - 感知器</a></li><li><a href="https://www.zybuluo.com/hanbingtao/note/448086" target="_blank" rel="noopener">零基础入门深度学习(2) - 线性单元和梯度下降</a></li><li><a href="https://www.zybuluo.com/hanbingtao/note/476663" target="_blank" rel="noopener">零基础入门深度学习(3) - 神经网络和反向传播算法</a></li><li><a href="https://www.zybuluo.com/hanbingtao/note/485480" target="_blank" rel="noopener">零基础入门深度学习(4) - 卷积神经网络</a></li><li><a href="https://www.zybuluo.com/hanbingtao/note/541458" target="_blank" rel="noopener">零基础入门深度学习(5) - 循环神经网络</a></li><li><a href="https://zybuluo.com/hanbingtao/note/581764" target="_blank" rel="noopener">零基础入门深度学习(6) - 长短时记忆网络(LSTM)</a></li><li><a href="https://www.zybuluo.com/hanbingtao/note/626300" target="_blank" rel="noopener">零基础入门深度学习(7) - 递归神经网络</a></li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Tags:深度学习&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;#深度学习参考资料&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;lstm实现&lt;br&gt;&lt;a href=&quot;http://blog.csdn.net/mydear_11000/article/details/52414342###&quot; target=
      
    
    </summary>
    
    
  </entry>
  
</feed>
